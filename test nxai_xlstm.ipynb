{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a14811",
   "metadata": {},
   "source": [
    "# Cell width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94552ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d92ef3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8e34d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys \n",
    "\n",
    "# sys.path.append('..')\n",
    "from omegaconf import OmegaConf\n",
    "from pprint import pprint\n",
    "from dacite import from_dict\n",
    "from dacite import Config as DaciteConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xlstm.xlstm_block_stack import xLSTMBlockStack, xLSTMBlockStackConfig\n",
    "\n",
    "from xlstm_moex.data.download import get_historical_data\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a733e2",
   "metadata": {},
   "source": [
    "# Init config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d2cbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlstm_cfg = f\"\"\" \n",
    "# mlstm_block:\n",
    "#   mlstm:\n",
    "#     conv1d_kernel_size: 4\n",
    "#     qkv_proj_blocksize: 1\n",
    "#     num_heads: 1\n",
    "#     proj_factor: 1\n",
    "# slstm_block:\n",
    "#   slstm:\n",
    "#     backend: {'cuda' if torch.cuda.is_available() else 'vanilla'} #! only vanilla here works\n",
    "#     num_heads: 1\n",
    "#     conv1d_kernel_size: 0\n",
    "#     bias_init: powerlaw_blockdependent\n",
    "#   feedforward:\n",
    "#     proj_factor: 1.2\n",
    "#     act_fn: gelu\n",
    "# context_length: 50\n",
    "# num_blocks: 2\n",
    "# embedding_dim: 1 # same as `in_features` in Pytorch LSTM\n",
    "# slstm_at: [1] #[1] # for [] it also works, so if no sLSTM is in the stack\n",
    "# \"\"\"\n",
    "\n",
    "xlstm_cfg = f\"\"\" \n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 4\n",
    "    qkv_proj_blocksize: 4\n",
    "    num_heads: 4\n",
    "slstm_block:\n",
    "  slstm:\n",
    "    backend: {'cuda' if torch.cuda.is_available() else 'vanilla'} #! only vanilla here works\n",
    "    num_heads: 4\n",
    "    conv1d_kernel_size: 4\n",
    "    bias_init: powerlaw_blockdependent\n",
    "  feedforward:\n",
    "    proj_factor: 1.3\n",
    "    act_fn: gelu\n",
    "context_length: 256\n",
    "num_blocks: 7\n",
    "embedding_dim: 128\n",
    "add_post_blocks_norm: False\n",
    "slstm_at: [1] #[1] # for [] it also works, so if no sLSTM is in the stack\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f502f8c",
   "metadata": {},
   "source": [
    "# Init XLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea0fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/home/nick/anaconda3/envs/nxai_xlstm/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=128', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/nick/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/nick/.cache/torch_extensions/py311_cu121/slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
      "Building extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module slstm_HS128BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    }
   ],
   "source": [
    "cfg = OmegaConf.create(xlstm_cfg)\n",
    "cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(cfg), config=DaciteConfig(strict=True))\n",
    "xlstm_stack = xLSTMBlockStack(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e64fdc7",
   "metadata": {},
   "source": [
    "# Inspect config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2209e832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "pprint(cfg.embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048a604",
   "metadata": {},
   "source": [
    "# Inspect layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dae4b998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xLSTMBlockStack(\n",
       "  (blocks): ModuleList(\n",
       "    (0): mLSTMBlock(\n",
       "      (xlstm_norm): LayerNorm()\n",
       "      (xlstm): mLSTMLayer(\n",
       "        (proj_up): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (conv1d): CausalConv1d(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "        )\n",
       "        (conv_act_fn): SiLU()\n",
       "        (mlstm_cell): mLSTMCell(\n",
       "          (igate): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (fgate): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (outnorm): MultiHeadLayerNorm()\n",
       "        )\n",
       "        (ogate_act_fn): SiLU()\n",
       "        (proj_down): Linear(in_features=256, out_features=128, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): sLSTMBlock(\n",
       "      (xlstm_norm): LayerNorm()\n",
       "      (xlstm): sLSTMLayer(\n",
       "        (conv1d): CausalConv1d(\n",
       "          (conv): Conv1d(128, 128, kernel_size=(4,), stride=(1,), padding=(3,), groups=128)\n",
       "        )\n",
       "        (conv_act_fn): SiLU()\n",
       "        (fgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (igate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (zgate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (ogate): LinearHeadwiseExpand(in_features=128, num_heads=4, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (slstm_cell): sLSTMCell_cuda(function=slstm, hidden_size=128, num_heads=4)\n",
       "        (group_norm): MultiHeadLayerNorm()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffn_norm): LayerNorm()\n",
       "      (ffn): GatedFeedForward(\n",
       "        (proj_up): Linear(in_features=128, out_features=384, bias=False)\n",
       "        (proj_down): Linear(in_features=192, out_features=128, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2-6): 5 x mLSTMBlock(\n",
       "      (xlstm_norm): LayerNorm()\n",
       "      (xlstm): mLSTMLayer(\n",
       "        (proj_up): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (q_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (k_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (v_proj): LinearHeadwiseExpand(in_features=256, num_heads=64, expand_factor_up=1, bias=False, trainable_weight=True, trainable_bias=True, )\n",
       "        (conv1d): CausalConv1d(\n",
       "          (conv): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)\n",
       "        )\n",
       "        (conv_act_fn): SiLU()\n",
       "        (mlstm_cell): mLSTMCell(\n",
       "          (igate): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (fgate): Linear(in_features=768, out_features=4, bias=True)\n",
       "          (outnorm): MultiHeadLayerNorm()\n",
       "        )\n",
       "        (ogate_act_fn): SiLU()\n",
       "        (proj_down): Linear(in_features=256, out_features=128, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_blocks_norm): Identity()\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlstm_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2d5e86",
   "metadata": {},
   "source": [
    "# Generate synthetic examlpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9400311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 256, 128).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "678475b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 128])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e53df36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1, :].view(2,1,128).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108fe3c4",
   "metadata": {},
   "source": [
    "# Check model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "69e54f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlstm_stack = xlstm_stack.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9d30941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    y, states_dict = xlstm_stack.step(x[:, i, :].view(2,1,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30c0e8c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mlstm_state': (tensor([[[[ 5.4874e-06, -1.2764e-06, -4.2204e-06,  ..., -1.7712e-05,\n",
       "             -2.4697e-06, -7.9438e-06],\n",
       "            [ 4.2978e-05, -9.9968e-06, -3.3054e-05,  ..., -1.3872e-04,\n",
       "             -1.9342e-05, -6.2216e-05],\n",
       "            [-2.4128e-05,  5.6123e-06,  1.8557e-05,  ...,  7.7881e-05,\n",
       "              1.0859e-05,  3.4929e-05],\n",
       "            ...,\n",
       "            [-1.0465e-05,  2.4343e-06,  8.0490e-06,  ...,  3.3780e-05,\n",
       "              4.7100e-06,  1.5150e-05],\n",
       "            [-3.4788e-05,  8.0919e-06,  2.6756e-05,  ...,  1.1229e-04,\n",
       "              1.5657e-05,  5.0361e-05],\n",
       "            [-1.0198e-05,  2.3721e-06,  7.8432e-06,  ...,  3.2916e-05,\n",
       "              4.5896e-06,  1.4763e-05]],\n",
       "  \n",
       "           [[ 1.1881e-04, -4.0994e-04,  2.5100e-04,  ...,  1.2454e-06,\n",
       "             -2.5543e-04,  3.8301e-04],\n",
       "            [ 1.7321e-04, -5.9762e-04,  3.6592e-04,  ...,  1.8156e-06,\n",
       "             -3.7237e-04,  5.5836e-04],\n",
       "            [ 7.2518e-05, -2.5021e-04,  1.5320e-04,  ...,  7.6015e-07,\n",
       "             -1.5590e-04,  2.3377e-04],\n",
       "            ...,\n",
       "            [ 2.0607e-05, -7.1100e-05,  4.3534e-05,  ...,  2.1600e-07,\n",
       "             -4.4302e-05,  6.6429e-05],\n",
       "            [-3.2479e-05,  1.1206e-04, -6.8614e-05,  ..., -3.4045e-07,\n",
       "              6.9824e-05, -1.0470e-04],\n",
       "            [-1.3097e-04,  4.5191e-04, -2.7670e-04,  ..., -1.3729e-06,\n",
       "              2.8158e-04, -4.2222e-04]],\n",
       "  \n",
       "           [[ 1.9692e-04, -1.3150e-05,  1.0729e-04,  ...,  3.7522e-04,\n",
       "              4.8295e-05,  1.8662e-04],\n",
       "            [-2.1512e-04,  1.4366e-05, -1.1720e-04,  ..., -4.0990e-04,\n",
       "             -5.2758e-05, -2.0387e-04],\n",
       "            [-9.6857e-05,  6.4680e-06, -5.2770e-05,  ..., -1.8455e-04,\n",
       "             -2.3754e-05, -9.1789e-05],\n",
       "            ...,\n",
       "            [-9.4312e-05,  6.2981e-06, -5.1384e-05,  ..., -1.7970e-04,\n",
       "             -2.3130e-05, -8.9378e-05],\n",
       "            [-1.2983e-04,  8.6699e-06, -7.0735e-05,  ..., -2.4738e-04,\n",
       "             -3.1840e-05, -1.2304e-04],\n",
       "            [ 1.8608e-05, -1.2427e-06,  1.0138e-05,  ...,  3.5457e-05,\n",
       "              4.5637e-06,  1.7635e-05]],\n",
       "  \n",
       "           [[-5.4367e-05, -4.0511e-04, -9.4968e-05,  ..., -2.6166e-04,\n",
       "             -4.4386e-06, -2.4104e-04],\n",
       "            [-4.3100e-05, -3.2116e-04, -7.5288e-05,  ..., -2.0744e-04,\n",
       "             -3.5188e-06, -1.9109e-04],\n",
       "            [ 1.3977e-05,  1.0415e-04,  2.4415e-05,  ...,  6.7270e-05,\n",
       "              1.1411e-06,  6.1969e-05],\n",
       "            ...,\n",
       "            [ 1.3907e-06,  1.0363e-05,  2.4293e-06,  ...,  6.6934e-06,\n",
       "              1.1354e-07,  6.1660e-06],\n",
       "            [ 5.9665e-06,  4.4459e-05,  1.0422e-05,  ...,  2.8716e-05,\n",
       "              4.8711e-07,  2.6453e-05],\n",
       "            [ 8.6750e-06,  6.4641e-05,  1.5153e-05,  ...,  4.1751e-05,\n",
       "              7.0823e-07,  3.8462e-05]]],\n",
       "  \n",
       "  \n",
       "          [[[-2.4331e-05, -1.3676e-05,  1.1046e-05,  ...,  7.1812e-05,\n",
       "              3.7987e-05,  3.4354e-05],\n",
       "            [-1.4164e-04, -7.9613e-05,  6.4307e-05,  ...,  4.1805e-04,\n",
       "              2.2114e-04,  1.9999e-04],\n",
       "            [ 1.7568e-04,  9.8748e-05, -7.9763e-05,  ..., -5.1854e-04,\n",
       "             -2.7430e-04, -2.4806e-04],\n",
       "            ...,\n",
       "            [ 4.2534e-06,  2.3907e-06, -1.9311e-06,  ..., -1.2554e-05,\n",
       "             -6.6409e-06, -6.0058e-06],\n",
       "            [ 6.5182e-05,  3.6637e-05, -2.9594e-05,  ..., -1.9239e-04,\n",
       "             -1.0177e-04, -9.2037e-05],\n",
       "            [-1.0723e-04, -6.0274e-05,  4.8686e-05,  ...,  3.1651e-04,\n",
       "              1.6743e-04,  1.5141e-04]],\n",
       "  \n",
       "           [[ 1.4901e-04, -8.9038e-05,  2.2690e-04,  ...,  1.7878e-04,\n",
       "              2.3076e-04,  1.2214e-04],\n",
       "            [-2.3186e-04,  1.3854e-04, -3.5305e-04,  ..., -2.7818e-04,\n",
       "             -3.5905e-04, -1.9004e-04],\n",
       "            [ 2.6655e-05, -1.5927e-05,  4.0587e-05,  ...,  3.1980e-05,\n",
       "              4.1278e-05,  2.1848e-05],\n",
       "            ...,\n",
       "            [-7.8172e-06,  4.6709e-06, -1.1903e-05,  ..., -9.3789e-06,\n",
       "             -1.2106e-05, -6.4074e-06],\n",
       "            [-7.6446e-06,  4.5677e-06, -1.1640e-05,  ..., -9.1718e-06,\n",
       "             -1.1838e-05, -6.2659e-06],\n",
       "            [ 4.6285e-05, -2.7656e-05,  7.0478e-05,  ...,  5.5532e-05,\n",
       "              7.1677e-05,  3.7938e-05]],\n",
       "  \n",
       "           [[-4.3245e-05, -8.2412e-06, -3.0433e-05,  ...,  6.4366e-05,\n",
       "              3.9305e-05, -3.5553e-05],\n",
       "            [-3.8275e-05, -7.2941e-06, -2.6936e-05,  ...,  5.6969e-05,\n",
       "              3.4788e-05, -3.1467e-05],\n",
       "            [ 2.0669e-05,  3.9389e-06,  1.4546e-05,  ..., -3.0764e-05,\n",
       "             -1.8786e-05,  1.6993e-05],\n",
       "            ...,\n",
       "            [-8.4317e-05, -1.6068e-05, -5.9337e-05,  ...,  1.2550e-04,\n",
       "              7.6635e-05, -6.9320e-05],\n",
       "            [ 5.3778e-06,  1.0248e-06,  3.7846e-06,  ..., -8.0043e-06,\n",
       "             -4.8878e-06,  4.4213e-06],\n",
       "            [ 4.0627e-05,  7.7423e-06,  2.8591e-05,  ..., -6.0469e-05,\n",
       "             -3.6926e-05,  3.3401e-05]],\n",
       "  \n",
       "           [[ 4.2450e-06, -3.8220e-05, -9.1577e-05,  ...,  1.1215e-04,\n",
       "             -6.7087e-05,  1.6913e-04],\n",
       "            [ 3.1429e-06, -2.8297e-05, -6.7802e-05,  ...,  8.3037e-05,\n",
       "             -4.9670e-05,  1.2522e-04],\n",
       "            [-1.6711e-06,  1.5046e-05,  3.6050e-05,  ..., -4.4151e-05,\n",
       "              2.6410e-05, -6.6581e-05],\n",
       "            ...,\n",
       "            [ 1.0422e-06, -9.3833e-06, -2.2483e-05,  ...,  2.7535e-05,\n",
       "             -1.6471e-05,  4.1523e-05],\n",
       "            [ 3.5420e-06, -3.1890e-05, -7.6411e-05,  ...,  9.3582e-05,\n",
       "             -5.5977e-05,  1.4112e-04],\n",
       "            [-7.2793e-07,  6.5539e-06,  1.5704e-05,  ..., -1.9232e-05,\n",
       "              1.1504e-05, -2.9003e-05]]]], device='cuda:0',\n",
       "         grad_fn=<AddBackward0>),\n",
       "  tensor([[[[ 2.7608e-04],\n",
       "            [ 2.1623e-03],\n",
       "            [-1.2139e-03],\n",
       "            [-4.1895e-04],\n",
       "            [-1.7014e-03],\n",
       "            [-6.6401e-04],\n",
       "            [ 6.2111e-04],\n",
       "            [ 4.4712e-04],\n",
       "            [ 2.3477e-04],\n",
       "            [ 8.8621e-04],\n",
       "            [-3.6356e-04],\n",
       "            [ 4.5780e-04],\n",
       "            [ 6.5757e-04],\n",
       "            [ 1.7146e-04],\n",
       "            [-2.3261e-03],\n",
       "            [-2.6057e-03],\n",
       "            [ 1.7585e-03],\n",
       "            [-2.0602e-03],\n",
       "            [ 2.0379e-04],\n",
       "            [-1.3245e-03],\n",
       "            [-5.5536e-04],\n",
       "            [ 9.6102e-04],\n",
       "            [-7.0384e-04],\n",
       "            [ 2.2512e-03],\n",
       "            [-1.2940e-03],\n",
       "            [ 1.4963e-03],\n",
       "            [-6.9811e-04],\n",
       "            [-3.1317e-03],\n",
       "            [-6.2475e-04],\n",
       "            [ 2.2022e-03],\n",
       "            [ 4.2320e-03],\n",
       "            [ 2.9390e-03],\n",
       "            [ 5.3418e-04],\n",
       "            [ 2.0369e-03],\n",
       "            [-3.7784e-03],\n",
       "            [-3.0118e-03],\n",
       "            [-1.1762e-04],\n",
       "            [-1.2887e-03],\n",
       "            [ 3.3368e-05],\n",
       "            [-2.8913e-03],\n",
       "            [ 4.1378e-03],\n",
       "            [-1.7976e-03],\n",
       "            [-8.2947e-04],\n",
       "            [ 2.0879e-03],\n",
       "            [-2.0026e-03],\n",
       "            [ 9.3078e-05],\n",
       "            [-1.6552e-04],\n",
       "            [ 1.1724e-03],\n",
       "            [-1.8628e-03],\n",
       "            [-1.9626e-03],\n",
       "            [-4.5503e-04],\n",
       "            [-1.0372e-03],\n",
       "            [-2.8685e-03],\n",
       "            [ 5.5825e-04],\n",
       "            [ 8.7814e-04],\n",
       "            [ 1.4235e-03],\n",
       "            [ 1.8838e-03],\n",
       "            [-1.2668e-03],\n",
       "            [ 1.2308e-03],\n",
       "            [ 5.8887e-04],\n",
       "            [-1.6074e-03],\n",
       "            [-5.2653e-04],\n",
       "            [-1.7503e-03],\n",
       "            [-5.1307e-04]],\n",
       "  \n",
       "           [[ 3.8582e-03],\n",
       "            [ 5.6247e-03],\n",
       "            [ 2.3549e-03],\n",
       "            [ 1.2069e-03],\n",
       "            [ 3.3654e-03],\n",
       "            [ 3.8958e-03],\n",
       "            [ 1.8683e-03],\n",
       "            [-2.3820e-03],\n",
       "            [-1.8009e-04],\n",
       "            [ 2.7283e-03],\n",
       "            [ 1.4760e-03],\n",
       "            [-1.9471e-03],\n",
       "            [ 1.5524e-03],\n",
       "            [-1.7060e-03],\n",
       "            [ 1.1877e-03],\n",
       "            [ 2.6172e-03],\n",
       "            [ 1.3283e-04],\n",
       "            [ 1.1014e-03],\n",
       "            [ 2.0119e-04],\n",
       "            [-3.9448e-05],\n",
       "            [-7.8317e-04],\n",
       "            [-5.7702e-03],\n",
       "            [-1.4190e-03],\n",
       "            [-4.1757e-03],\n",
       "            [ 1.8905e-03],\n",
       "            [-8.0302e-05],\n",
       "            [ 6.7760e-04],\n",
       "            [ 5.2269e-05],\n",
       "            [ 4.0683e-03],\n",
       "            [ 1.0410e-03],\n",
       "            [-1.2067e-03],\n",
       "            [ 4.9330e-05],\n",
       "            [ 1.5181e-03],\n",
       "            [ 3.1408e-03],\n",
       "            [ 9.8748e-04],\n",
       "            [-1.8988e-03],\n",
       "            [ 4.7141e-04],\n",
       "            [-1.8109e-03],\n",
       "            [ 9.7368e-04],\n",
       "            [ 1.4652e-03],\n",
       "            [ 7.0446e-04],\n",
       "            [ 2.5992e-03],\n",
       "            [-8.0435e-04],\n",
       "            [-8.5857e-04],\n",
       "            [-2.0807e-03],\n",
       "            [ 1.6718e-03],\n",
       "            [-9.7068e-04],\n",
       "            [-4.4529e-03],\n",
       "            [-3.1527e-03],\n",
       "            [-1.1132e-03],\n",
       "            [ 3.1015e-03],\n",
       "            [ 2.4152e-03],\n",
       "            [-2.1577e-03],\n",
       "            [-1.5958e-03],\n",
       "            [-1.0280e-03],\n",
       "            [-2.4745e-03],\n",
       "            [-1.8250e-03],\n",
       "            [ 1.4629e-04],\n",
       "            [ 7.3443e-03],\n",
       "            [-2.5122e-03],\n",
       "            [ 9.5244e-04],\n",
       "            [ 6.6917e-04],\n",
       "            [-1.0547e-03],\n",
       "            [-4.2532e-03]],\n",
       "  \n",
       "           [[-1.8684e-03],\n",
       "            [ 2.0410e-03],\n",
       "            [ 9.1896e-04],\n",
       "            [-1.6814e-03],\n",
       "            [-8.6926e-04],\n",
       "            [-4.9777e-04],\n",
       "            [ 6.9829e-04],\n",
       "            [ 4.6657e-04],\n",
       "            [-2.8236e-03],\n",
       "            [-1.4628e-03],\n",
       "            [ 1.3150e-03],\n",
       "            [-8.6884e-04],\n",
       "            [ 2.8843e-04],\n",
       "            [-3.5942e-04],\n",
       "            [-6.7017e-04],\n",
       "            [ 1.9142e-03],\n",
       "            [ 1.4801e-03],\n",
       "            [-3.6859e-03],\n",
       "            [ 8.6833e-04],\n",
       "            [ 6.9103e-04],\n",
       "            [ 2.9048e-03],\n",
       "            [ 1.2014e-03],\n",
       "            [ 2.4870e-03],\n",
       "            [ 2.2164e-03],\n",
       "            [ 1.8008e-03],\n",
       "            [ 2.6936e-03],\n",
       "            [-1.1899e-03],\n",
       "            [-1.1362e-03],\n",
       "            [ 3.3643e-04],\n",
       "            [-3.7503e-03],\n",
       "            [-3.8435e-04],\n",
       "            [-4.3593e-03],\n",
       "            [-6.8870e-04],\n",
       "            [-1.1972e-03],\n",
       "            [ 1.7481e-03],\n",
       "            [ 2.0302e-03],\n",
       "            [-8.3553e-04],\n",
       "            [ 5.0815e-04],\n",
       "            [-2.3064e-03],\n",
       "            [ 1.5463e-03],\n",
       "            [ 1.5747e-03],\n",
       "            [-2.6824e-03],\n",
       "            [-1.6243e-03],\n",
       "            [-8.9566e-04],\n",
       "            [ 7.4245e-05],\n",
       "            [-4.9516e-04],\n",
       "            [ 1.1560e-03],\n",
       "            [ 5.7637e-04],\n",
       "            [ 1.2876e-03],\n",
       "            [ 1.3989e-03],\n",
       "            [ 2.2228e-03],\n",
       "            [-7.3625e-04],\n",
       "            [ 1.7464e-03],\n",
       "            [-2.0899e-03],\n",
       "            [-4.2256e-03],\n",
       "            [ 8.0615e-05],\n",
       "            [-1.1387e-04],\n",
       "            [-1.5181e-03],\n",
       "            [ 6.3408e-04],\n",
       "            [-7.3528e-04],\n",
       "            [-5.4463e-04],\n",
       "            [ 8.9482e-04],\n",
       "            [ 1.2318e-03],\n",
       "            [-1.7655e-04]],\n",
       "  \n",
       "           [[-3.8446e-03],\n",
       "            [-3.0479e-03],\n",
       "            [ 9.8840e-04],\n",
       "            [ 5.2323e-03],\n",
       "            [-1.2138e-03],\n",
       "            [ 4.6541e-04],\n",
       "            [-5.9364e-04],\n",
       "            [ 9.1899e-04],\n",
       "            [-2.3866e-03],\n",
       "            [-1.9490e-03],\n",
       "            [-2.1124e-03],\n",
       "            [-1.2585e-03],\n",
       "            [-2.3281e-04],\n",
       "            [ 1.1924e-03],\n",
       "            [-9.5968e-04],\n",
       "            [-1.9987e-03],\n",
       "            [ 1.3445e-03],\n",
       "            [ 2.4344e-03],\n",
       "            [ 3.3196e-03],\n",
       "            [ 1.6639e-03],\n",
       "            [ 1.0796e-03],\n",
       "            [ 1.8343e-03],\n",
       "            [ 2.9373e-03],\n",
       "            [ 2.3490e-03],\n",
       "            [ 9.5056e-04],\n",
       "            [ 6.7359e-04],\n",
       "            [-8.6671e-05],\n",
       "            [ 1.2617e-04],\n",
       "            [-1.5351e-03],\n",
       "            [ 2.8904e-04],\n",
       "            [-2.1986e-03],\n",
       "            [ 9.5534e-04],\n",
       "            [-1.2245e-04],\n",
       "            [ 2.8076e-03],\n",
       "            [ 3.8572e-04],\n",
       "            [ 8.0471e-04],\n",
       "            [-1.3686e-03],\n",
       "            [ 2.9820e-05],\n",
       "            [ 1.9844e-03],\n",
       "            [ 2.8210e-03],\n",
       "            [ 6.2759e-03],\n",
       "            [ 5.1319e-03],\n",
       "            [-9.8201e-05],\n",
       "            [ 3.5556e-03],\n",
       "            [ 4.1937e-03],\n",
       "            [ 1.6453e-03],\n",
       "            [ 5.0788e-04],\n",
       "            [-1.0825e-03],\n",
       "            [ 1.5470e-03],\n",
       "            [-1.5864e-03],\n",
       "            [ 2.3326e-03],\n",
       "            [-2.9377e-03],\n",
       "            [ 1.7219e-03],\n",
       "            [-4.7732e-03],\n",
       "            [-2.7160e-03],\n",
       "            [-1.2846e-03],\n",
       "            [-1.7569e-03],\n",
       "            [-1.9580e-03],\n",
       "            [ 7.9647e-04],\n",
       "            [-5.7302e-03],\n",
       "            [-2.0346e-03],\n",
       "            [ 9.8347e-05],\n",
       "            [ 4.2192e-04],\n",
       "            [ 6.1346e-04]]],\n",
       "  \n",
       "  \n",
       "          [[[ 2.4982e-04],\n",
       "            [ 1.4543e-03],\n",
       "            [-1.8039e-03],\n",
       "            [ 7.7998e-04],\n",
       "            [-2.8575e-04],\n",
       "            [-4.7009e-04],\n",
       "            [-1.1035e-04],\n",
       "            [ 6.3002e-04],\n",
       "            [ 1.5202e-04],\n",
       "            [ 5.2911e-04],\n",
       "            [-3.3054e-04],\n",
       "            [ 1.0157e-03],\n",
       "            [-1.1538e-03],\n",
       "            [-1.2092e-03],\n",
       "            [-1.4172e-03],\n",
       "            [ 3.5688e-04],\n",
       "            [ 9.7523e-04],\n",
       "            [ 6.4120e-04],\n",
       "            [ 1.7334e-03],\n",
       "            [-9.0818e-04],\n",
       "            [-7.6733e-04],\n",
       "            [ 1.4071e-03],\n",
       "            [-9.2605e-05],\n",
       "            [ 1.7577e-04],\n",
       "            [ 1.5978e-03],\n",
       "            [ 2.4052e-04],\n",
       "            [-7.3397e-04],\n",
       "            [-2.3867e-03],\n",
       "            [-3.1912e-04],\n",
       "            [ 2.2908e-04],\n",
       "            [ 1.5586e-03],\n",
       "            [ 3.8022e-03],\n",
       "            [-1.9335e-04],\n",
       "            [ 2.7069e-03],\n",
       "            [-3.3619e-03],\n",
       "            [-3.1713e-03],\n",
       "            [-5.4365e-04],\n",
       "            [-1.2267e-03],\n",
       "            [ 8.6646e-04],\n",
       "            [-1.2658e-03],\n",
       "            [ 1.3067e-03],\n",
       "            [ 1.7227e-03],\n",
       "            [-2.0387e-03],\n",
       "            [ 1.5230e-03],\n",
       "            [ 2.1784e-03],\n",
       "            [ 1.5992e-04],\n",
       "            [ 1.0405e-03],\n",
       "            [-9.4578e-04],\n",
       "            [-1.4372e-03],\n",
       "            [-1.2968e-03],\n",
       "            [-6.2143e-04],\n",
       "            [-5.1876e-04],\n",
       "            [-1.1410e-03],\n",
       "            [-4.5042e-04],\n",
       "            [ 1.8650e-04],\n",
       "            [ 2.5805e-03],\n",
       "            [ 1.2132e-03],\n",
       "            [ 1.4054e-03],\n",
       "            [ 3.8849e-04],\n",
       "            [-2.3456e-04],\n",
       "            [-7.7673e-04],\n",
       "            [-4.3674e-05],\n",
       "            [-6.6928e-04],\n",
       "            [ 1.1011e-03]],\n",
       "  \n",
       "           [[-3.6402e-03],\n",
       "            [ 5.6640e-03],\n",
       "            [-6.5114e-04],\n",
       "            [ 3.2678e-04],\n",
       "            [ 5.6414e-03],\n",
       "            [ 4.7739e-03],\n",
       "            [ 4.0496e-03],\n",
       "            [-4.7434e-03],\n",
       "            [-1.2026e-03],\n",
       "            [ 4.3972e-03],\n",
       "            [-8.0133e-05],\n",
       "            [-1.1505e-03],\n",
       "            [ 1.6252e-03],\n",
       "            [-1.4728e-03],\n",
       "            [ 1.0094e-03],\n",
       "            [ 2.6484e-03],\n",
       "            [-1.8341e-04],\n",
       "            [ 3.1917e-04],\n",
       "            [ 1.5373e-05],\n",
       "            [ 5.5205e-04],\n",
       "            [-2.4315e-03],\n",
       "            [-4.6965e-03],\n",
       "            [ 1.3835e-03],\n",
       "            [-3.0694e-03],\n",
       "            [ 5.4443e-03],\n",
       "            [-1.7226e-04],\n",
       "            [ 1.0632e-03],\n",
       "            [ 3.3784e-04],\n",
       "            [ 2.7404e-03],\n",
       "            [ 3.6797e-04],\n",
       "            [-1.3043e-03],\n",
       "            [-4.8545e-04],\n",
       "            [ 1.2722e-03],\n",
       "            [ 3.1309e-03],\n",
       "            [ 2.9636e-03],\n",
       "            [-1.2255e-03],\n",
       "            [ 1.6186e-03],\n",
       "            [ 1.0212e-03],\n",
       "            [ 3.9029e-04],\n",
       "            [ 1.0000e-04],\n",
       "            [ 3.1474e-04],\n",
       "            [ 2.3637e-03],\n",
       "            [-1.5819e-03],\n",
       "            [ 1.2880e-04],\n",
       "            [-4.0595e-04],\n",
       "            [-2.6759e-05],\n",
       "            [-1.5859e-04],\n",
       "            [-2.3426e-03],\n",
       "            [-6.8476e-04],\n",
       "            [-1.9057e-03],\n",
       "            [-7.3424e-04],\n",
       "            [ 4.7437e-03],\n",
       "            [-2.9060e-03],\n",
       "            [-2.2384e-03],\n",
       "            [-1.3439e-03],\n",
       "            [-3.4787e-03],\n",
       "            [-1.6601e-03],\n",
       "            [-2.0680e-04],\n",
       "            [ 6.6109e-03],\n",
       "            [-1.7878e-03],\n",
       "            [-1.0635e-04],\n",
       "            [ 1.9096e-04],\n",
       "            [ 1.8675e-04],\n",
       "            [-1.1307e-03]],\n",
       "  \n",
       "           [[-9.7784e-04],\n",
       "            [-8.6546e-04],\n",
       "            [ 4.6736e-04],\n",
       "            [-1.9996e-03],\n",
       "            [-5.4786e-04],\n",
       "            [-1.8622e-03],\n",
       "            [-1.1231e-03],\n",
       "            [ 1.9822e-04],\n",
       "            [-2.1465e-03],\n",
       "            [ 4.3478e-04],\n",
       "            [ 1.6918e-03],\n",
       "            [-1.1772e-03],\n",
       "            [-2.2195e-05],\n",
       "            [-7.7037e-04],\n",
       "            [-2.0452e-03],\n",
       "            [ 1.9331e-03],\n",
       "            [ 6.7849e-04],\n",
       "            [-2.4374e-03],\n",
       "            [-9.2251e-04],\n",
       "            [ 9.3053e-04],\n",
       "            [ 3.8996e-03],\n",
       "            [ 1.7936e-03],\n",
       "            [ 3.4723e-03],\n",
       "            [ 1.9683e-03],\n",
       "            [ 2.2217e-03],\n",
       "            [ 3.9868e-03],\n",
       "            [-2.8343e-03],\n",
       "            [-9.2453e-04],\n",
       "            [-1.2666e-03],\n",
       "            [-4.0236e-03],\n",
       "            [-3.2799e-03],\n",
       "            [ 9.8861e-06],\n",
       "            [-4.2020e-04],\n",
       "            [-1.6683e-03],\n",
       "            [ 4.0981e-03],\n",
       "            [ 2.9084e-03],\n",
       "            [-1.0778e-03],\n",
       "            [ 1.9637e-04],\n",
       "            [-1.6896e-03],\n",
       "            [ 7.6126e-04],\n",
       "            [ 1.5627e-03],\n",
       "            [-2.6554e-03],\n",
       "            [-1.4614e-03],\n",
       "            [-8.2320e-04],\n",
       "            [ 2.5569e-04],\n",
       "            [-2.9827e-04],\n",
       "            [ 1.2553e-03],\n",
       "            [ 1.1094e-03],\n",
       "            [ 1.1391e-03],\n",
       "            [ 1.2863e-03],\n",
       "            [ 1.6047e-03],\n",
       "            [-4.8864e-04],\n",
       "            [ 1.9288e-03],\n",
       "            [-1.4559e-03],\n",
       "            [-3.9611e-03],\n",
       "            [ 5.2251e-04],\n",
       "            [-6.5004e-04],\n",
       "            [-9.5112e-04],\n",
       "            [ 7.2124e-04],\n",
       "            [-9.2894e-04],\n",
       "            [ 1.4210e-03],\n",
       "            [-1.9065e-03],\n",
       "            [ 1.2160e-04],\n",
       "            [ 9.1864e-04]],\n",
       "  \n",
       "           [[-1.7476e-03],\n",
       "            [-1.2939e-03],\n",
       "            [ 6.8796e-04],\n",
       "            [ 3.3912e-03],\n",
       "            [-1.3779e-03],\n",
       "            [ 2.1461e-04],\n",
       "            [-9.0123e-04],\n",
       "            [ 8.8814e-04],\n",
       "            [-1.8864e-03],\n",
       "            [-1.5598e-03],\n",
       "            [-1.8535e-03],\n",
       "            [ 8.5927e-04],\n",
       "            [-5.4456e-04],\n",
       "            [ 1.1173e-03],\n",
       "            [-4.5042e-04],\n",
       "            [-1.3123e-03],\n",
       "            [ 1.4039e-03],\n",
       "            [ 2.6648e-03],\n",
       "            [ 3.9458e-03],\n",
       "            [ 1.5234e-03],\n",
       "            [ 1.3229e-03],\n",
       "            [ 2.3254e-03],\n",
       "            [ 2.3362e-03],\n",
       "            [ 2.3167e-03],\n",
       "            [ 1.6073e-04],\n",
       "            [ 2.3889e-04],\n",
       "            [-1.1998e-03],\n",
       "            [ 7.7246e-04],\n",
       "            [-1.0039e-03],\n",
       "            [ 6.2068e-04],\n",
       "            [-2.3163e-04],\n",
       "            [-1.2898e-03],\n",
       "            [-8.5890e-04],\n",
       "            [ 1.5209e-03],\n",
       "            [-8.0890e-04],\n",
       "            [ 3.4803e-03],\n",
       "            [ 2.9582e-03],\n",
       "            [-3.6589e-04],\n",
       "            [-3.8682e-03],\n",
       "            [ 3.8294e-04],\n",
       "            [ 5.8284e-03],\n",
       "            [ 7.6976e-03],\n",
       "            [ 2.3573e-03],\n",
       "            [ 3.9125e-03],\n",
       "            [ 2.7939e-03],\n",
       "            [ 1.1098e-03],\n",
       "            [ 5.5652e-04],\n",
       "            [-2.2830e-03],\n",
       "            [-3.1241e-05],\n",
       "            [ 7.8873e-04],\n",
       "            [-2.3132e-03],\n",
       "            [-3.4050e-03],\n",
       "            [ 1.8433e-03],\n",
       "            [-9.3361e-03],\n",
       "            [-4.5011e-03],\n",
       "            [-2.8527e-03],\n",
       "            [-1.3837e-03],\n",
       "            [-1.2856e-03],\n",
       "            [ 9.1937e-04],\n",
       "            [-5.9588e-03],\n",
       "            [-9.4508e-04],\n",
       "            [-4.2905e-04],\n",
       "            [-1.4582e-03],\n",
       "            [ 2.9968e-04]]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       "  tensor([[[[-0.0486]],\n",
       "  \n",
       "           [[ 0.0763]],\n",
       "  \n",
       "           [[-0.0067]],\n",
       "  \n",
       "           [[-0.0025]]],\n",
       "  \n",
       "  \n",
       "          [[[-0.0486]],\n",
       "  \n",
       "           [[ 0.0763]],\n",
       "  \n",
       "           [[-0.0067]],\n",
       "  \n",
       "           [[-0.0025]]]], device='cuda:0', grad_fn=<MaximumBackward0>)),\n",
       " 'conv_state': (tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0172, -0.2684,  0.1761,  ...,  0.3680, -0.4299, -0.1354]],\n",
       "  \n",
       "          [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "           [ 0.5152, -0.8060, -0.8263,  ..., -0.0824,  0.8335, -0.6172]]],\n",
       "         device='cuda:0', grad_fn=<CopySlices>),)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states_dict['block_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f050a8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f2925ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4978,  1.5445,  1.2798, -1.4677, -2.1897,  0.7250,  0.6384,\n",
       "          -0.3543, -0.6553, -0.5294,  0.9895,  0.8925,  0.0369,  0.3600,\n",
       "           1.0671, -0.8967,  0.7769,  1.7208, -0.9602, -2.3994, -0.3734,\n",
       "          -0.0866,  1.4061,  0.0087, -0.7891, -0.0173, -1.9538, -0.3937,\n",
       "          -0.5488, -0.5184,  1.9376, -0.7643, -0.7506,  1.1501, -0.1561,\n",
       "           0.4513, -1.2406, -0.9217,  0.3434,  0.6700, -0.2392,  0.1344,\n",
       "          -0.0117, -0.9852, -1.2051,  0.3428, -0.1593, -1.8795, -0.0729,\n",
       "           1.1581,  0.8664,  0.4295, -0.0295,  0.2280,  1.7564,  0.3873,\n",
       "          -0.2153,  1.3113,  0.0999,  0.7173, -1.0597, -2.2170,  1.2809,\n",
       "          -1.7322,  0.5820,  1.3519,  0.9275, -0.4028,  0.1005,  0.5075,\n",
       "          -0.2481,  0.8091,  0.7399, -0.1949,  0.4818, -0.6697,  0.0183,\n",
       "           0.0428, -0.2187, -1.7713,  1.3467, -1.7573,  0.5467,  1.1787,\n",
       "           1.0028, -0.7336,  1.9220,  0.9596, -0.5030, -1.3327,  1.2682,\n",
       "           0.0318,  0.2944, -0.4946, -1.2384, -0.1656, -0.3940,  1.0485,\n",
       "          -0.4632, -1.1228, -0.5251,  0.5717,  0.2948, -0.4290,  0.0289,\n",
       "          -0.8129,  0.2868,  1.6683, -1.0464,  2.5269,  1.0870,  0.5450,\n",
       "           0.6013, -0.2129,  0.6708,  0.0966, -1.1242,  0.5395, -1.8758,\n",
       "          -0.4532, -1.4618,  0.8290, -1.4697, -1.6924,  1.1864, -0.0253,\n",
       "          -0.5767,  0.8827]],\n",
       "\n",
       "        [[ 0.9334, -1.2102, -0.0377, -0.9914,  1.1374, -0.6964, -2.1566,\n",
       "           0.0867, -1.2958, -0.8911, -0.4628, -0.1006,  1.1174, -1.4709,\n",
       "          -0.0618,  0.4479, -0.5596, -1.1242,  0.0741,  0.0120, -0.1888,\n",
       "           0.8209, -0.7181, -1.2087,  0.1952, -0.1759, -1.9716,  0.1519,\n",
       "           0.8163, -1.1344, -2.8472,  0.8609,  1.1940,  0.7629,  1.2135,\n",
       "           0.0656, -0.9970,  0.0897,  1.6817, -1.7808,  0.5168,  0.3522,\n",
       "          -1.0580,  1.0344,  0.2993,  1.1969,  1.0318,  0.2166, -0.1280,\n",
       "          -0.7411, -1.6344, -0.2734,  1.6382,  0.9097, -0.2570, -0.1459,\n",
       "           0.2190, -0.1522, -0.8757, -0.9675,  0.6123,  0.5670, -1.0976,\n",
       "           0.6741, -1.1917, -0.0072, -0.5910, -0.7187, -1.1439, -1.5196,\n",
       "           0.0703, -0.2792,  0.6984, -1.1594, -0.3775, -0.2540,  0.0112,\n",
       "           1.6391, -0.0964,  0.3535,  0.4311,  1.7643,  1.2465,  1.1254,\n",
       "          -0.5507,  1.0061,  1.1580,  0.6267,  0.5892,  0.0884,  0.1545,\n",
       "           0.9896,  0.4511,  1.7102,  0.8337, -0.1289, -1.5180,  0.6231,\n",
       "           0.0507, -1.5795, -1.3735,  2.8177,  0.8232, -0.5468,  0.5911,\n",
       "           0.0849, -1.4707,  2.3111, -0.5652,  0.2453,  0.1099,  1.5030,\n",
       "           0.7650,  0.2854,  1.1034, -0.7298, -0.8161, -2.6155,  0.7264,\n",
       "           0.3519,  1.5335,  0.7074, -0.1672, -0.3217, -0.4971, -0.0633,\n",
       "          -0.0332, -0.7820]]], device='cuda:0',\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "18339dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-8.8171e-01,  7.5984e-01,  1.3308e+00, -8.0911e-01, -7.1669e-01,\n",
       "           1.2324e-01,  6.4953e-01,  1.9917e-01, -2.8007e-02,  2.0094e-01,\n",
       "           8.0642e-01,  1.5618e+00, -3.5212e-01,  3.7325e-02,  9.7363e-01,\n",
       "          -6.5723e-01, -6.3216e-01, -2.7189e-01, -6.8714e-01, -2.0393e+00,\n",
       "          -3.4308e-02,  9.7309e-01, -2.3616e-01, -3.9586e-01, -6.4092e-02,\n",
       "           9.2531e-01, -2.7790e+00, -6.8198e-01, -1.6832e-02, -2.0715e-01,\n",
       "           2.5983e+00, -7.3946e-01, -1.0449e+00,  7.0546e-01,  1.0123e+00,\n",
       "           7.0937e-01, -5.6492e-01, -1.9830e+00, -2.7441e-01,  8.7571e-01,\n",
       "           1.7398e-02,  9.0983e-01,  1.3253e-01,  4.8852e-02, -1.4840e+00,\n",
       "          -2.9523e-02,  1.3925e+00,  1.0114e-01,  3.2748e-01,  1.0978e+00,\n",
       "          -8.5351e-01, -3.0628e-01, -4.6161e-01, -3.8436e-01,  1.0335e+00,\n",
       "           6.7830e-01, -3.3133e-01,  6.6896e-01, -4.6368e-01,  7.4131e-01,\n",
       "          -1.6848e+00, -1.2495e+00,  1.8919e+00, -6.0618e-01,  2.2224e+00,\n",
       "           9.6635e-01,  4.2401e-01,  1.1269e+00, -1.1653e-01,  1.1286e+00,\n",
       "          -1.2535e+00,  8.7811e-01,  6.8122e-01,  1.4079e+00, -6.4005e-01,\n",
       "          -4.5989e-01,  1.5494e-01,  2.7774e-03,  6.3177e-02, -1.0466e+00,\n",
       "           5.7080e-01, -9.7741e-01, -3.1996e-01,  1.3899e+00,  1.0370e+00,\n",
       "          -6.5162e-01,  4.1860e-01,  1.9799e+00,  1.0571e+00, -1.4456e+00,\n",
       "           1.4473e+00, -7.6656e-01,  7.1982e-01, -5.5781e-01, -1.1384e-01,\n",
       "          -8.1582e-01, -2.9180e-01,  9.1907e-01, -7.8861e-01, -9.0185e-01,\n",
       "           1.9076e-01, -6.4722e-01, -1.4857e-02,  1.6904e-01,  1.0597e+00,\n",
       "           6.2165e-01, -2.3145e-02,  3.5268e-01, -7.2788e-01,  2.1487e+00,\n",
       "           9.6088e-01,  1.2678e+00,  4.6572e-01, -1.4514e+00,  8.1127e-02,\n",
       "           6.4038e-01, -8.9556e-01,  4.9515e-02, -2.6099e-01, -7.8746e-01,\n",
       "          -2.9623e-01,  5.9028e-01, -1.9566e+00, -5.5886e-01,  1.3593e+00,\n",
       "           1.6644e-01, -7.0879e-01,  5.1795e-01]],\n",
       "\n",
       "        [[-6.9481e-01, -3.3496e-01, -2.7828e+00, -8.0376e-01,  4.3376e-01,\n",
       "          -1.0741e+00, -1.5963e+00, -8.8417e-01, -6.5766e-01, -5.2060e-01,\n",
       "          -1.0565e+00, -8.1344e-01,  7.3373e-01, -1.1279e+00, -1.2273e+00,\n",
       "          -7.7926e-01,  4.7679e-02, -7.0081e-01,  2.5808e-01, -3.0838e-01,\n",
       "          -6.4613e-01,  1.6163e+00, -9.1016e-01, -8.0878e-01, -6.8357e-02,\n",
       "          -1.4478e-01, -1.0830e+00,  5.8447e-01, -1.6431e-01, -3.1648e-01,\n",
       "          -2.2837e+00,  6.7378e-01,  3.5603e-01, -5.4729e-01,  1.6685e+00,\n",
       "           4.0609e-01,  2.6830e-01,  1.5550e+00,  1.6027e+00, -1.4875e+00,\n",
       "           1.3778e-03, -7.5260e-01, -7.1929e-01,  9.8138e-01, -3.0450e-01,\n",
       "           5.9015e-01,  1.0246e+00,  6.9005e-01,  3.4637e-03, -8.3053e-01,\n",
       "           4.4276e-01, -4.4566e-01,  1.1101e+00, -4.1343e-02,  6.3975e-01,\n",
       "          -1.0715e+00, -3.8311e-01, -8.0375e-01,  3.5566e-01, -3.7449e-02,\n",
       "           8.8641e-01, -8.9410e-01, -1.2865e+00,  5.0383e-01, -1.0347e-01,\n",
       "           1.0908e-01, -1.6730e+00, -4.2485e-01, -4.7244e-01,  3.3324e-01,\n",
       "           5.4684e-01, -4.0130e-02,  1.0211e+00, -3.9064e-01,  1.2668e+00,\n",
       "          -7.2437e-01,  1.2207e+00,  1.2659e-01, -7.8922e-01,  1.0378e-01,\n",
       "          -1.2252e-01,  1.2038e+00,  1.1310e+00,  2.1365e-01, -1.8951e-01,\n",
       "          -2.4226e-01,  5.3730e-01,  6.2443e-01,  1.8596e-01,  8.4706e-01,\n",
       "           6.7549e-01,  7.2867e-01,  5.5951e-01,  1.9297e+00, -4.9332e-01,\n",
       "          -8.3681e-02, -1.1987e+00,  6.4985e-01,  9.8057e-02, -1.4477e+00,\n",
       "          -7.4657e-01,  1.5699e+00, -1.2217e-02, -1.7952e-01,  3.4638e-01,\n",
       "          -6.4114e-01, -1.3560e-01,  1.3614e+00,  4.8611e-01, -7.4796e-01,\n",
       "           1.8114e-01,  1.2593e+00,  9.7829e-02, -6.6460e-02,  1.6142e+00,\n",
       "          -5.4310e-01,  2.1800e-02, -1.9007e+00, -8.7532e-01, -2.6255e-01,\n",
       "           2.4655e-01,  1.8631e+00, -1.2244e-01, -9.0577e-01, -9.0693e-01,\n",
       "          -1.1550e-02,  9.7653e-01, -6.5556e-02]]], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 1, :].view(2,1,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "941e9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = xlstm_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2a517df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8839,  0.6531, -0.8390, -0.1645,  1.6704,  0.5127, -0.0529,  0.2271,\n",
       "         -2.5904,  1.3150,  0.3769, -0.1745,  0.2888,  0.9407, -0.1206,  0.6557,\n",
       "          0.1330,  0.1118,  0.4273,  0.5373, -0.3927, -0.0368, -0.3819,  1.1103,\n",
       "          0.2225,  0.5210, -0.3059, -0.2255, -0.9224, -0.4568, -2.2560,  1.0137,\n",
       "         -0.1084, -0.6037,  0.5965, -0.5597, -0.5377, -0.5736,  0.8941, -2.7295,\n",
       "          1.3067,  0.7518,  0.9739,  0.5398, -0.4596,  0.3005, -0.3543, -0.9072,\n",
       "          0.3558,  0.1623,  0.5433, -0.9490,  0.1389, -1.0022, -1.1491, -0.3044,\n",
       "         -0.1395, -0.2475, -0.1755,  0.2795,  1.3190,  0.0260, -0.0259, -0.6402,\n",
       "         -0.3443, -1.1712,  0.2273, -0.3507,  1.4633, -0.2927,  1.0462,  1.0782,\n",
       "          0.0841, -0.4016,  0.4079,  0.1693,  0.1225,  0.6326,  0.0118, -1.4028,\n",
       "         -1.1295,  0.6233,  0.1547,  0.0442, -0.7434, -0.5714, -0.8025,  0.6202,\n",
       "          0.8696,  1.3488,  0.3080,  0.0314, -0.9079, -0.9560, -0.4306,  0.1162,\n",
       "         -0.6938,  0.3198, -0.6172, -0.0802,  0.4478,  0.6517,  0.3259, -0.6607,\n",
       "         -0.0058, -0.8933, -1.3278, -0.9682, -1.3721, -0.1751, -1.1058,  0.5873,\n",
       "          1.0897, -0.5731, -0.8285,  0.6343,  0.3317, -1.0103,  0.2368,  1.2283,\n",
       "         -0.5585, -1.4380, -0.4125,  0.0473, -0.3615, -0.7621, -2.7164, -0.4451],\n",
       "        [-0.3827,  0.6727, -0.3629,  0.4629,  0.0308,  0.8585, -1.8790, -0.6913,\n",
       "         -0.9136, -0.2676, -1.9039,  0.3693, -1.2268, -1.0328, -0.0491,  0.6238,\n",
       "         -1.5192, -1.8714,  1.8139, -0.1238,  0.2295, -0.6171,  0.7421,  0.6322,\n",
       "          0.7471, -0.9088, -0.9387,  0.0782, -0.1878,  0.5642, -0.6711,  0.9300,\n",
       "          0.2423, -0.0376,  0.4959,  0.3500,  0.3852,  2.1136,  0.1459,  0.8818,\n",
       "         -0.6836, -0.0776,  0.5029, -0.0239,  0.6573, -0.1307, -0.1154, -0.9019,\n",
       "         -0.5382,  0.7179,  0.3475,  1.2513, -0.8723,  1.6007,  0.4090, -1.0772,\n",
       "          1.9917, -1.8331,  1.0468,  1.0652, -0.4125,  0.2769,  0.0759, -1.5849,\n",
       "          0.8636,  0.5630,  1.4489,  0.5942, -1.9542,  0.9873, -0.1048,  0.2065,\n",
       "          1.6200,  1.4928, -1.3015, -1.9462,  1.8560,  0.9927, -0.9948, -2.2841,\n",
       "         -0.5198, -0.2991,  1.0433, -0.7337, -0.5173, -0.4010, -1.0583, -1.5304,\n",
       "         -0.6108, -0.7000, -1.5357,  0.1049, -0.3842, -0.3203,  0.3014, -1.0532,\n",
       "         -0.2254, -1.3821, -0.3260,  0.1846,  1.9155, -0.7575,  0.2046, -1.1032,\n",
       "         -0.3164, -0.4936, -1.0507, -2.2490,  1.4500, -0.6602,  0.6414,  0.6353,\n",
       "          1.8056,  0.6655, -1.1536, -1.6325,  0.5948,  1.4622, -0.8092,  0.4165,\n",
       "          0.4700, -0.3251, -0.3940,  0.7203,  1.4847, -0.2178, -0.5617,  0.9877]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3a376563",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.8777e-01,  7.0733e-01, -1.1354e+00,  6.8297e-02,  1.2867e+00,\n",
       "          8.6632e-01,  8.6892e-01,  9.2484e-01, -1.2396e+00,  1.7039e+00,\n",
       "         -1.0790e+00, -1.0183e+00, -6.6037e-01,  3.6460e-01,  9.4218e-01,\n",
       "         -9.8400e-02, -1.5814e+00,  4.7198e-01,  1.7564e+00,  1.6153e+00,\n",
       "          2.3493e-01,  2.9874e-01, -6.3176e-01,  7.8221e-01,  2.1035e+00,\n",
       "          3.4908e-01, -2.4726e+00,  9.4857e-01, -6.0690e-01,  6.2530e-01,\n",
       "         -4.8214e+00,  1.9170e+00, -3.0467e-01, -5.0227e-01,  8.7043e-01,\n",
       "         -4.5736e-01,  4.8147e-03, -5.5708e-01,  2.0257e-01, -2.0661e+00,\n",
       "          9.1403e-01,  1.8090e+00,  2.6168e-01,  1.3294e+00, -2.0494e+00,\n",
       "         -1.7052e+00, -5.7928e-01, -7.9424e-01,  9.7212e-01,  2.6963e-01,\n",
       "          2.0599e+00,  3.2527e-02,  1.6426e+00, -1.7454e+00,  1.4678e+00,\n",
       "         -6.0697e-01,  5.2550e-01,  4.7229e-01, -5.6340e-01, -1.4185e+00,\n",
       "          1.2872e+00, -1.8675e+00, -5.2358e-01, -6.4815e-01, -3.6819e-01,\n",
       "         -1.1029e+00,  3.5903e-01, -6.5322e-01,  2.1855e+00, -9.8429e-01,\n",
       "          4.7338e-02,  1.9631e+00,  2.3712e-01, -3.2350e-01,  1.0997e+00,\n",
       "         -1.3424e+00,  4.0225e-01,  2.9826e+00,  1.8755e+00,  3.0315e-01,\n",
       "         -2.6439e+00,  1.1850e+00, -1.6624e-01,  2.4756e+00, -1.8235e+00,\n",
       "         -1.2479e+00, -3.8709e-01, -3.6853e-03,  4.8232e-01,  9.0048e-01,\n",
       "         -4.8087e-01, -1.9007e+00, -8.6864e-01, -1.6412e+00, -2.4140e-01,\n",
       "         -1.2030e-01, -5.9634e-01,  2.2609e-01, -8.4867e-01, -5.4964e-01,\n",
       "          9.4604e-01,  1.1193e-01,  1.7950e+00, -7.7007e-01, -2.2277e+00,\n",
       "         -1.0216e+00, -2.3193e+00, -7.6367e-01, -9.2248e-01, -4.1240e-01,\n",
       "         -1.2538e+00, -4.0597e-01,  2.0933e+00, -3.2336e-01, -1.0468e+00,\n",
       "          1.2108e+00,  1.7244e+00, -1.7337e+00, -6.0633e-02,  2.0858e+00,\n",
       "         -2.4479e+00, -3.4970e+00, -1.5893e+00,  2.1653e+00,  1.1738e+00,\n",
       "         -5.0309e-01, -1.7179e+00,  8.2249e-01],\n",
       "        [-7.7607e-01,  7.9096e-01, -4.5531e-01,  1.3640e+00, -4.1790e-01,\n",
       "          1.6173e+00, -9.0503e-01, -1.1564e+00, -1.6561e+00,  5.8542e-01,\n",
       "         -2.8300e+00,  3.3120e-02,  4.6051e-02, -5.8380e-01, -2.8246e-01,\n",
       "          6.9055e-01, -1.9752e+00, -1.8799e+00,  4.1304e+00,  7.7659e-01,\n",
       "         -1.0225e+00,  1.2650e+00, -6.0231e-01,  1.1616e+00,  1.4708e+00,\n",
       "         -1.5379e+00, -8.1291e-01, -2.2182e+00,  1.1417e+00,  5.1248e-03,\n",
       "         -2.2913e+00,  3.8535e-01, -2.8455e-01, -9.9707e-01,  1.1484e-01,\n",
       "          2.4217e-01,  5.8775e-02,  1.9382e+00, -3.7781e-01, -1.2148e+00,\n",
       "         -1.1922e+00,  1.6484e+00,  6.2771e-01, -1.3431e+00,  2.3404e+00,\n",
       "          7.5461e-01,  1.0526e+00,  1.2947e+00,  2.4429e-02,  3.4572e-01,\n",
       "          7.3915e-01,  1.9359e-01, -1.1139e+00,  3.7749e+00,  2.4437e-01,\n",
       "         -9.9601e-01,  3.3257e+00, -1.9338e+00, -2.0381e-01,  7.6811e-01,\n",
       "         -7.5693e-01, -6.0249e-02, -4.9941e-01, -2.4407e+00,  9.3638e-01,\n",
       "          5.4791e-01,  1.1277e+00,  8.8687e-01, -2.1569e+00, -9.6181e-02,\n",
       "          7.3453e-02,  1.2264e-01,  7.5122e-01,  3.0090e+00, -6.6579e-01,\n",
       "         -2.5155e+00,  3.2339e+00, -9.9963e-01,  3.2716e-01, -1.9679e+00,\n",
       "         -4.3306e-01,  2.8259e-01,  1.6089e+00,  3.1172e-01, -2.4559e+00,\n",
       "         -9.8379e-01,  8.5528e-01, -1.4282e+00, -1.1079e+00, -2.0211e+00,\n",
       "          5.8212e-01, -8.9954e-01, -1.2386e+00, -2.2073e-01,  5.1236e-01,\n",
       "         -1.5990e+00,  9.2565e-03, -1.9688e+00, -4.5214e-01, -2.4236e-01,\n",
       "          1.6053e+00, -1.0878e+00,  2.6744e-01, -1.8616e+00, -1.2423e+00,\n",
       "         -2.5155e-01, -2.4363e+00, -3.7161e+00,  3.0693e+00, -1.8987e+00,\n",
       "          4.4557e-01,  1.0513e+00,  1.7520e+00,  9.4038e-01,  4.9629e-01,\n",
       "         -2.7692e+00,  1.9345e+00,  2.8626e+00,  6.3561e-01,  2.0574e+00,\n",
       "         -4.8814e-01, -3.9396e-01, -7.7253e-01,  6.5247e-01,  2.6302e+00,\n",
       "         -8.7039e-01,  7.7251e-01, -6.6832e-01]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "df10283a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 128])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9840dbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.8577, -1.1099, -0.3923,  ...,  0.0940,  0.0937,  0.7333],\n",
      "         [ 0.8142,  0.0359,  0.3861,  ..., -0.1010, -0.7600, -0.0066],\n",
      "         [ 2.4055, -1.4971,  0.7610,  ..., -0.5647,  1.4988, -1.2182],\n",
      "         ...,\n",
      "         [-1.9958,  0.2881,  0.2207,  ...,  0.1564, -1.6645, -0.3780],\n",
      "         [-0.4549,  0.3158, -0.3409,  ...,  0.4510, -0.3832,  1.3363],\n",
      "         [-0.8845, -0.6907,  0.5125,  ...,  1.1808, -1.4142,  1.1701]],\n",
      "\n",
      "        [[ 0.8740, -2.0196,  0.3545,  ..., -0.7747, -0.1946, -1.2063],\n",
      "         [-0.3995,  0.4758,  0.2324,  ...,  1.6014, -0.7852,  0.3335],\n",
      "         [ 0.2124, -0.1924, -0.0830,  ...,  0.5713,  1.6000,  1.5129],\n",
      "         ...,\n",
      "         [-0.1742,  1.3780, -0.5989,  ...,  0.0386, -0.5372,  1.5216],\n",
      "         [ 0.4534,  0.9240, -0.4873,  ...,  0.6691, -0.8149,  1.2498],\n",
      "         [-0.2121, -0.4231, -1.2146,  ...,  0.4834,  1.7986, -0.7707]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pprint(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9fa31d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1480, -1.0595,  2.0452, -0.0863, -0.1370,  2.0175, -0.4361,\n",
      "          -0.1611, -1.5872,  1.1023, -2.6816,  0.6252,  1.0867, -1.2995,\n",
      "          -0.2884,  1.1804,  1.3891, -1.3155,  0.1274, -3.0621, -4.0834,\n",
      "           1.0835,  0.5426,  0.4090,  1.8845,  0.7048, -2.5606, -0.6336,\n",
      "          -0.5072, -0.1934, -0.1107,  4.0092, -0.6715, -0.6509, -1.7275,\n",
      "           0.2459,  0.1660,  1.4571, -1.2948, -2.2396, -1.9346, -0.5495,\n",
      "           1.1377, -0.1067,  0.4261, -0.1087, -0.5682,  5.4285,  0.8706,\n",
      "           1.8102, -1.4242,  0.7603, -0.2171, -1.3306, -0.8624,  1.6319,\n",
      "          -0.9979, -0.4409,  0.2527, -0.4009, -2.9918, -2.7180,  0.6599,\n",
      "          -0.0263, -0.8976,  1.0564,  0.7353, -0.1279, -0.2253, -0.8580,\n",
      "           1.5953, -1.0426,  2.8536, -2.9724, -2.6748,  0.6674,  0.2355,\n",
      "           0.4488,  3.3318, -1.6286,  0.3944, -1.5674,  0.0701,  0.0965,\n",
      "           2.1487, -0.0439, -0.0311, -1.9621,  2.6495,  1.4265,  1.3333,\n",
      "           0.7164, -3.4484, -1.8838, -0.6050,  2.5486, -0.8059, -0.5637,\n",
      "           0.1255,  0.2867,  0.6235,  2.1331, -2.1081, -0.1586, -1.8509,\n",
      "          -0.0214,  0.9245,  0.3611, -0.9547,  1.1558, -1.0693, -2.4451,\n",
      "           0.7093, -0.5890,  0.1410,  1.3126, -0.6508, -1.4434, -0.5006,\n",
      "          -2.7367, -0.5864, -0.4888,  0.2231, -1.1984,  1.2541,  2.6641,\n",
      "          -1.4962,  0.1091],\n",
      "         [-2.0870, -0.0983,  0.5668, -0.4324,  1.0654, -0.6384, -0.2609,\n",
      "          -1.4670,  0.3880, -1.5677, -0.0985, -1.0433, -0.4949,  0.2063,\n",
      "           0.7126,  1.2308,  0.3282, -0.0658, -0.0805,  2.8441, -1.4153,\n",
      "           0.3099,  3.3654, -0.2192,  0.3791, -1.0280, -1.2306,  2.9627,\n",
      "          -1.2756, -0.4061, -2.5895, -3.1035, -1.0896,  0.1018, -1.4100,\n",
      "          -0.5312,  1.4615, -1.0449, -1.3888, -1.6502,  0.1526, -2.6744,\n",
      "          -0.1781,  0.7281, -0.7773, -1.0685, -0.9596, -1.4403, -0.9191,\n",
      "          -0.0148, -0.6152, -0.9176, -1.8729, -1.5484, -0.6618,  1.5877,\n",
      "           1.5973,  0.3003, -0.8633,  1.5330, -1.6578, -0.2430,  2.1612,\n",
      "           2.7347, -0.1967,  0.4263, -0.0972, -1.3968,  0.1985,  2.1294,\n",
      "          -0.5306,  1.1960, -0.5846, -0.6870,  0.1140, -1.5758,  1.1071,\n",
      "          -1.2173, -0.5227,  0.0138, -3.1062,  2.4040, -1.9325, -1.3027,\n",
      "          -0.0821, -0.2135, -1.1811, -0.1238, -0.4193, -3.6282,  2.0588,\n",
      "           1.2508, -0.2063,  0.7860, -1.3420, -0.5838,  1.5116, -0.7512,\n",
      "           0.1340,  0.4318,  0.0472, -1.5990,  0.3333,  1.4573, -0.1115,\n",
      "           1.3645, -1.3308,  1.7445, -0.5798,  0.1030,  2.8135,  1.4825,\n",
      "          -0.9390,  0.0997,  1.4260,  1.1132, -1.6760,  1.1526,  1.2560,\n",
      "          -2.2271,  2.4074,  1.8287, -0.6618, -1.5050,  1.4916,  0.1344,\n",
      "           0.9490, -2.4933]]], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pprint(y[:, -1, :].view(1, -1, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "93cac937",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ()\n",
    "for i in range(5):\n",
    "    t += (i,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8dd057cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a162b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "rnn = nn.LSTM(10, 20, 2, batch_first=True)\n",
    "inpt = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 5, 20)\n",
    "c0 = torch.randn(2, 5, 20)\n",
    "output, (hn, cn) = rnn(inpt, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f528b23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 20])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ea8955b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 20])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cb41ca31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2291,  0.0706, -0.2873, -0.0193,  0.0982,  0.1942,  0.0228, -0.1276,\n",
       "          0.0790,  0.0730, -0.1216,  0.0562,  0.1600,  0.1633, -0.0206, -0.0394,\n",
       "         -0.0367, -0.0733,  0.1518, -0.1802],\n",
       "        [-0.1929,  0.1946,  0.0548, -0.1062,  0.0587,  0.0891, -0.0085, -0.0888,\n",
       "          0.0230,  0.2114,  0.1167,  0.1953,  0.0377, -0.0480, -0.0815,  0.1854,\n",
       "          0.0207, -0.0434, -0.0672, -0.1421],\n",
       "        [-0.2902,  0.2361, -0.2112, -0.0509, -0.0608,  0.1652, -0.0098, -0.1355,\n",
       "          0.1445,  0.1078,  0.0399,  0.1191,  0.0123, -0.0428,  0.0851, -0.0684,\n",
       "         -0.2345, -0.0469,  0.2690,  0.1379],\n",
       "        [-0.2975,  0.0864,  0.1136, -0.1358,  0.0094,  0.1324,  0.0084, -0.1017,\n",
       "         -0.1106,  0.1565, -0.1292,  0.1546, -0.0853, -0.1034,  0.0358,  0.1535,\n",
       "         -0.0611, -0.0737, -0.1640, -0.1349],\n",
       "        [-0.0851,  0.1141,  0.2472, -0.2493, -0.1072,  0.0791, -0.1482, -0.1104,\n",
       "          0.1480,  0.1121,  0.0041,  0.1225,  0.1576,  0.0570, -0.0797, -0.0238,\n",
       "         -0.1713, -0.0056,  0.1925,  0.1231]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8205b2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2291,  0.0706, -0.2873, -0.0193,  0.0982,  0.1942,  0.0228, -0.1276,\n",
       "          0.0790,  0.0730, -0.1216,  0.0562,  0.1600,  0.1633, -0.0206, -0.0394,\n",
       "         -0.0367, -0.0733,  0.1518, -0.1802],\n",
       "        [-0.1929,  0.1946,  0.0548, -0.1062,  0.0587,  0.0891, -0.0085, -0.0888,\n",
       "          0.0230,  0.2114,  0.1167,  0.1953,  0.0377, -0.0480, -0.0815,  0.1854,\n",
       "          0.0207, -0.0434, -0.0672, -0.1421],\n",
       "        [-0.2902,  0.2361, -0.2112, -0.0509, -0.0608,  0.1652, -0.0098, -0.1355,\n",
       "          0.1445,  0.1078,  0.0399,  0.1191,  0.0123, -0.0428,  0.0851, -0.0684,\n",
       "         -0.2345, -0.0469,  0.2690,  0.1379],\n",
       "        [-0.2975,  0.0864,  0.1136, -0.1358,  0.0094,  0.1324,  0.0084, -0.1017,\n",
       "         -0.1106,  0.1565, -0.1292,  0.1546, -0.0853, -0.1034,  0.0358,  0.1535,\n",
       "         -0.0611, -0.0737, -0.1640, -0.1349],\n",
       "        [-0.0851,  0.1141,  0.2472, -0.2493, -0.1072,  0.0791, -0.1482, -0.1104,\n",
       "          0.1480,  0.1121,  0.0041,  0.1225,  0.1576,  0.0570, -0.0797, -0.0238,\n",
       "         -0.1713, -0.0056,  0.1925,  0.1231]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd80ab75",
   "metadata": {},
   "source": [
    "# Model prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a64aaa",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd89b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xLSTMtime(nn.Module):\n",
    "    def __init__(self, input_size, output_size, xlstm_blockstack_config):\n",
    "        super(xLSTMtime, self).__init__()\n",
    "        self.fc_in = nn.Linear(in_features=input_size, out_features=xlstm_blockstack_config.embedding_dim)\n",
    "        self.xlstm = xLSTMBlockStack(xlstm_blockstack_config)\n",
    "        self.fc_out = nn.Linear(in_features=xlstm_blockstack_config.embedding_dim, out_features=output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_in(x)\n",
    "        x = self.xlstm(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8084c46",
   "metadata": {},
   "source": [
    "## Test config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24440399",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cfg_str = f\"\"\" \n",
    "mlstm_block:\n",
    "  mlstm:\n",
    "    conv1d_kernel_size: 4\n",
    "    qkv_proj_blocksize: 4\n",
    "    num_heads: 4\n",
    "slstm_block:\n",
    "  slstm:\n",
    "    backend: {'cuda' if torch.cuda.is_available() else 'vanilla'} #! only vanilla here works\n",
    "    num_heads: 4\n",
    "    conv1d_kernel_size: 4\n",
    "    bias_init: powerlaw_blockdependent\n",
    "  feedforward:\n",
    "    proj_factor: 1.3\n",
    "    act_fn: gelu\n",
    "context_length: 50\n",
    "num_blocks: 3\n",
    "embedding_dim: 16\n",
    "add_post_blocks_norm: False\n",
    "slstm_at: [1] #[1] # for [] it also works, so if no sLSTM is in the stack\n",
    "\"\"\"\n",
    "\n",
    "test_cfg = OmegaConf.create(test_cfg_str)\n",
    "test_cfg = from_dict(data_class=xLSTMBlockStackConfig, data=OmegaConf.to_container(test_cfg), config=DaciteConfig(strict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0d4386",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "077da337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verbose': True, 'with_cuda': True, 'extra_ldflags': ['-L/home/nick/anaconda3/envs/nxai_xlstm/lib', '-lcublas'], 'extra_cflags': ['-DSLSTM_HIDDEN_SIZE=16', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__'], 'extra_cuda_cflags': ['-Xptxas=\"-v\"', '-gencode', 'arch=compute_80,code=compute_80', '-res-usage', '--use_fast_math', '-O3', '-Xptxas -O3', '--extra-device-vectorization', '-DSLSTM_HIDDEN_SIZE=16', '-DSLSTM_BATCH_SIZE=8', '-DSLSTM_NUM_HEADS=4', '-DSLSTM_NUM_STATES=4', '-DSLSTM_DTYPE_B=float', '-DSLSTM_DTYPE_R=__nv_bfloat16', '-DSLSTM_DTYPE_W=__nv_bfloat16', '-DSLSTM_DTYPE_G=__nv_bfloat16', '-DSLSTM_DTYPE_S=__nv_bfloat16', '-DSLSTM_DTYPE_A=float', '-DSLSTM_NUM_GATES=4', '-DSLSTM_SIMPLE_AGG=true', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL_VALID=false', '-DSLSTM_GRADIENT_RECURRENT_CLIPVAL=0.0', '-DSLSTM_FORWARD_CLIPVAL_VALID=false', '-DSLSTM_FORWARD_CLIPVAL=0.0', '-U__CUDA_NO_HALF_OPERATORS__', '-U__CUDA_NO_HALF_CONVERSIONS__', '-U__CUDA_NO_BFLOAT16_OPERATORS__', '-U__CUDA_NO_BFLOAT16_CONVERSIONS__', '-U__CUDA_NO_BFLOAT162_OPERATORS__', '-U__CUDA_NO_BFLOAT162_CONVERSIONS__']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/nick/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/nick/.cache/torch_extensions/py311_cu121/slstm_HS16BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0/build.ninja...\n",
      "Building extension module slstm_HS16BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module slstm_HS16BS8NH4NS4DBfDRbDWbDGbDSbDAfNG4SA1GRCV0GRC0d0FCV0FC0d0...\n"
     ]
    }
   ],
   "source": [
    "xlstmtime = xLSTMtime(\n",
    "    input_size=1,\n",
    "    output_size=1,\n",
    "    xlstm_blockstack_config=test_cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35defdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlstmtime = xlstmtime.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753fb551",
   "metadata": {},
   "source": [
    "## Test input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67ebfba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(2, 50, 1).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acbc4735",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = xlstmtime(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c81ba2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[:,-1,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8382a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3219],\n",
       "        [-1.0757]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input[:,-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2dc48",
   "metadata": {},
   "source": [
    "## Test dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91860069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# await get_historical_data(\n",
    "#     output_filename='yndx_20170901_20200901.csv',\n",
    "#     stock='YNDX',\n",
    "#     start_date='2017-09-01',\n",
    "#     end_date='2020-09-01'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d5d8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = (\n",
    "    pd\n",
    "    .read_csv('yndx_20170901_20200901.csv')\n",
    "    .sort_values(by=['TRADEDATE'], ascending=[True])\n",
    "    ['OPEN']\n",
    "    .to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "271bb3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "756"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e97c66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_examples = []\n",
    "y_examples = []\n",
    "for i in range(len(test_data) - 50 - 1):\n",
    "    X_examples.append(test_data[i:i+50])\n",
    "    y_examples.append(test_data[i+50])\n",
    "X_examples = np.array(X_examples)\n",
    "y_examples = np.array(y_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd1749bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1746.5, 1817. , 1831.5, ..., 1944. , 1933.5, 1890. ],\n",
       "       [1817. , 1831.5, 1861.5, ..., 1933.5, 1890. , 1856. ],\n",
       "       [1831.5, 1861.5, 1849. , ..., 1890. , 1856. , 1874.5],\n",
       "       ...,\n",
       "       [2984.2, 3024.8, 3058.6, ..., 4800. , 4920. , 5060.4],\n",
       "       [3024.8, 3058.6, 3108. , ..., 4920. , 5060.4, 4993.6],\n",
       "       [3058.6, 3108. , 3170.2, ..., 5060.4, 4993.6, 4870.2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7852ff94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1856. , 1874.5, 1890. , 1889. , 1926.5, 1965. , 2001. , 2030. ,\n",
       "       1980.5, 1981.5, 2033. , 2019. , 2002.5, 1949.5, 1935. , 1932. ,\n",
       "       1900. , 1899.5, 1920. , 1949.5, 1960. , 1996. , 1935. , 1935. ,\n",
       "       1915. , 1895. , 1892. , 1898. , 1885.5, 1865. , 1832.5, 1852. ,\n",
       "       1845. , 1867.5, 1880. , 1919. , 1943. , 1966. , 1957.5, 1963. ,\n",
       "       1963.5, 1955. , 1958. , 1964. , 1993. , 2050. , 2112. , 2095.5,\n",
       "       2131.5, 2159.5, 2138.5, 2135. , 2191.5, 2176.5, 2160.5, 2183. ,\n",
       "       2190.5, 2145.5, 2073. , 2106. , 2137. , 2090.5, 2101.5, 2124. ,\n",
       "       2155. , 2250. , 2341.5, 2371. , 2420. , 2424.5, 2469. , 2450. ,\n",
       "       2363. , 2381.5, 2329.5, 2309. , 2313.5, 2377.5, 2399. , 2474. ,\n",
       "       2465. , 2449. , 2425. , 2442.5, 2441. , 2435. , 2427.5, 2465. ,\n",
       "       2487.5, 2390. , 2377. , 2360.5, 2339.5, 2279. , 2288.5, 2290. ,\n",
       "       2250. , 2255. , 2264.5, 2313. , 2310. , 2090.5, 2168.5, 2211. ,\n",
       "       2139.5, 2051. , 2148.5, 2080. , 2160. , 2111. , 2095. , 2142. ,\n",
       "       2090.5, 2051. , 2075. , 2035. , 2035.5, 2106.5, 2203.5, 2149. ,\n",
       "       2115.5, 2152.5, 2140.5, 2181. , 2196. , 2172.5, 2126. , 2123. ,\n",
       "       2120. , 2123. , 2149. , 2100. , 2103.5, 2084.5, 2089. , 2102. ,\n",
       "       2059.5, 2062.5, 2088.5, 2132.5, 2188.5, 2216. , 2234.5, 2159.5,\n",
       "       2147. , 2164. , 2252.5, 2289. , 2300. , 2255.5, 2310. , 2295. ,\n",
       "       2304.5, 2257. , 2232. , 2179. , 2197.5, 2202.5, 2206. , 2238.5,\n",
       "       2280. , 2243. , 2230. , 2266.5, 2320. , 2330. , 2299.5, 2302.5,\n",
       "       2376. , 2383. , 2333.5, 2419.5, 2390. , 2341. , 2338.5, 2354. ,\n",
       "       2363. , 2410.5, 2384.5, 2350. , 2240.5, 2257. , 2266. , 2260.5,\n",
       "       2260. , 2241. , 2255. , 2142. , 2134. , 2140. , 2165. , 2142. ,\n",
       "       2105.5, 2123.5, 2108. , 2125. , 2163.5, 2150. , 2118. , 2142. ,\n",
       "       2158. , 2159. , 2170. , 2132. , 2159. , 2183.5, 2170. , 2122.5,\n",
       "       2080. , 2100.5, 2123.5, 2097. , 2070.5, 2145. , 2192. , 2117. ,\n",
       "       2140. , 2152.5, 2178. , 2202.5, 2196.5, 2213. , 2194. , 2195. ,\n",
       "       2168. , 2136. , 2143.5, 2185.5, 2151. , 2143. , 2197.5, 2219.5,\n",
       "       2111.5, 2130. , 2152. , 2178. , 2308. , 2322. , 2012. , 1830. ,\n",
       "       1764.5, 1769.5, 1749.5, 1770. , 1777. , 1829.5, 1875. , 1965.5,\n",
       "       2006.5, 2000.5, 1999.5, 1987.5, 1954.5, 1928.5, 1886. , 1905.5,\n",
       "       1940. , 1933. , 1927. , 1915. , 1879.5, 1924.5, 1912. , 1894.5,\n",
       "       1924. , 1949.5, 1976. , 1950. , 1975. , 1995. , 1949.5, 1936. ,\n",
       "       1913.5, 1911.5, 1890. , 1849.5, 1900. , 1869. , 1868.5, 1850. ,\n",
       "       1850. , 1850. , 1860. , 1850. , 1829.5, 1830. , 1870. , 1894. ,\n",
       "       1920.5, 1918.5, 1861.5, 1933. , 1936.5, 1970.5, 1983. , 1985. ,\n",
       "       1993.5, 1997.5, 1979.5, 2024. , 2048.5, 2033. , 2039. , 2057. ,\n",
       "       2099.5, 2170. , 2176.5, 2147. , 2180.5, 2200.2, 2195. , 2220. ,\n",
       "       2261. , 2230.8, 2189. , 2170. , 2206. , 2233. , 2229. , 2290. ,\n",
       "       2140. , 2169.8, 2161. , 2167. , 2108. , 2181. , 2198. , 2177.8,\n",
       "       2165. , 2235.6, 2260. , 2290.4, 2355. , 2363. , 2320. , 2369. ,\n",
       "       2349.6, 2344.8, 2325. , 2354.8, 2323.8, 2318.8, 2299.8, 2290. ,\n",
       "       2274. , 2295. , 2306.6, 2270. , 2332.2, 2289.4, 2238. , 2271.6,\n",
       "       2265. , 2280. , 2291.4, 2312. , 2330.2, 2374. , 2435.2, 2483.6,\n",
       "       2477. , 2499. , 2449.2, 2383.4, 2417. , 2399.8, 2420.8, 2415.2,\n",
       "       2407.6, 2387. , 2406. , 2445.2, 2409.8, 2401. , 2400. , 2396. ,\n",
       "       2371. , 2350. , 2299.8, 2302. , 2330. , 2395.8, 2449. , 2398. ,\n",
       "       2400. , 2411.4, 2365.2, 2392. , 2396.4, 2300. , 2357. , 2360. ,\n",
       "       2330.2, 2319. , 2354.8, 2366. , 2402.2, 2415. , 2438. , 2488.4,\n",
       "       2488. , 2459.8, 2475.8, 2488.8, 2490. , 2488.8, 2483.8, 2446.8,\n",
       "       2399.8, 2391.8, 2435.2, 2420. , 2482.8, 2501.2, 2499.6, 2494.8,\n",
       "       2488.6, 2456.2, 2493.4, 2503.6, 2509.2, 2565. , 2538.2, 2554.8,\n",
       "       2499.2, 2506. , 2491. , 2496. , 2497.2, 2520. , 2456. , 2436. ,\n",
       "       2486.6, 2481.4, 2488.6, 2489.8, 2449.8, 2381.4, 2419. , 2417.2,\n",
       "       2425. , 2410. , 2384.6, 2440. , 2390. , 2374.8, 2420. , 2437.6,\n",
       "       2438. , 2412. , 2410. , 2379.8, 2394. , 2431. , 2424. , 2443.4,\n",
       "       2470. , 2473.6, 2487. , 2500.4, 2514. , 2507. , 2463.2, 2456.4,\n",
       "       2466.2, 2457. , 2449. , 2440. , 2439.2, 2419.8, 2409.8, 2343.8,\n",
       "       2305. , 2308. , 2332. , 2323.8, 2310. , 2271.4, 2284. , 2234.2,\n",
       "       2280.2, 2270. , 2296.6, 2305.4, 2307.4, 2314.8, 1935.8, 1970.2,\n",
       "       1975. , 1949.6, 1929.2, 2043. , 2093. , 2054.6, 2049.8, 2100. ,\n",
       "       2073.4, 2138.8, 2142. , 2156. , 2141.8, 2162.6, 2178. , 2185. ,\n",
       "       2176.8, 2184.8, 2183. , 2175. , 2180. , 2201.6, 2375. , 2545.2,\n",
       "       2492.6, 2538. , 2541. , 2570.4, 2624.6, 2648.8, 2669. , 2624.6,\n",
       "       2695. , 2631.2, 2601. , 2603.8, 2568.6, 2575. , 2578.8, 2577. ,\n",
       "       2610.8, 2636. , 2700.2, 2676. , 2634.4, 2617.2, 2653. , 2680.4,\n",
       "       2704.6, 2688. , 2689. , 2713.4, 2736. , 2712.8, 2682.4, 2663.8,\n",
       "       2682.6, 2728.4, 2715. , 2719. , 2677. , 2678.8, 2730. , 2791. ,\n",
       "       2800. , 2800. , 2790.8, 2798. , 2780.2, 2784.6, 2812.4, 2881.4,\n",
       "       2910. , 2856.8, 2912.4, 3018.2, 3029.4, 3058.6, 3103.8, 3095. ,\n",
       "       3084.6, 3067. , 3120. , 2980. , 2993. , 2918.6, 2942.8, 2970.2,\n",
       "       2800. , 2661.6, 2666. , 2622. , 2749.8, 2745. , 2680. , 2755. ,\n",
       "       2700.8, 2509.2, 2616.8, 2559.8, 2439.8, 2459.6, 2312.8, 2357. ,\n",
       "       2346.8, 2482.6, 2368.6, 2363. , 2550. , 2584. , 2650. , 2577.2,\n",
       "       2700. , 2652. , 2621. , 2540.2, 2566.4, 2609. , 2622. , 2724.8,\n",
       "       2705.6, 2659.6, 2698.8, 2718.6, 2603. , 2659.6, 2670. , 2681.6,\n",
       "       2651.6, 2666. , 2634.4, 2651.6, 2730.2, 2807. , 2898. , 2809.8,\n",
       "       2849. , 2849.2, 2869. , 2979.2, 2968.2, 2959. , 2875. , 2856.8,\n",
       "       2885.4, 2975.2, 2956.8, 2942. , 2887.6, 2914.2, 2968.2, 2905.8,\n",
       "       2876.8, 2865. , 2849.6, 2845. , 2837.2, 2856. , 2871. , 2896.6,\n",
       "       2897. , 2896.2, 2873. , 2836. , 2984.2, 3024.8, 3058.6, 3108. ,\n",
       "       3170.2, 3240.4, 3432.2, 3476.2, 3431.2, 3467.6, 3524. , 3560.4,\n",
       "       3572.8, 3643. , 3595.4, 3590. , 3629.6, 3637. , 3670.2, 3762. ,\n",
       "       3844.8, 3824. , 3957. , 4015. , 3965. , 4020. , 3982. , 4035. ,\n",
       "       4194. , 4131. , 4182.2, 4213. , 4261.2, 4304. , 4333.2, 4335. ,\n",
       "       4394. , 4381. , 4228.2, 4208.2, 4411. , 4406.6, 4409.8, 4491.6,\n",
       "       4513. , 4594.8, 4725. , 4800. , 4920. , 5060.4, 4993.6, 4870.2,\n",
       "       4890.6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f0e4316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X = torch.from_numpy(X.astype('float32')).to(device=device).reshape(-1, 50 , 1)\n",
    "        self.y = torch.from_numpy(y.astype('float32')).to(device=device)\n",
    "        print(self.X.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e312d6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([705, 50, 1])\n"
     ]
    }
   ],
   "source": [
    "dataset = TimeSeriesDataset(X=X_examples, y=y_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de0954d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0b8500b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 1])\n",
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in dataloader:\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c32d32",
   "metadata": {},
   "source": [
    "## Test train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcf6a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(xlstmtime.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46dedc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Batch [5/23], Loss: 525909.6875\n",
      "Epoch [1/500], Batch [10/23], Loss: 112678.0938\n",
      "Epoch [1/500], Batch [15/23], Loss: 264791.5000\n",
      "Epoch [1/500], Batch [20/23], Loss: 37600.4492\n",
      "Epoch [2/500], Batch [5/23], Loss: 53250.7383\n",
      "Epoch [2/500], Batch [10/23], Loss: 22515.0000\n",
      "Epoch [2/500], Batch [15/23], Loss: 4963.0923\n",
      "Epoch [2/500], Batch [20/23], Loss: 13830.1445\n",
      "Epoch [3/500], Batch [5/23], Loss: 2805.5898\n",
      "Epoch [3/500], Batch [10/23], Loss: 4656.0732\n",
      "Epoch [3/500], Batch [15/23], Loss: 2581.5762\n",
      "Epoch [3/500], Batch [20/23], Loss: 2295.8210\n",
      "Epoch [4/500], Batch [5/23], Loss: 2676.9995\n",
      "Epoch [4/500], Batch [10/23], Loss: 3180.3267\n",
      "Epoch [4/500], Batch [15/23], Loss: 2387.1011\n",
      "Epoch [4/500], Batch [20/23], Loss: 3496.9402\n",
      "Epoch [5/500], Batch [5/23], Loss: 2981.3496\n",
      "Epoch [5/500], Batch [10/23], Loss: 3260.6243\n",
      "Epoch [5/500], Batch [15/23], Loss: 5995.5913\n",
      "Epoch [5/500], Batch [20/23], Loss: 1658.5823\n",
      "Epoch [6/500], Batch [5/23], Loss: 1544.5593\n",
      "Epoch [6/500], Batch [10/23], Loss: 3631.8887\n",
      "Epoch [6/500], Batch [15/23], Loss: 3098.7942\n",
      "Epoch [6/500], Batch [20/23], Loss: 2442.4817\n",
      "Epoch [7/500], Batch [5/23], Loss: 2955.2720\n",
      "Epoch [7/500], Batch [10/23], Loss: 1871.4762\n",
      "Epoch [7/500], Batch [15/23], Loss: 4152.2090\n",
      "Epoch [7/500], Batch [20/23], Loss: 2370.5127\n",
      "Epoch [8/500], Batch [5/23], Loss: 1777.8936\n",
      "Epoch [8/500], Batch [10/23], Loss: 2607.3296\n",
      "Epoch [8/500], Batch [15/23], Loss: 1481.6824\n",
      "Epoch [8/500], Batch [20/23], Loss: 831.9006\n",
      "Epoch [9/500], Batch [5/23], Loss: 3379.1670\n",
      "Epoch [9/500], Batch [10/23], Loss: 2688.1941\n",
      "Epoch [9/500], Batch [15/23], Loss: 4982.4170\n",
      "Epoch [9/500], Batch [20/23], Loss: 1860.6963\n",
      "Epoch [10/500], Batch [5/23], Loss: 2886.3774\n",
      "Epoch [10/500], Batch [10/23], Loss: 2708.8757\n",
      "Epoch [10/500], Batch [15/23], Loss: 1357.5925\n",
      "Epoch [10/500], Batch [20/23], Loss: 2379.6514\n",
      "Epoch [11/500], Batch [5/23], Loss: 2531.9988\n",
      "Epoch [11/500], Batch [10/23], Loss: 1230.1702\n",
      "Epoch [11/500], Batch [15/23], Loss: 2304.8687\n",
      "Epoch [11/500], Batch [20/23], Loss: 2025.6489\n",
      "Epoch [12/500], Batch [5/23], Loss: 7609.0786\n",
      "Epoch [12/500], Batch [10/23], Loss: 1481.7197\n",
      "Epoch [12/500], Batch [15/23], Loss: 4045.3013\n",
      "Epoch [12/500], Batch [20/23], Loss: 2485.9146\n",
      "Epoch [13/500], Batch [5/23], Loss: 1336.4778\n",
      "Epoch [13/500], Batch [10/23], Loss: 2920.4707\n",
      "Epoch [13/500], Batch [15/23], Loss: 3084.2053\n",
      "Epoch [13/500], Batch [20/23], Loss: 4108.6338\n",
      "Epoch [14/500], Batch [5/23], Loss: 8137.7080\n",
      "Epoch [14/500], Batch [10/23], Loss: 1612.0107\n",
      "Epoch [14/500], Batch [15/23], Loss: 2941.0537\n",
      "Epoch [14/500], Batch [20/23], Loss: 2497.1616\n",
      "Epoch [15/500], Batch [5/23], Loss: 2333.6101\n",
      "Epoch [15/500], Batch [10/23], Loss: 1421.2489\n",
      "Epoch [15/500], Batch [15/23], Loss: 2651.0864\n",
      "Epoch [15/500], Batch [20/23], Loss: 3102.1196\n",
      "Epoch [16/500], Batch [5/23], Loss: 2158.8281\n",
      "Epoch [16/500], Batch [10/23], Loss: 2736.9556\n",
      "Epoch [16/500], Batch [15/23], Loss: 2435.5195\n",
      "Epoch [16/500], Batch [20/23], Loss: 4170.7646\n",
      "Epoch [17/500], Batch [5/23], Loss: 2627.4954\n",
      "Epoch [17/500], Batch [10/23], Loss: 1671.3958\n",
      "Epoch [17/500], Batch [15/23], Loss: 923.5317\n",
      "Epoch [17/500], Batch [20/23], Loss: 2205.8618\n",
      "Epoch [18/500], Batch [5/23], Loss: 2699.7119\n",
      "Epoch [18/500], Batch [10/23], Loss: 1100.5990\n",
      "Epoch [18/500], Batch [15/23], Loss: 2701.1812\n",
      "Epoch [18/500], Batch [20/23], Loss: 3620.6113\n",
      "Epoch [19/500], Batch [5/23], Loss: 3179.6414\n",
      "Epoch [19/500], Batch [10/23], Loss: 2737.9146\n",
      "Epoch [19/500], Batch [15/23], Loss: 980.5526\n",
      "Epoch [19/500], Batch [20/23], Loss: 1559.3168\n",
      "Epoch [20/500], Batch [5/23], Loss: 3728.2527\n",
      "Epoch [20/500], Batch [10/23], Loss: 2015.6123\n",
      "Epoch [20/500], Batch [15/23], Loss: 6647.6748\n",
      "Epoch [20/500], Batch [20/23], Loss: 2871.4299\n",
      "Epoch [21/500], Batch [5/23], Loss: 4063.7812\n",
      "Epoch [21/500], Batch [10/23], Loss: 4221.9570\n",
      "Epoch [21/500], Batch [15/23], Loss: 1396.2119\n",
      "Epoch [21/500], Batch [20/23], Loss: 3204.8740\n",
      "Epoch [22/500], Batch [5/23], Loss: 3425.1909\n",
      "Epoch [22/500], Batch [10/23], Loss: 3021.3811\n",
      "Epoch [22/500], Batch [15/23], Loss: 1893.2026\n",
      "Epoch [22/500], Batch [20/23], Loss: 2142.7944\n",
      "Epoch [23/500], Batch [5/23], Loss: 1685.7445\n",
      "Epoch [23/500], Batch [10/23], Loss: 5315.1611\n",
      "Epoch [23/500], Batch [15/23], Loss: 2680.9209\n",
      "Epoch [23/500], Batch [20/23], Loss: 2687.1860\n",
      "Epoch [24/500], Batch [5/23], Loss: 2841.8860\n",
      "Epoch [24/500], Batch [10/23], Loss: 1828.1074\n",
      "Epoch [24/500], Batch [15/23], Loss: 2197.2595\n",
      "Epoch [24/500], Batch [20/23], Loss: 2744.8833\n",
      "Epoch [25/500], Batch [5/23], Loss: 4253.7300\n",
      "Epoch [25/500], Batch [10/23], Loss: 5120.0771\n",
      "Epoch [25/500], Batch [15/23], Loss: 3137.2437\n",
      "Epoch [25/500], Batch [20/23], Loss: 3354.2319\n",
      "Epoch [26/500], Batch [5/23], Loss: 2302.8652\n",
      "Epoch [26/500], Batch [10/23], Loss: 4422.0459\n",
      "Epoch [26/500], Batch [15/23], Loss: 4325.7847\n",
      "Epoch [26/500], Batch [20/23], Loss: 2762.4500\n",
      "Epoch [27/500], Batch [5/23], Loss: 2354.2454\n",
      "Epoch [27/500], Batch [10/23], Loss: 4033.3843\n",
      "Epoch [27/500], Batch [15/23], Loss: 3367.1416\n",
      "Epoch [27/500], Batch [20/23], Loss: 2964.3179\n",
      "Epoch [28/500], Batch [5/23], Loss: 1160.4487\n",
      "Epoch [28/500], Batch [10/23], Loss: 5794.0742\n",
      "Epoch [28/500], Batch [15/23], Loss: 1478.1219\n",
      "Epoch [28/500], Batch [20/23], Loss: 1757.1387\n",
      "Epoch [29/500], Batch [5/23], Loss: 1657.8351\n",
      "Epoch [29/500], Batch [10/23], Loss: 2063.9795\n",
      "Epoch [29/500], Batch [15/23], Loss: 4574.8940\n",
      "Epoch [29/500], Batch [20/23], Loss: 2951.9739\n",
      "Epoch [30/500], Batch [5/23], Loss: 2785.2383\n",
      "Epoch [30/500], Batch [10/23], Loss: 3481.3770\n",
      "Epoch [30/500], Batch [15/23], Loss: 4683.6826\n",
      "Epoch [30/500], Batch [20/23], Loss: 2585.8665\n",
      "Epoch [31/500], Batch [5/23], Loss: 3552.8257\n",
      "Epoch [31/500], Batch [10/23], Loss: 3346.1697\n",
      "Epoch [31/500], Batch [15/23], Loss: 7385.5586\n",
      "Epoch [31/500], Batch [20/23], Loss: 2504.8206\n",
      "Epoch [32/500], Batch [5/23], Loss: 1972.6447\n",
      "Epoch [32/500], Batch [10/23], Loss: 838.1601\n",
      "Epoch [32/500], Batch [15/23], Loss: 2936.8794\n",
      "Epoch [32/500], Batch [20/23], Loss: 2817.5266\n",
      "Epoch [33/500], Batch [5/23], Loss: 2936.5325\n",
      "Epoch [33/500], Batch [10/23], Loss: 1925.3523\n",
      "Epoch [33/500], Batch [15/23], Loss: 2157.1609\n",
      "Epoch [33/500], Batch [20/23], Loss: 3367.9146\n",
      "Epoch [34/500], Batch [5/23], Loss: 1622.4304\n",
      "Epoch [34/500], Batch [10/23], Loss: 1192.4645\n",
      "Epoch [34/500], Batch [15/23], Loss: 4274.0913\n",
      "Epoch [34/500], Batch [20/23], Loss: 1498.3140\n",
      "Epoch [35/500], Batch [5/23], Loss: 1060.2006\n",
      "Epoch [35/500], Batch [10/23], Loss: 4296.9512\n",
      "Epoch [35/500], Batch [15/23], Loss: 1841.3381\n",
      "Epoch [35/500], Batch [20/23], Loss: 1385.9763\n",
      "Epoch [36/500], Batch [5/23], Loss: 3329.0562\n",
      "Epoch [36/500], Batch [10/23], Loss: 1629.6035\n",
      "Epoch [36/500], Batch [15/23], Loss: 2331.6277\n",
      "Epoch [36/500], Batch [20/23], Loss: 4444.6309\n",
      "Epoch [37/500], Batch [5/23], Loss: 1502.2587\n",
      "Epoch [37/500], Batch [10/23], Loss: 2179.6040\n",
      "Epoch [37/500], Batch [15/23], Loss: 1331.0129\n",
      "Epoch [37/500], Batch [20/23], Loss: 5937.6206\n",
      "Epoch [38/500], Batch [5/23], Loss: 2378.5718\n",
      "Epoch [38/500], Batch [10/23], Loss: 4376.6689\n",
      "Epoch [38/500], Batch [15/23], Loss: 2631.8179\n",
      "Epoch [38/500], Batch [20/23], Loss: 1902.5519\n",
      "Epoch [39/500], Batch [5/23], Loss: 7447.8433\n",
      "Epoch [39/500], Batch [10/23], Loss: 3626.1274\n",
      "Epoch [39/500], Batch [15/23], Loss: 2520.6377\n",
      "Epoch [39/500], Batch [20/23], Loss: 4737.5869\n",
      "Epoch [40/500], Batch [5/23], Loss: 3680.0986\n",
      "Epoch [40/500], Batch [10/23], Loss: 1337.1241\n",
      "Epoch [40/500], Batch [15/23], Loss: 1683.8352\n",
      "Epoch [40/500], Batch [20/23], Loss: 2917.9082\n",
      "Epoch [41/500], Batch [5/23], Loss: 1131.9894\n",
      "Epoch [41/500], Batch [10/23], Loss: 2048.3743\n",
      "Epoch [41/500], Batch [15/23], Loss: 3184.0618\n",
      "Epoch [41/500], Batch [20/23], Loss: 2743.9607\n",
      "Epoch [42/500], Batch [5/23], Loss: 1851.4091\n",
      "Epoch [42/500], Batch [10/23], Loss: 4160.9585\n",
      "Epoch [42/500], Batch [15/23], Loss: 1677.0155\n",
      "Epoch [42/500], Batch [20/23], Loss: 7692.6025\n",
      "Epoch [43/500], Batch [5/23], Loss: 4080.0879\n",
      "Epoch [43/500], Batch [10/23], Loss: 4002.8035\n",
      "Epoch [43/500], Batch [15/23], Loss: 3281.3284\n",
      "Epoch [43/500], Batch [20/23], Loss: 1992.5243\n",
      "Epoch [44/500], Batch [5/23], Loss: 2744.8738\n",
      "Epoch [44/500], Batch [10/23], Loss: 2772.3430\n",
      "Epoch [44/500], Batch [15/23], Loss: 3239.8699\n",
      "Epoch [44/500], Batch [20/23], Loss: 3826.4897\n",
      "Epoch [45/500], Batch [5/23], Loss: 1164.3938\n",
      "Epoch [45/500], Batch [10/23], Loss: 2999.6638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/500], Batch [15/23], Loss: 2292.1318\n",
      "Epoch [45/500], Batch [20/23], Loss: 1531.8344\n",
      "Epoch [46/500], Batch [5/23], Loss: 1444.2363\n",
      "Epoch [46/500], Batch [10/23], Loss: 5465.8994\n",
      "Epoch [46/500], Batch [15/23], Loss: 1911.7239\n",
      "Epoch [46/500], Batch [20/23], Loss: 3630.7888\n",
      "Epoch [47/500], Batch [5/23], Loss: 1296.8921\n",
      "Epoch [47/500], Batch [10/23], Loss: 2605.0479\n",
      "Epoch [47/500], Batch [15/23], Loss: 2716.8328\n",
      "Epoch [47/500], Batch [20/23], Loss: 3290.8320\n",
      "Epoch [48/500], Batch [5/23], Loss: 720.4559\n",
      "Epoch [48/500], Batch [10/23], Loss: 1607.8583\n",
      "Epoch [48/500], Batch [15/23], Loss: 6445.1406\n",
      "Epoch [48/500], Batch [20/23], Loss: 3760.4595\n",
      "Epoch [49/500], Batch [5/23], Loss: 959.4189\n",
      "Epoch [49/500], Batch [10/23], Loss: 3865.3804\n",
      "Epoch [49/500], Batch [15/23], Loss: 3159.1924\n",
      "Epoch [49/500], Batch [20/23], Loss: 5330.9746\n",
      "Epoch [50/500], Batch [5/23], Loss: 2361.0549\n",
      "Epoch [50/500], Batch [10/23], Loss: 2003.9792\n",
      "Epoch [50/500], Batch [15/23], Loss: 4069.8569\n",
      "Epoch [50/500], Batch [20/23], Loss: 2042.1459\n",
      "Epoch [51/500], Batch [5/23], Loss: 3178.3196\n",
      "Epoch [51/500], Batch [10/23], Loss: 902.6212\n",
      "Epoch [51/500], Batch [15/23], Loss: 4811.0547\n",
      "Epoch [51/500], Batch [20/23], Loss: 2220.0071\n",
      "Epoch [52/500], Batch [5/23], Loss: 5656.5776\n",
      "Epoch [52/500], Batch [10/23], Loss: 1849.2012\n",
      "Epoch [52/500], Batch [15/23], Loss: 3385.5969\n",
      "Epoch [52/500], Batch [20/23], Loss: 1158.8479\n",
      "Epoch [53/500], Batch [5/23], Loss: 1724.0186\n",
      "Epoch [53/500], Batch [10/23], Loss: 1272.7135\n",
      "Epoch [53/500], Batch [15/23], Loss: 3935.2842\n",
      "Epoch [53/500], Batch [20/23], Loss: 7703.2188\n",
      "Epoch [54/500], Batch [5/23], Loss: 2378.0078\n",
      "Epoch [54/500], Batch [10/23], Loss: 1414.6746\n",
      "Epoch [54/500], Batch [15/23], Loss: 985.9886\n",
      "Epoch [54/500], Batch [20/23], Loss: 2797.8398\n",
      "Epoch [55/500], Batch [5/23], Loss: 2610.1587\n",
      "Epoch [55/500], Batch [10/23], Loss: 2153.1223\n",
      "Epoch [55/500], Batch [15/23], Loss: 2374.9192\n",
      "Epoch [55/500], Batch [20/23], Loss: 2121.8115\n",
      "Epoch [56/500], Batch [5/23], Loss: 2912.9683\n",
      "Epoch [56/500], Batch [10/23], Loss: 3653.0444\n",
      "Epoch [56/500], Batch [15/23], Loss: 2067.2136\n",
      "Epoch [56/500], Batch [20/23], Loss: 5545.8320\n",
      "Epoch [57/500], Batch [5/23], Loss: 1830.7230\n",
      "Epoch [57/500], Batch [10/23], Loss: 2772.5156\n",
      "Epoch [57/500], Batch [15/23], Loss: 3870.5964\n",
      "Epoch [57/500], Batch [20/23], Loss: 3075.0649\n",
      "Epoch [58/500], Batch [5/23], Loss: 2114.8872\n",
      "Epoch [58/500], Batch [10/23], Loss: 3208.3538\n",
      "Epoch [58/500], Batch [15/23], Loss: 957.3700\n",
      "Epoch [58/500], Batch [20/23], Loss: 2718.5901\n",
      "Epoch [59/500], Batch [5/23], Loss: 3045.0552\n",
      "Epoch [59/500], Batch [10/23], Loss: 1828.7986\n",
      "Epoch [59/500], Batch [15/23], Loss: 1331.4181\n",
      "Epoch [59/500], Batch [20/23], Loss: 3156.8599\n",
      "Epoch [60/500], Batch [5/23], Loss: 3974.2766\n",
      "Epoch [60/500], Batch [10/23], Loss: 1546.0784\n",
      "Epoch [60/500], Batch [15/23], Loss: 2103.6758\n",
      "Epoch [60/500], Batch [20/23], Loss: 2521.6467\n",
      "Epoch [61/500], Batch [5/23], Loss: 1444.5217\n",
      "Epoch [61/500], Batch [10/23], Loss: 3040.2661\n",
      "Epoch [61/500], Batch [15/23], Loss: 1858.4612\n",
      "Epoch [61/500], Batch [20/23], Loss: 3316.8013\n",
      "Epoch [62/500], Batch [5/23], Loss: 4157.0864\n",
      "Epoch [62/500], Batch [10/23], Loss: 4092.1333\n",
      "Epoch [62/500], Batch [15/23], Loss: 1748.0750\n",
      "Epoch [62/500], Batch [20/23], Loss: 2318.9917\n",
      "Epoch [63/500], Batch [5/23], Loss: 2267.4497\n",
      "Epoch [63/500], Batch [10/23], Loss: 1783.4487\n",
      "Epoch [63/500], Batch [15/23], Loss: 3736.4768\n",
      "Epoch [63/500], Batch [20/23], Loss: 3486.9651\n",
      "Epoch [64/500], Batch [5/23], Loss: 4299.3232\n",
      "Epoch [64/500], Batch [10/23], Loss: 7764.4346\n",
      "Epoch [64/500], Batch [15/23], Loss: 4094.5447\n",
      "Epoch [64/500], Batch [20/23], Loss: 2760.8247\n",
      "Epoch [65/500], Batch [5/23], Loss: 3277.5093\n",
      "Epoch [65/500], Batch [10/23], Loss: 1371.5148\n",
      "Epoch [65/500], Batch [15/23], Loss: 2278.1162\n",
      "Epoch [65/500], Batch [20/23], Loss: 3435.0493\n",
      "Epoch [66/500], Batch [5/23], Loss: 2388.0388\n",
      "Epoch [66/500], Batch [10/23], Loss: 2449.8345\n",
      "Epoch [66/500], Batch [15/23], Loss: 2983.2112\n",
      "Epoch [66/500], Batch [20/23], Loss: 2635.7319\n",
      "Epoch [67/500], Batch [5/23], Loss: 1249.7290\n",
      "Epoch [67/500], Batch [10/23], Loss: 1975.7566\n",
      "Epoch [67/500], Batch [15/23], Loss: 1347.1100\n",
      "Epoch [67/500], Batch [20/23], Loss: 2104.6284\n",
      "Epoch [68/500], Batch [5/23], Loss: 2460.3511\n",
      "Epoch [68/500], Batch [10/23], Loss: 3403.4165\n",
      "Epoch [68/500], Batch [15/23], Loss: 2447.9180\n",
      "Epoch [68/500], Batch [20/23], Loss: 1663.4443\n",
      "Epoch [69/500], Batch [5/23], Loss: 3690.9326\n",
      "Epoch [69/500], Batch [10/23], Loss: 2934.5662\n",
      "Epoch [69/500], Batch [15/23], Loss: 2841.1177\n",
      "Epoch [69/500], Batch [20/23], Loss: 2251.6074\n",
      "Epoch [70/500], Batch [5/23], Loss: 1895.3738\n",
      "Epoch [70/500], Batch [10/23], Loss: 4733.6709\n",
      "Epoch [70/500], Batch [15/23], Loss: 2398.1260\n",
      "Epoch [70/500], Batch [20/23], Loss: 2243.7917\n",
      "Epoch [71/500], Batch [5/23], Loss: 2713.4795\n",
      "Epoch [71/500], Batch [10/23], Loss: 1970.7485\n",
      "Epoch [71/500], Batch [15/23], Loss: 4530.3477\n",
      "Epoch [71/500], Batch [20/23], Loss: 4833.3438\n",
      "Epoch [72/500], Batch [5/23], Loss: 837.8119\n",
      "Epoch [72/500], Batch [10/23], Loss: 2157.3784\n",
      "Epoch [72/500], Batch [15/23], Loss: 12573.3818\n",
      "Epoch [72/500], Batch [20/23], Loss: 2505.9287\n",
      "Epoch [73/500], Batch [5/23], Loss: 1872.8640\n",
      "Epoch [73/500], Batch [10/23], Loss: 1900.1997\n",
      "Epoch [73/500], Batch [15/23], Loss: 3792.0637\n",
      "Epoch [73/500], Batch [20/23], Loss: 1468.2032\n",
      "Epoch [74/500], Batch [5/23], Loss: 1113.8677\n",
      "Epoch [74/500], Batch [10/23], Loss: 1527.7667\n",
      "Epoch [74/500], Batch [15/23], Loss: 2153.4602\n",
      "Epoch [74/500], Batch [20/23], Loss: 2117.5977\n",
      "Epoch [75/500], Batch [5/23], Loss: 1177.8058\n",
      "Epoch [75/500], Batch [10/23], Loss: 3231.3921\n",
      "Epoch [75/500], Batch [15/23], Loss: 2867.7979\n",
      "Epoch [75/500], Batch [20/23], Loss: 2152.7993\n",
      "Epoch [76/500], Batch [5/23], Loss: 2654.8984\n",
      "Epoch [76/500], Batch [10/23], Loss: 3118.9739\n",
      "Epoch [76/500], Batch [15/23], Loss: 1955.9800\n",
      "Epoch [76/500], Batch [20/23], Loss: 2933.5793\n",
      "Epoch [77/500], Batch [5/23], Loss: 2298.0928\n",
      "Epoch [77/500], Batch [10/23], Loss: 4714.3770\n",
      "Epoch [77/500], Batch [15/23], Loss: 5162.3145\n",
      "Epoch [77/500], Batch [20/23], Loss: 1365.2451\n",
      "Epoch [78/500], Batch [5/23], Loss: 2514.5911\n",
      "Epoch [78/500], Batch [10/23], Loss: 6365.7305\n",
      "Epoch [78/500], Batch [15/23], Loss: 3229.0366\n",
      "Epoch [78/500], Batch [20/23], Loss: 2082.4814\n",
      "Epoch [79/500], Batch [5/23], Loss: 1732.0686\n",
      "Epoch [79/500], Batch [10/23], Loss: 3759.7146\n",
      "Epoch [79/500], Batch [15/23], Loss: 2016.0643\n",
      "Epoch [79/500], Batch [20/23], Loss: 3366.0459\n",
      "Epoch [80/500], Batch [5/23], Loss: 2790.8970\n",
      "Epoch [80/500], Batch [10/23], Loss: 3282.9299\n",
      "Epoch [80/500], Batch [15/23], Loss: 5283.9053\n",
      "Epoch [80/500], Batch [20/23], Loss: 1336.3926\n",
      "Epoch [81/500], Batch [5/23], Loss: 1556.5042\n",
      "Epoch [81/500], Batch [10/23], Loss: 3935.7761\n",
      "Epoch [81/500], Batch [15/23], Loss: 2399.2043\n",
      "Epoch [81/500], Batch [20/23], Loss: 2818.2229\n",
      "Epoch [82/500], Batch [5/23], Loss: 2533.2417\n",
      "Epoch [82/500], Batch [10/23], Loss: 2247.5474\n",
      "Epoch [82/500], Batch [15/23], Loss: 2831.5107\n",
      "Epoch [82/500], Batch [20/23], Loss: 1392.5405\n",
      "Epoch [83/500], Batch [5/23], Loss: 6917.0762\n",
      "Epoch [83/500], Batch [10/23], Loss: 3554.3745\n",
      "Epoch [83/500], Batch [15/23], Loss: 2541.3403\n",
      "Epoch [83/500], Batch [20/23], Loss: 2154.9900\n",
      "Epoch [84/500], Batch [5/23], Loss: 2003.6984\n",
      "Epoch [84/500], Batch [10/23], Loss: 2225.8933\n",
      "Epoch [84/500], Batch [15/23], Loss: 2988.3228\n",
      "Epoch [84/500], Batch [20/23], Loss: 5272.2017\n",
      "Epoch [85/500], Batch [5/23], Loss: 5645.1455\n",
      "Epoch [85/500], Batch [10/23], Loss: 2176.2563\n",
      "Epoch [85/500], Batch [15/23], Loss: 1354.7958\n",
      "Epoch [85/500], Batch [20/23], Loss: 3452.8982\n",
      "Epoch [86/500], Batch [5/23], Loss: 6049.7515\n",
      "Epoch [86/500], Batch [10/23], Loss: 2973.9773\n",
      "Epoch [86/500], Batch [15/23], Loss: 2084.8875\n",
      "Epoch [86/500], Batch [20/23], Loss: 1746.1426\n",
      "Epoch [87/500], Batch [5/23], Loss: 1849.8040\n",
      "Epoch [87/500], Batch [10/23], Loss: 3983.3716\n",
      "Epoch [87/500], Batch [15/23], Loss: 8426.5625\n",
      "Epoch [87/500], Batch [20/23], Loss: 3988.2747\n",
      "Epoch [88/500], Batch [5/23], Loss: 2291.8877\n",
      "Epoch [88/500], Batch [10/23], Loss: 4025.3083\n",
      "Epoch [88/500], Batch [15/23], Loss: 7528.7861\n",
      "Epoch [88/500], Batch [20/23], Loss: 4027.4482\n",
      "Epoch [89/500], Batch [5/23], Loss: 6481.8896\n",
      "Epoch [89/500], Batch [10/23], Loss: 3531.9294\n",
      "Epoch [89/500], Batch [15/23], Loss: 1697.6765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/500], Batch [20/23], Loss: 2235.5925\n",
      "Epoch [90/500], Batch [5/23], Loss: 9633.1592\n",
      "Epoch [90/500], Batch [10/23], Loss: 3238.7998\n",
      "Epoch [90/500], Batch [15/23], Loss: 2196.7900\n",
      "Epoch [90/500], Batch [20/23], Loss: 2257.2231\n",
      "Epoch [91/500], Batch [5/23], Loss: 4002.8926\n",
      "Epoch [91/500], Batch [10/23], Loss: 7191.5493\n",
      "Epoch [91/500], Batch [15/23], Loss: 1887.4539\n",
      "Epoch [91/500], Batch [20/23], Loss: 4793.0137\n",
      "Epoch [92/500], Batch [5/23], Loss: 1930.2026\n",
      "Epoch [92/500], Batch [10/23], Loss: 2330.6177\n",
      "Epoch [92/500], Batch [15/23], Loss: 1416.1548\n",
      "Epoch [92/500], Batch [20/23], Loss: 4440.9116\n",
      "Epoch [93/500], Batch [5/23], Loss: 1993.2611\n",
      "Epoch [93/500], Batch [10/23], Loss: 6068.4810\n",
      "Epoch [93/500], Batch [15/23], Loss: 2099.0801\n",
      "Epoch [93/500], Batch [20/23], Loss: 2420.1094\n",
      "Epoch [94/500], Batch [5/23], Loss: 2386.1340\n",
      "Epoch [94/500], Batch [10/23], Loss: 3872.7864\n",
      "Epoch [94/500], Batch [15/23], Loss: 1969.6316\n",
      "Epoch [94/500], Batch [20/23], Loss: 1073.7394\n",
      "Epoch [95/500], Batch [5/23], Loss: 3836.3481\n",
      "Epoch [95/500], Batch [10/23], Loss: 3176.2119\n",
      "Epoch [95/500], Batch [15/23], Loss: 7347.9893\n",
      "Epoch [95/500], Batch [20/23], Loss: 1947.5631\n",
      "Epoch [96/500], Batch [5/23], Loss: 3370.0515\n",
      "Epoch [96/500], Batch [10/23], Loss: 5294.4346\n",
      "Epoch [96/500], Batch [15/23], Loss: 2918.3110\n",
      "Epoch [96/500], Batch [20/23], Loss: 1914.2070\n",
      "Epoch [97/500], Batch [5/23], Loss: 2186.0996\n",
      "Epoch [97/500], Batch [10/23], Loss: 3275.5676\n",
      "Epoch [97/500], Batch [15/23], Loss: 2663.5828\n",
      "Epoch [97/500], Batch [20/23], Loss: 3669.8896\n",
      "Epoch [98/500], Batch [5/23], Loss: 2285.3672\n",
      "Epoch [98/500], Batch [10/23], Loss: 1243.2393\n",
      "Epoch [98/500], Batch [15/23], Loss: 6445.9897\n",
      "Epoch [98/500], Batch [20/23], Loss: 1882.8613\n",
      "Epoch [99/500], Batch [5/23], Loss: 1507.6816\n",
      "Epoch [99/500], Batch [10/23], Loss: 3621.8650\n",
      "Epoch [99/500], Batch [15/23], Loss: 1317.6608\n",
      "Epoch [99/500], Batch [20/23], Loss: 2481.5454\n",
      "Epoch [100/500], Batch [5/23], Loss: 1431.8920\n",
      "Epoch [100/500], Batch [10/23], Loss: 2475.3135\n",
      "Epoch [100/500], Batch [15/23], Loss: 5687.2080\n",
      "Epoch [100/500], Batch [20/23], Loss: 2000.8221\n",
      "Epoch [101/500], Batch [5/23], Loss: 2663.0200\n",
      "Epoch [101/500], Batch [10/23], Loss: 2384.8447\n",
      "Epoch [101/500], Batch [15/23], Loss: 2205.3928\n",
      "Epoch [101/500], Batch [20/23], Loss: 1520.7246\n",
      "Epoch [102/500], Batch [5/23], Loss: 1645.4034\n",
      "Epoch [102/500], Batch [10/23], Loss: 1736.5427\n",
      "Epoch [102/500], Batch [15/23], Loss: 2889.6892\n",
      "Epoch [102/500], Batch [20/23], Loss: 2506.2969\n",
      "Epoch [103/500], Batch [5/23], Loss: 1692.4033\n",
      "Epoch [103/500], Batch [10/23], Loss: 3285.2065\n",
      "Epoch [103/500], Batch [15/23], Loss: 1943.8267\n",
      "Epoch [103/500], Batch [20/23], Loss: 2919.6191\n",
      "Epoch [104/500], Batch [5/23], Loss: 2205.1584\n",
      "Epoch [104/500], Batch [10/23], Loss: 2634.3804\n",
      "Epoch [104/500], Batch [15/23], Loss: 1631.1656\n",
      "Epoch [104/500], Batch [20/23], Loss: 2123.0579\n",
      "Epoch [105/500], Batch [5/23], Loss: 1744.7344\n",
      "Epoch [105/500], Batch [10/23], Loss: 2877.3674\n",
      "Epoch [105/500], Batch [15/23], Loss: 3170.8398\n",
      "Epoch [105/500], Batch [20/23], Loss: 3360.8721\n",
      "Epoch [106/500], Batch [5/23], Loss: 1612.0406\n",
      "Epoch [106/500], Batch [10/23], Loss: 6153.3525\n",
      "Epoch [106/500], Batch [15/23], Loss: 2599.9519\n",
      "Epoch [106/500], Batch [20/23], Loss: 2491.0312\n",
      "Epoch [107/500], Batch [5/23], Loss: 1054.9302\n",
      "Epoch [107/500], Batch [10/23], Loss: 1970.6840\n",
      "Epoch [107/500], Batch [15/23], Loss: 2274.2373\n",
      "Epoch [107/500], Batch [20/23], Loss: 4308.3843\n",
      "Epoch [108/500], Batch [5/23], Loss: 3099.0222\n",
      "Epoch [108/500], Batch [10/23], Loss: 1948.0094\n",
      "Epoch [108/500], Batch [15/23], Loss: 4821.3730\n",
      "Epoch [108/500], Batch [20/23], Loss: 1617.6226\n",
      "Epoch [109/500], Batch [5/23], Loss: 1257.0073\n",
      "Epoch [109/500], Batch [10/23], Loss: 4014.4951\n",
      "Epoch [109/500], Batch [15/23], Loss: 2220.1953\n",
      "Epoch [109/500], Batch [20/23], Loss: 3709.1992\n",
      "Epoch [110/500], Batch [5/23], Loss: 2455.6455\n",
      "Epoch [110/500], Batch [10/23], Loss: 2766.8611\n",
      "Epoch [110/500], Batch [15/23], Loss: 2017.7913\n",
      "Epoch [110/500], Batch [20/23], Loss: 1880.2894\n",
      "Epoch [111/500], Batch [5/23], Loss: 1219.3640\n",
      "Epoch [111/500], Batch [10/23], Loss: 3427.4590\n",
      "Epoch [111/500], Batch [15/23], Loss: 1830.2292\n",
      "Epoch [111/500], Batch [20/23], Loss: 3795.9583\n",
      "Epoch [112/500], Batch [5/23], Loss: 6913.4404\n",
      "Epoch [112/500], Batch [10/23], Loss: 1285.1548\n",
      "Epoch [112/500], Batch [15/23], Loss: 3807.8152\n",
      "Epoch [112/500], Batch [20/23], Loss: 2118.4661\n",
      "Epoch [113/500], Batch [5/23], Loss: 2280.4705\n",
      "Epoch [113/500], Batch [10/23], Loss: 10728.9795\n",
      "Epoch [113/500], Batch [15/23], Loss: 3053.8840\n",
      "Epoch [113/500], Batch [20/23], Loss: 1722.6774\n",
      "Epoch [114/500], Batch [5/23], Loss: 2770.0249\n",
      "Epoch [114/500], Batch [10/23], Loss: 2671.0107\n",
      "Epoch [114/500], Batch [15/23], Loss: 3501.0840\n",
      "Epoch [114/500], Batch [20/23], Loss: 2458.4424\n",
      "Epoch [115/500], Batch [5/23], Loss: 1768.0505\n",
      "Epoch [115/500], Batch [10/23], Loss: 2722.6687\n",
      "Epoch [115/500], Batch [15/23], Loss: 1337.4048\n",
      "Epoch [115/500], Batch [20/23], Loss: 1567.4109\n",
      "Epoch [116/500], Batch [5/23], Loss: 3495.8877\n",
      "Epoch [116/500], Batch [10/23], Loss: 7182.3867\n",
      "Epoch [116/500], Batch [15/23], Loss: 4135.9004\n",
      "Epoch [116/500], Batch [20/23], Loss: 2634.5671\n",
      "Epoch [117/500], Batch [5/23], Loss: 4421.5386\n",
      "Epoch [117/500], Batch [10/23], Loss: 4226.3184\n",
      "Epoch [117/500], Batch [15/23], Loss: 2615.3601\n",
      "Epoch [117/500], Batch [20/23], Loss: 2135.1785\n",
      "Epoch [118/500], Batch [5/23], Loss: 2840.4238\n",
      "Epoch [118/500], Batch [10/23], Loss: 2036.7529\n",
      "Epoch [118/500], Batch [15/23], Loss: 2427.9128\n",
      "Epoch [118/500], Batch [20/23], Loss: 1923.1106\n",
      "Epoch [119/500], Batch [5/23], Loss: 2100.9243\n",
      "Epoch [119/500], Batch [10/23], Loss: 3587.7715\n",
      "Epoch [119/500], Batch [15/23], Loss: 3429.2686\n",
      "Epoch [119/500], Batch [20/23], Loss: 3167.3562\n",
      "Epoch [120/500], Batch [5/23], Loss: 1130.4849\n",
      "Epoch [120/500], Batch [10/23], Loss: 2730.5017\n",
      "Epoch [120/500], Batch [15/23], Loss: 3618.4189\n",
      "Epoch [120/500], Batch [20/23], Loss: 2768.2263\n",
      "Epoch [121/500], Batch [5/23], Loss: 1815.4666\n",
      "Epoch [121/500], Batch [10/23], Loss: 3949.6626\n",
      "Epoch [121/500], Batch [15/23], Loss: 3430.7227\n",
      "Epoch [121/500], Batch [20/23], Loss: 1534.2893\n",
      "Epoch [122/500], Batch [5/23], Loss: 5568.2959\n",
      "Epoch [122/500], Batch [10/23], Loss: 4719.4214\n",
      "Epoch [122/500], Batch [15/23], Loss: 2008.8143\n",
      "Epoch [122/500], Batch [20/23], Loss: 2082.7842\n",
      "Epoch [123/500], Batch [5/23], Loss: 2924.3975\n",
      "Epoch [123/500], Batch [10/23], Loss: 1893.7389\n",
      "Epoch [123/500], Batch [15/23], Loss: 6169.5283\n",
      "Epoch [123/500], Batch [20/23], Loss: 2283.3154\n",
      "Epoch [124/500], Batch [5/23], Loss: 9722.9883\n",
      "Epoch [124/500], Batch [10/23], Loss: 19373.6016\n",
      "Epoch [124/500], Batch [15/23], Loss: 17370.1602\n",
      "Epoch [124/500], Batch [20/23], Loss: 6100.9287\n",
      "Epoch [125/500], Batch [5/23], Loss: 3652.2576\n",
      "Epoch [125/500], Batch [10/23], Loss: 2097.6621\n",
      "Epoch [125/500], Batch [15/23], Loss: 2372.8276\n",
      "Epoch [125/500], Batch [20/23], Loss: 2396.6592\n",
      "Epoch [126/500], Batch [5/23], Loss: 1473.0508\n",
      "Epoch [126/500], Batch [10/23], Loss: 3940.8750\n",
      "Epoch [126/500], Batch [15/23], Loss: 1381.5858\n",
      "Epoch [126/500], Batch [20/23], Loss: 2291.2280\n",
      "Epoch [127/500], Batch [5/23], Loss: 2371.9399\n",
      "Epoch [127/500], Batch [10/23], Loss: 3689.3167\n",
      "Epoch [127/500], Batch [15/23], Loss: 1119.4048\n",
      "Epoch [127/500], Batch [20/23], Loss: 3438.1177\n",
      "Epoch [128/500], Batch [5/23], Loss: 6474.7568\n",
      "Epoch [128/500], Batch [10/23], Loss: 4631.5020\n",
      "Epoch [128/500], Batch [15/23], Loss: 2980.0684\n",
      "Epoch [128/500], Batch [20/23], Loss: 1174.3652\n",
      "Epoch [129/500], Batch [5/23], Loss: 2813.2244\n",
      "Epoch [129/500], Batch [10/23], Loss: 5806.7661\n",
      "Epoch [129/500], Batch [15/23], Loss: 3010.7393\n",
      "Epoch [129/500], Batch [20/23], Loss: 3117.4207\n",
      "Epoch [130/500], Batch [5/23], Loss: 1589.3743\n",
      "Epoch [130/500], Batch [10/23], Loss: 1036.8149\n",
      "Epoch [130/500], Batch [15/23], Loss: 1869.8184\n",
      "Epoch [130/500], Batch [20/23], Loss: 3823.3672\n",
      "Epoch [131/500], Batch [5/23], Loss: 2757.1030\n",
      "Epoch [131/500], Batch [10/23], Loss: 5144.6162\n",
      "Epoch [131/500], Batch [15/23], Loss: 2245.0103\n",
      "Epoch [131/500], Batch [20/23], Loss: 2143.1938\n",
      "Epoch [132/500], Batch [5/23], Loss: 4876.0742\n",
      "Epoch [132/500], Batch [10/23], Loss: 2542.7573\n",
      "Epoch [132/500], Batch [15/23], Loss: 2873.4934\n",
      "Epoch [132/500], Batch [20/23], Loss: 2793.1624\n",
      "Epoch [133/500], Batch [5/23], Loss: 8159.0195\n",
      "Epoch [133/500], Batch [10/23], Loss: 3534.3782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [133/500], Batch [15/23], Loss: 2809.9922\n",
      "Epoch [133/500], Batch [20/23], Loss: 1274.3938\n",
      "Epoch [134/500], Batch [5/23], Loss: 2418.7656\n",
      "Epoch [134/500], Batch [10/23], Loss: 8268.4316\n",
      "Epoch [134/500], Batch [15/23], Loss: 3158.8682\n",
      "Epoch [134/500], Batch [20/23], Loss: 2657.6250\n",
      "Epoch [135/500], Batch [5/23], Loss: 2282.4863\n",
      "Epoch [135/500], Batch [10/23], Loss: 1810.8452\n",
      "Epoch [135/500], Batch [15/23], Loss: 6167.7910\n",
      "Epoch [135/500], Batch [20/23], Loss: 1043.1510\n",
      "Epoch [136/500], Batch [5/23], Loss: 1532.1732\n",
      "Epoch [136/500], Batch [10/23], Loss: 3359.7112\n",
      "Epoch [136/500], Batch [15/23], Loss: 3670.9307\n",
      "Epoch [136/500], Batch [20/23], Loss: 2296.3652\n",
      "Epoch [137/500], Batch [5/23], Loss: 1700.4159\n",
      "Epoch [137/500], Batch [10/23], Loss: 1800.1833\n",
      "Epoch [137/500], Batch [15/23], Loss: 1840.0750\n",
      "Epoch [137/500], Batch [20/23], Loss: 3156.7200\n",
      "Epoch [138/500], Batch [5/23], Loss: 1671.3950\n",
      "Epoch [138/500], Batch [10/23], Loss: 1654.3689\n",
      "Epoch [138/500], Batch [15/23], Loss: 1836.5793\n",
      "Epoch [138/500], Batch [20/23], Loss: 2435.6172\n",
      "Epoch [139/500], Batch [5/23], Loss: 1263.9692\n",
      "Epoch [139/500], Batch [10/23], Loss: 4794.6011\n",
      "Epoch [139/500], Batch [15/23], Loss: 3751.3264\n",
      "Epoch [139/500], Batch [20/23], Loss: 1588.2780\n",
      "Epoch [140/500], Batch [5/23], Loss: 2399.0618\n",
      "Epoch [140/500], Batch [10/23], Loss: 4303.4609\n",
      "Epoch [140/500], Batch [15/23], Loss: 1757.4211\n",
      "Epoch [140/500], Batch [20/23], Loss: 3462.1157\n",
      "Epoch [141/500], Batch [5/23], Loss: 4225.3394\n",
      "Epoch [141/500], Batch [10/23], Loss: 2638.2175\n",
      "Epoch [141/500], Batch [15/23], Loss: 1310.3368\n",
      "Epoch [141/500], Batch [20/23], Loss: 2141.2729\n",
      "Epoch [142/500], Batch [5/23], Loss: 1385.8936\n",
      "Epoch [142/500], Batch [10/23], Loss: 4842.3037\n",
      "Epoch [142/500], Batch [15/23], Loss: 2307.1230\n",
      "Epoch [142/500], Batch [20/23], Loss: 2121.0269\n",
      "Epoch [143/500], Batch [5/23], Loss: 3120.5420\n",
      "Epoch [143/500], Batch [10/23], Loss: 2432.7744\n",
      "Epoch [143/500], Batch [15/23], Loss: 1710.4399\n",
      "Epoch [143/500], Batch [20/23], Loss: 1499.2654\n",
      "Epoch [144/500], Batch [5/23], Loss: 2009.7327\n",
      "Epoch [144/500], Batch [10/23], Loss: 1596.2953\n",
      "Epoch [144/500], Batch [15/23], Loss: 2194.4922\n",
      "Epoch [144/500], Batch [20/23], Loss: 4725.2295\n",
      "Epoch [145/500], Batch [5/23], Loss: 3236.9941\n",
      "Epoch [145/500], Batch [10/23], Loss: 2474.1621\n",
      "Epoch [145/500], Batch [15/23], Loss: 6852.7715\n",
      "Epoch [145/500], Batch [20/23], Loss: 2660.2310\n",
      "Epoch [146/500], Batch [5/23], Loss: 3137.1873\n",
      "Epoch [146/500], Batch [10/23], Loss: 3242.7222\n",
      "Epoch [146/500], Batch [15/23], Loss: 2161.6753\n",
      "Epoch [146/500], Batch [20/23], Loss: 2734.6638\n",
      "Epoch [147/500], Batch [5/23], Loss: 8323.1445\n",
      "Epoch [147/500], Batch [10/23], Loss: 12764.1240\n",
      "Epoch [147/500], Batch [15/23], Loss: 4785.6167\n",
      "Epoch [147/500], Batch [20/23], Loss: 1167.5112\n",
      "Epoch [148/500], Batch [5/23], Loss: 846.6787\n",
      "Epoch [148/500], Batch [10/23], Loss: 1997.5513\n",
      "Epoch [148/500], Batch [15/23], Loss: 5040.1431\n",
      "Epoch [148/500], Batch [20/23], Loss: 1634.3905\n",
      "Epoch [149/500], Batch [5/23], Loss: 2669.9260\n",
      "Epoch [149/500], Batch [10/23], Loss: 3486.3755\n",
      "Epoch [149/500], Batch [15/23], Loss: 2308.1362\n",
      "Epoch [149/500], Batch [20/23], Loss: 2974.5706\n",
      "Epoch [150/500], Batch [5/23], Loss: 3250.8425\n",
      "Epoch [150/500], Batch [10/23], Loss: 3880.5435\n",
      "Epoch [150/500], Batch [15/23], Loss: 3574.2109\n",
      "Epoch [150/500], Batch [20/23], Loss: 3686.3389\n",
      "Epoch [151/500], Batch [5/23], Loss: 5496.6055\n",
      "Epoch [151/500], Batch [10/23], Loss: 2635.9287\n",
      "Epoch [151/500], Batch [15/23], Loss: 2952.6948\n",
      "Epoch [151/500], Batch [20/23], Loss: 7055.0898\n",
      "Epoch [152/500], Batch [5/23], Loss: 7786.5215\n",
      "Epoch [152/500], Batch [10/23], Loss: 1709.8866\n",
      "Epoch [152/500], Batch [15/23], Loss: 4016.2231\n",
      "Epoch [152/500], Batch [20/23], Loss: 2283.2544\n",
      "Epoch [153/500], Batch [5/23], Loss: 2114.7810\n",
      "Epoch [153/500], Batch [10/23], Loss: 3169.2437\n",
      "Epoch [153/500], Batch [15/23], Loss: 4358.8745\n",
      "Epoch [153/500], Batch [20/23], Loss: 2318.0903\n",
      "Epoch [154/500], Batch [5/23], Loss: 2382.8484\n",
      "Epoch [154/500], Batch [10/23], Loss: 1978.6626\n",
      "Epoch [154/500], Batch [15/23], Loss: 4608.9902\n",
      "Epoch [154/500], Batch [20/23], Loss: 1274.4524\n",
      "Epoch [155/500], Batch [5/23], Loss: 4218.7207\n",
      "Epoch [155/500], Batch [10/23], Loss: 6600.8413\n",
      "Epoch [155/500], Batch [15/23], Loss: 8089.8882\n",
      "Epoch [155/500], Batch [20/23], Loss: 3597.2429\n",
      "Epoch [156/500], Batch [5/23], Loss: 3662.4788\n",
      "Epoch [156/500], Batch [10/23], Loss: 2588.3247\n",
      "Epoch [156/500], Batch [15/23], Loss: 2446.9536\n",
      "Epoch [156/500], Batch [20/23], Loss: 3395.1499\n",
      "Epoch [157/500], Batch [5/23], Loss: 3019.8979\n",
      "Epoch [157/500], Batch [10/23], Loss: 3629.2019\n",
      "Epoch [157/500], Batch [15/23], Loss: 2103.6428\n",
      "Epoch [157/500], Batch [20/23], Loss: 2262.3452\n",
      "Epoch [158/500], Batch [5/23], Loss: 2308.4229\n",
      "Epoch [158/500], Batch [10/23], Loss: 2006.2727\n",
      "Epoch [158/500], Batch [15/23], Loss: 5975.7764\n",
      "Epoch [158/500], Batch [20/23], Loss: 1358.7207\n",
      "Epoch [159/500], Batch [5/23], Loss: 6234.3291\n",
      "Epoch [159/500], Batch [10/23], Loss: 3171.9321\n",
      "Epoch [159/500], Batch [15/23], Loss: 3229.1147\n",
      "Epoch [159/500], Batch [20/23], Loss: 1059.6986\n",
      "Epoch [160/500], Batch [5/23], Loss: 3815.4180\n",
      "Epoch [160/500], Batch [10/23], Loss: 3638.9248\n",
      "Epoch [160/500], Batch [15/23], Loss: 1612.7748\n",
      "Epoch [160/500], Batch [20/23], Loss: 1409.7467\n",
      "Epoch [161/500], Batch [5/23], Loss: 2340.3315\n",
      "Epoch [161/500], Batch [10/23], Loss: 5943.5176\n",
      "Epoch [161/500], Batch [15/23], Loss: 1814.2358\n",
      "Epoch [161/500], Batch [20/23], Loss: 2554.5049\n",
      "Epoch [162/500], Batch [5/23], Loss: 2230.2900\n",
      "Epoch [162/500], Batch [10/23], Loss: 1914.1853\n",
      "Epoch [162/500], Batch [15/23], Loss: 3769.0088\n",
      "Epoch [162/500], Batch [20/23], Loss: 2063.0366\n",
      "Epoch [163/500], Batch [5/23], Loss: 3057.2251\n",
      "Epoch [163/500], Batch [10/23], Loss: 1591.9132\n",
      "Epoch [163/500], Batch [15/23], Loss: 1280.7896\n",
      "Epoch [163/500], Batch [20/23], Loss: 1890.2031\n",
      "Epoch [164/500], Batch [5/23], Loss: 2385.6877\n",
      "Epoch [164/500], Batch [10/23], Loss: 1266.0674\n",
      "Epoch [164/500], Batch [15/23], Loss: 1972.5432\n",
      "Epoch [164/500], Batch [20/23], Loss: 5267.4131\n",
      "Epoch [165/500], Batch [5/23], Loss: 5503.0762\n",
      "Epoch [165/500], Batch [10/23], Loss: 7814.7925\n",
      "Epoch [165/500], Batch [15/23], Loss: 2989.9717\n",
      "Epoch [165/500], Batch [20/23], Loss: 10204.9727\n",
      "Epoch [166/500], Batch [5/23], Loss: 3217.8574\n",
      "Epoch [166/500], Batch [10/23], Loss: 9686.3174\n",
      "Epoch [166/500], Batch [15/23], Loss: 12776.0947\n",
      "Epoch [166/500], Batch [20/23], Loss: 4748.9443\n",
      "Epoch [167/500], Batch [5/23], Loss: 4415.6152\n",
      "Epoch [167/500], Batch [10/23], Loss: 2734.6069\n",
      "Epoch [167/500], Batch [15/23], Loss: 1377.1365\n",
      "Epoch [167/500], Batch [20/23], Loss: 5103.5171\n",
      "Epoch [168/500], Batch [5/23], Loss: 2653.9409\n",
      "Epoch [168/500], Batch [10/23], Loss: 2669.0029\n",
      "Epoch [168/500], Batch [15/23], Loss: 1951.1173\n",
      "Epoch [168/500], Batch [20/23], Loss: 3506.6450\n",
      "Epoch [169/500], Batch [5/23], Loss: 6629.2378\n",
      "Epoch [169/500], Batch [10/23], Loss: 2428.9795\n",
      "Epoch [169/500], Batch [15/23], Loss: 2315.8953\n",
      "Epoch [169/500], Batch [20/23], Loss: 1552.9797\n",
      "Epoch [170/500], Batch [5/23], Loss: 2220.9497\n",
      "Epoch [170/500], Batch [10/23], Loss: 1873.4851\n",
      "Epoch [170/500], Batch [15/23], Loss: 1647.7854\n",
      "Epoch [170/500], Batch [20/23], Loss: 4702.3149\n",
      "Epoch [171/500], Batch [5/23], Loss: 2070.4524\n",
      "Epoch [171/500], Batch [10/23], Loss: 1917.0859\n",
      "Epoch [171/500], Batch [15/23], Loss: 9641.3311\n",
      "Epoch [171/500], Batch [20/23], Loss: 2203.2173\n",
      "Epoch [172/500], Batch [5/23], Loss: 1112.3704\n",
      "Epoch [172/500], Batch [10/23], Loss: 1154.3058\n",
      "Epoch [172/500], Batch [15/23], Loss: 2651.6201\n",
      "Epoch [172/500], Batch [20/23], Loss: 2827.4719\n",
      "Epoch [173/500], Batch [5/23], Loss: 1469.9487\n",
      "Epoch [173/500], Batch [10/23], Loss: 1541.6919\n",
      "Epoch [173/500], Batch [15/23], Loss: 1842.3303\n",
      "Epoch [173/500], Batch [20/23], Loss: 2209.3037\n",
      "Epoch [174/500], Batch [5/23], Loss: 6019.1538\n",
      "Epoch [174/500], Batch [10/23], Loss: 1951.7491\n",
      "Epoch [174/500], Batch [15/23], Loss: 1674.1245\n",
      "Epoch [174/500], Batch [20/23], Loss: 3433.5630\n",
      "Epoch [175/500], Batch [5/23], Loss: 1534.0127\n",
      "Epoch [175/500], Batch [10/23], Loss: 903.8997\n",
      "Epoch [175/500], Batch [15/23], Loss: 1860.6982\n",
      "Epoch [175/500], Batch [20/23], Loss: 8217.9131\n",
      "Epoch [176/500], Batch [5/23], Loss: 5436.1021\n",
      "Epoch [176/500], Batch [10/23], Loss: 2810.8782\n",
      "Epoch [176/500], Batch [15/23], Loss: 3158.6724\n",
      "Epoch [176/500], Batch [20/23], Loss: 2111.6433\n",
      "Epoch [177/500], Batch [5/23], Loss: 1751.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [177/500], Batch [10/23], Loss: 3744.3584\n",
      "Epoch [177/500], Batch [15/23], Loss: 3738.5120\n",
      "Epoch [177/500], Batch [20/23], Loss: 2316.9241\n",
      "Epoch [178/500], Batch [5/23], Loss: 5685.3340\n",
      "Epoch [178/500], Batch [10/23], Loss: 3404.7529\n",
      "Epoch [178/500], Batch [15/23], Loss: 2409.9043\n",
      "Epoch [178/500], Batch [20/23], Loss: 3911.3584\n",
      "Epoch [179/500], Batch [5/23], Loss: 2680.6260\n",
      "Epoch [179/500], Batch [10/23], Loss: 4317.6182\n",
      "Epoch [179/500], Batch [15/23], Loss: 2197.7588\n",
      "Epoch [179/500], Batch [20/23], Loss: 2640.0276\n",
      "Epoch [180/500], Batch [5/23], Loss: 2218.1387\n",
      "Epoch [180/500], Batch [10/23], Loss: 1439.3127\n",
      "Epoch [180/500], Batch [15/23], Loss: 3154.6792\n",
      "Epoch [180/500], Batch [20/23], Loss: 5669.5771\n",
      "Epoch [181/500], Batch [5/23], Loss: 4120.5430\n",
      "Epoch [181/500], Batch [10/23], Loss: 2223.2754\n",
      "Epoch [181/500], Batch [15/23], Loss: 9943.7480\n",
      "Epoch [181/500], Batch [20/23], Loss: 2785.6458\n",
      "Epoch [182/500], Batch [5/23], Loss: 3440.4211\n",
      "Epoch [182/500], Batch [10/23], Loss: 1990.7134\n",
      "Epoch [182/500], Batch [15/23], Loss: 3024.2244\n",
      "Epoch [182/500], Batch [20/23], Loss: 3367.6763\n",
      "Epoch [183/500], Batch [5/23], Loss: 1883.4556\n",
      "Epoch [183/500], Batch [10/23], Loss: 3827.2681\n",
      "Epoch [183/500], Batch [15/23], Loss: 1777.5138\n",
      "Epoch [183/500], Batch [20/23], Loss: 5788.1499\n",
      "Epoch [184/500], Batch [5/23], Loss: 1877.7344\n",
      "Epoch [184/500], Batch [10/23], Loss: 2724.0176\n",
      "Epoch [184/500], Batch [15/23], Loss: 2429.7441\n",
      "Epoch [184/500], Batch [20/23], Loss: 3004.1602\n",
      "Epoch [185/500], Batch [5/23], Loss: 2169.2720\n",
      "Epoch [185/500], Batch [10/23], Loss: 2281.1196\n",
      "Epoch [185/500], Batch [15/23], Loss: 3317.7351\n",
      "Epoch [185/500], Batch [20/23], Loss: 1491.1013\n",
      "Epoch [186/500], Batch [5/23], Loss: 4409.7666\n",
      "Epoch [186/500], Batch [10/23], Loss: 4098.9102\n",
      "Epoch [186/500], Batch [15/23], Loss: 3032.7849\n",
      "Epoch [186/500], Batch [20/23], Loss: 4096.7651\n",
      "Epoch [187/500], Batch [5/23], Loss: 4194.0928\n",
      "Epoch [187/500], Batch [10/23], Loss: 5295.9141\n",
      "Epoch [187/500], Batch [15/23], Loss: 3673.7458\n",
      "Epoch [187/500], Batch [20/23], Loss: 2418.6377\n",
      "Epoch [188/500], Batch [5/23], Loss: 5621.3174\n",
      "Epoch [188/500], Batch [10/23], Loss: 1040.7102\n",
      "Epoch [188/500], Batch [15/23], Loss: 1802.0192\n",
      "Epoch [188/500], Batch [20/23], Loss: 2241.9980\n",
      "Epoch [189/500], Batch [5/23], Loss: 3720.6248\n",
      "Epoch [189/500], Batch [10/23], Loss: 1942.4575\n",
      "Epoch [189/500], Batch [15/23], Loss: 1334.4548\n",
      "Epoch [189/500], Batch [20/23], Loss: 2774.9688\n",
      "Epoch [190/500], Batch [5/23], Loss: 2643.3958\n",
      "Epoch [190/500], Batch [10/23], Loss: 1518.9504\n",
      "Epoch [190/500], Batch [15/23], Loss: 3426.9375\n",
      "Epoch [190/500], Batch [20/23], Loss: 2048.9331\n",
      "Epoch [191/500], Batch [5/23], Loss: 2761.9990\n",
      "Epoch [191/500], Batch [10/23], Loss: 3092.2583\n",
      "Epoch [191/500], Batch [15/23], Loss: 4369.1240\n",
      "Epoch [191/500], Batch [20/23], Loss: 2515.3486\n",
      "Epoch [192/500], Batch [5/23], Loss: 2271.3848\n",
      "Epoch [192/500], Batch [10/23], Loss: 1642.4534\n",
      "Epoch [192/500], Batch [15/23], Loss: 1656.4122\n",
      "Epoch [192/500], Batch [20/23], Loss: 4016.2598\n",
      "Epoch [193/500], Batch [5/23], Loss: 2729.5088\n",
      "Epoch [193/500], Batch [10/23], Loss: 6669.3945\n",
      "Epoch [193/500], Batch [15/23], Loss: 2745.5239\n",
      "Epoch [193/500], Batch [20/23], Loss: 3627.6465\n",
      "Epoch [194/500], Batch [5/23], Loss: 3287.6929\n",
      "Epoch [194/500], Batch [10/23], Loss: 1332.6196\n",
      "Epoch [194/500], Batch [15/23], Loss: 2312.5972\n",
      "Epoch [194/500], Batch [20/23], Loss: 2756.2827\n",
      "Epoch [195/500], Batch [5/23], Loss: 3531.1880\n",
      "Epoch [195/500], Batch [10/23], Loss: 2629.7515\n",
      "Epoch [195/500], Batch [15/23], Loss: 2716.2053\n",
      "Epoch [195/500], Batch [20/23], Loss: 2221.9438\n",
      "Epoch [196/500], Batch [5/23], Loss: 2024.7745\n",
      "Epoch [196/500], Batch [10/23], Loss: 2030.9652\n",
      "Epoch [196/500], Batch [15/23], Loss: 3000.9807\n",
      "Epoch [196/500], Batch [20/23], Loss: 6820.9136\n",
      "Epoch [197/500], Batch [5/23], Loss: 3436.7827\n",
      "Epoch [197/500], Batch [10/23], Loss: 2757.0386\n",
      "Epoch [197/500], Batch [15/23], Loss: 1608.9531\n",
      "Epoch [197/500], Batch [20/23], Loss: 3156.6289\n",
      "Epoch [198/500], Batch [5/23], Loss: 1532.5369\n",
      "Epoch [198/500], Batch [10/23], Loss: 1926.6517\n",
      "Epoch [198/500], Batch [15/23], Loss: 2892.0166\n",
      "Epoch [198/500], Batch [20/23], Loss: 2636.6208\n",
      "Epoch [199/500], Batch [5/23], Loss: 2062.3679\n",
      "Epoch [199/500], Batch [10/23], Loss: 3027.5186\n",
      "Epoch [199/500], Batch [15/23], Loss: 1846.6467\n",
      "Epoch [199/500], Batch [20/23], Loss: 975.4156\n",
      "Epoch [200/500], Batch [5/23], Loss: 3923.8013\n",
      "Epoch [200/500], Batch [10/23], Loss: 3949.2185\n",
      "Epoch [200/500], Batch [15/23], Loss: 2669.3853\n",
      "Epoch [200/500], Batch [20/23], Loss: 1936.5889\n",
      "Epoch [201/500], Batch [5/23], Loss: 4339.2617\n",
      "Epoch [201/500], Batch [10/23], Loss: 5859.4697\n",
      "Epoch [201/500], Batch [15/23], Loss: 2651.1443\n",
      "Epoch [201/500], Batch [20/23], Loss: 2861.2495\n",
      "Epoch [202/500], Batch [5/23], Loss: 2681.8792\n",
      "Epoch [202/500], Batch [10/23], Loss: 2888.5552\n",
      "Epoch [202/500], Batch [15/23], Loss: 3663.2412\n",
      "Epoch [202/500], Batch [20/23], Loss: 2911.2222\n",
      "Epoch [203/500], Batch [5/23], Loss: 3268.3320\n",
      "Epoch [203/500], Batch [10/23], Loss: 3059.0950\n",
      "Epoch [203/500], Batch [15/23], Loss: 2348.0747\n",
      "Epoch [203/500], Batch [20/23], Loss: 3204.5259\n",
      "Epoch [204/500], Batch [5/23], Loss: 1086.6152\n",
      "Epoch [204/500], Batch [10/23], Loss: 4268.9629\n",
      "Epoch [204/500], Batch [15/23], Loss: 3836.5667\n",
      "Epoch [204/500], Batch [20/23], Loss: 2437.1304\n",
      "Epoch [205/500], Batch [5/23], Loss: 4490.3057\n",
      "Epoch [205/500], Batch [10/23], Loss: 4026.1997\n",
      "Epoch [205/500], Batch [15/23], Loss: 3189.1499\n",
      "Epoch [205/500], Batch [20/23], Loss: 3318.9946\n",
      "Epoch [206/500], Batch [5/23], Loss: 2124.4014\n",
      "Epoch [206/500], Batch [10/23], Loss: 3130.4253\n",
      "Epoch [206/500], Batch [15/23], Loss: 3989.1440\n",
      "Epoch [206/500], Batch [20/23], Loss: 3525.3994\n",
      "Epoch [207/500], Batch [5/23], Loss: 5618.1108\n",
      "Epoch [207/500], Batch [10/23], Loss: 3035.7708\n",
      "Epoch [207/500], Batch [15/23], Loss: 8586.8281\n",
      "Epoch [207/500], Batch [20/23], Loss: 5195.2222\n",
      "Epoch [208/500], Batch [5/23], Loss: 4118.1328\n",
      "Epoch [208/500], Batch [10/23], Loss: 5433.7275\n",
      "Epoch [208/500], Batch [15/23], Loss: 1189.3295\n",
      "Epoch [208/500], Batch [20/23], Loss: 1286.0046\n",
      "Epoch [209/500], Batch [5/23], Loss: 1928.1274\n",
      "Epoch [209/500], Batch [10/23], Loss: 2427.3386\n",
      "Epoch [209/500], Batch [15/23], Loss: 1817.1562\n",
      "Epoch [209/500], Batch [20/23], Loss: 4694.4932\n",
      "Epoch [210/500], Batch [5/23], Loss: 2899.1221\n",
      "Epoch [210/500], Batch [10/23], Loss: 1411.1411\n",
      "Epoch [210/500], Batch [15/23], Loss: 1918.9777\n",
      "Epoch [210/500], Batch [20/23], Loss: 1513.1765\n",
      "Epoch [211/500], Batch [5/23], Loss: 7379.6152\n",
      "Epoch [211/500], Batch [10/23], Loss: 3238.1245\n",
      "Epoch [211/500], Batch [15/23], Loss: 1914.2612\n",
      "Epoch [211/500], Batch [20/23], Loss: 3393.3425\n",
      "Epoch [212/500], Batch [5/23], Loss: 4089.4077\n",
      "Epoch [212/500], Batch [10/23], Loss: 6751.0684\n",
      "Epoch [212/500], Batch [15/23], Loss: 2645.7002\n",
      "Epoch [212/500], Batch [20/23], Loss: 2920.4707\n",
      "Epoch [213/500], Batch [5/23], Loss: 2060.6519\n",
      "Epoch [213/500], Batch [10/23], Loss: 1595.9290\n",
      "Epoch [213/500], Batch [15/23], Loss: 3998.1304\n",
      "Epoch [213/500], Batch [20/23], Loss: 2244.5291\n",
      "Epoch [214/500], Batch [5/23], Loss: 3007.7969\n",
      "Epoch [214/500], Batch [10/23], Loss: 1936.4492\n",
      "Epoch [214/500], Batch [15/23], Loss: 3266.9629\n",
      "Epoch [214/500], Batch [20/23], Loss: 2502.0354\n",
      "Epoch [215/500], Batch [5/23], Loss: 4226.3506\n",
      "Epoch [215/500], Batch [10/23], Loss: 1709.4364\n",
      "Epoch [215/500], Batch [15/23], Loss: 5551.9307\n",
      "Epoch [215/500], Batch [20/23], Loss: 2309.6128\n",
      "Epoch [216/500], Batch [5/23], Loss: 2243.5664\n",
      "Epoch [216/500], Batch [10/23], Loss: 3993.0146\n",
      "Epoch [216/500], Batch [15/23], Loss: 3273.9832\n",
      "Epoch [216/500], Batch [20/23], Loss: 1147.9896\n",
      "Epoch [217/500], Batch [5/23], Loss: 3371.2085\n",
      "Epoch [217/500], Batch [10/23], Loss: 1904.6477\n",
      "Epoch [217/500], Batch [15/23], Loss: 3376.6992\n",
      "Epoch [217/500], Batch [20/23], Loss: 1310.3657\n",
      "Epoch [218/500], Batch [5/23], Loss: 4635.3672\n",
      "Epoch [218/500], Batch [10/23], Loss: 4966.6602\n",
      "Epoch [218/500], Batch [15/23], Loss: 6048.6738\n",
      "Epoch [218/500], Batch [20/23], Loss: 3304.1731\n",
      "Epoch [219/500], Batch [5/23], Loss: 3492.3608\n",
      "Epoch [219/500], Batch [10/23], Loss: 2666.0833\n",
      "Epoch [219/500], Batch [15/23], Loss: 2369.7742\n",
      "Epoch [219/500], Batch [20/23], Loss: 3982.4500\n",
      "Epoch [220/500], Batch [5/23], Loss: 2486.1704\n",
      "Epoch [220/500], Batch [10/23], Loss: 1932.1934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/500], Batch [15/23], Loss: 1891.3589\n",
      "Epoch [220/500], Batch [20/23], Loss: 4946.2710\n",
      "Epoch [221/500], Batch [5/23], Loss: 2119.5842\n",
      "Epoch [221/500], Batch [10/23], Loss: 2317.3774\n",
      "Epoch [221/500], Batch [15/23], Loss: 3598.0100\n",
      "Epoch [221/500], Batch [20/23], Loss: 3656.7119\n",
      "Epoch [222/500], Batch [5/23], Loss: 1484.6097\n",
      "Epoch [222/500], Batch [10/23], Loss: 1372.0193\n",
      "Epoch [222/500], Batch [15/23], Loss: 1891.3309\n",
      "Epoch [222/500], Batch [20/23], Loss: 4880.9316\n",
      "Epoch [223/500], Batch [5/23], Loss: 5261.0254\n",
      "Epoch [223/500], Batch [10/23], Loss: 2662.8062\n",
      "Epoch [223/500], Batch [15/23], Loss: 1467.4210\n",
      "Epoch [223/500], Batch [20/23], Loss: 3114.1470\n",
      "Epoch [224/500], Batch [5/23], Loss: 3498.8330\n",
      "Epoch [224/500], Batch [10/23], Loss: 7127.2500\n",
      "Epoch [224/500], Batch [15/23], Loss: 4144.9819\n",
      "Epoch [224/500], Batch [20/23], Loss: 1985.4536\n",
      "Epoch [225/500], Batch [5/23], Loss: 2245.1365\n",
      "Epoch [225/500], Batch [10/23], Loss: 2520.3074\n",
      "Epoch [225/500], Batch [15/23], Loss: 1812.6049\n",
      "Epoch [225/500], Batch [20/23], Loss: 5207.7925\n",
      "Epoch [226/500], Batch [5/23], Loss: 2808.1782\n",
      "Epoch [226/500], Batch [10/23], Loss: 1322.8057\n",
      "Epoch [226/500], Batch [15/23], Loss: 791.5016\n",
      "Epoch [226/500], Batch [20/23], Loss: 5901.0239\n",
      "Epoch [227/500], Batch [5/23], Loss: 2970.7422\n",
      "Epoch [227/500], Batch [10/23], Loss: 7272.4023\n",
      "Epoch [227/500], Batch [15/23], Loss: 2444.5601\n",
      "Epoch [227/500], Batch [20/23], Loss: 2288.6812\n",
      "Epoch [228/500], Batch [5/23], Loss: 3332.5276\n",
      "Epoch [228/500], Batch [10/23], Loss: 2009.4481\n",
      "Epoch [228/500], Batch [15/23], Loss: 2210.3462\n",
      "Epoch [228/500], Batch [20/23], Loss: 1399.5598\n",
      "Epoch [229/500], Batch [5/23], Loss: 1996.1357\n",
      "Epoch [229/500], Batch [10/23], Loss: 2035.5691\n",
      "Epoch [229/500], Batch [15/23], Loss: 6739.9341\n",
      "Epoch [229/500], Batch [20/23], Loss: 1192.5564\n",
      "Epoch [230/500], Batch [5/23], Loss: 2315.3657\n",
      "Epoch [230/500], Batch [10/23], Loss: 2889.5293\n",
      "Epoch [230/500], Batch [15/23], Loss: 2140.2622\n",
      "Epoch [230/500], Batch [20/23], Loss: 2237.8813\n",
      "Epoch [231/500], Batch [5/23], Loss: 2950.9529\n",
      "Epoch [231/500], Batch [10/23], Loss: 3055.5522\n",
      "Epoch [231/500], Batch [15/23], Loss: 5132.4980\n",
      "Epoch [231/500], Batch [20/23], Loss: 2603.7244\n",
      "Epoch [232/500], Batch [5/23], Loss: 4957.2119\n",
      "Epoch [232/500], Batch [10/23], Loss: 1397.5618\n",
      "Epoch [232/500], Batch [15/23], Loss: 4781.4482\n",
      "Epoch [232/500], Batch [20/23], Loss: 1491.4594\n",
      "Epoch [233/500], Batch [5/23], Loss: 4755.8989\n",
      "Epoch [233/500], Batch [10/23], Loss: 3891.6499\n",
      "Epoch [233/500], Batch [15/23], Loss: 3873.7222\n",
      "Epoch [233/500], Batch [20/23], Loss: 2352.4001\n",
      "Epoch [234/500], Batch [5/23], Loss: 2622.1555\n",
      "Epoch [234/500], Batch [10/23], Loss: 3358.4453\n",
      "Epoch [234/500], Batch [15/23], Loss: 4102.6753\n",
      "Epoch [234/500], Batch [20/23], Loss: 1150.9097\n",
      "Epoch [235/500], Batch [5/23], Loss: 2877.5637\n",
      "Epoch [235/500], Batch [10/23], Loss: 5895.4331\n",
      "Epoch [235/500], Batch [15/23], Loss: 2886.7251\n",
      "Epoch [235/500], Batch [20/23], Loss: 2461.8774\n",
      "Epoch [236/500], Batch [5/23], Loss: 3474.4448\n",
      "Epoch [236/500], Batch [10/23], Loss: 1142.3679\n",
      "Epoch [236/500], Batch [15/23], Loss: 1614.2296\n",
      "Epoch [236/500], Batch [20/23], Loss: 2356.9473\n",
      "Epoch [237/500], Batch [5/23], Loss: 2508.9199\n",
      "Epoch [237/500], Batch [10/23], Loss: 1824.1483\n",
      "Epoch [237/500], Batch [15/23], Loss: 1895.0408\n",
      "Epoch [237/500], Batch [20/23], Loss: 4640.7749\n",
      "Epoch [238/500], Batch [5/23], Loss: 10959.0947\n",
      "Epoch [238/500], Batch [10/23], Loss: 2346.4326\n",
      "Epoch [238/500], Batch [15/23], Loss: 5071.4229\n",
      "Epoch [238/500], Batch [20/23], Loss: 3820.3469\n",
      "Epoch [239/500], Batch [5/23], Loss: 1371.9657\n",
      "Epoch [239/500], Batch [10/23], Loss: 3120.8232\n",
      "Epoch [239/500], Batch [15/23], Loss: 2661.2349\n",
      "Epoch [239/500], Batch [20/23], Loss: 1171.8024\n",
      "Epoch [240/500], Batch [5/23], Loss: 1898.7885\n",
      "Epoch [240/500], Batch [10/23], Loss: 1655.3851\n",
      "Epoch [240/500], Batch [15/23], Loss: 4554.7085\n",
      "Epoch [240/500], Batch [20/23], Loss: 1941.1560\n",
      "Epoch [241/500], Batch [5/23], Loss: 5046.6079\n",
      "Epoch [241/500], Batch [10/23], Loss: 2812.7314\n",
      "Epoch [241/500], Batch [15/23], Loss: 3040.7622\n",
      "Epoch [241/500], Batch [20/23], Loss: 3516.2939\n",
      "Epoch [242/500], Batch [5/23], Loss: 2534.0327\n",
      "Epoch [242/500], Batch [10/23], Loss: 2738.4314\n",
      "Epoch [242/500], Batch [15/23], Loss: 1411.6829\n",
      "Epoch [242/500], Batch [20/23], Loss: 1542.6796\n",
      "Epoch [243/500], Batch [5/23], Loss: 3637.6475\n",
      "Epoch [243/500], Batch [10/23], Loss: 2493.2949\n",
      "Epoch [243/500], Batch [15/23], Loss: 2132.3572\n",
      "Epoch [243/500], Batch [20/23], Loss: 5455.9727\n",
      "Epoch [244/500], Batch [5/23], Loss: 4495.7856\n",
      "Epoch [244/500], Batch [10/23], Loss: 11653.1445\n",
      "Epoch [244/500], Batch [15/23], Loss: 4222.9814\n",
      "Epoch [244/500], Batch [20/23], Loss: 1888.6276\n",
      "Epoch [245/500], Batch [5/23], Loss: 2186.5483\n",
      "Epoch [245/500], Batch [10/23], Loss: 3365.6543\n",
      "Epoch [245/500], Batch [15/23], Loss: 1644.8826\n",
      "Epoch [245/500], Batch [20/23], Loss: 2654.1338\n",
      "Epoch [246/500], Batch [5/23], Loss: 10039.5488\n",
      "Epoch [246/500], Batch [10/23], Loss: 24460.6953\n",
      "Epoch [246/500], Batch [15/23], Loss: 15863.6377\n",
      "Epoch [246/500], Batch [20/23], Loss: 6177.3931\n",
      "Epoch [247/500], Batch [5/23], Loss: 1443.2854\n",
      "Epoch [247/500], Batch [10/23], Loss: 8327.6582\n",
      "Epoch [247/500], Batch [15/23], Loss: 2300.8545\n",
      "Epoch [247/500], Batch [20/23], Loss: 2899.1655\n",
      "Epoch [248/500], Batch [5/23], Loss: 1780.4939\n",
      "Epoch [248/500], Batch [10/23], Loss: 4627.1260\n",
      "Epoch [248/500], Batch [15/23], Loss: 1564.5439\n",
      "Epoch [248/500], Batch [20/23], Loss: 2091.1621\n",
      "Epoch [249/500], Batch [5/23], Loss: 2981.1660\n",
      "Epoch [249/500], Batch [10/23], Loss: 5151.7090\n",
      "Epoch [249/500], Batch [15/23], Loss: 2048.3225\n",
      "Epoch [249/500], Batch [20/23], Loss: 6906.2646\n",
      "Epoch [250/500], Batch [5/23], Loss: 2745.8599\n",
      "Epoch [250/500], Batch [10/23], Loss: 3530.4465\n",
      "Epoch [250/500], Batch [15/23], Loss: 1632.3339\n",
      "Epoch [250/500], Batch [20/23], Loss: 2101.4521\n",
      "Epoch [251/500], Batch [5/23], Loss: 2563.0713\n",
      "Epoch [251/500], Batch [10/23], Loss: 4047.8911\n",
      "Epoch [251/500], Batch [15/23], Loss: 2669.3838\n",
      "Epoch [251/500], Batch [20/23], Loss: 3752.2097\n",
      "Epoch [252/500], Batch [5/23], Loss: 1236.0144\n",
      "Epoch [252/500], Batch [10/23], Loss: 1982.4973\n",
      "Epoch [252/500], Batch [15/23], Loss: 6624.1074\n",
      "Epoch [252/500], Batch [20/23], Loss: 1711.2488\n",
      "Epoch [253/500], Batch [5/23], Loss: 1979.3545\n",
      "Epoch [253/500], Batch [10/23], Loss: 3412.5618\n",
      "Epoch [253/500], Batch [15/23], Loss: 1216.1045\n",
      "Epoch [253/500], Batch [20/23], Loss: 4157.0640\n",
      "Epoch [254/500], Batch [5/23], Loss: 1727.4379\n",
      "Epoch [254/500], Batch [10/23], Loss: 4167.1694\n",
      "Epoch [254/500], Batch [15/23], Loss: 3121.8398\n",
      "Epoch [254/500], Batch [20/23], Loss: 4537.5713\n",
      "Epoch [255/500], Batch [5/23], Loss: 1903.6755\n",
      "Epoch [255/500], Batch [10/23], Loss: 1613.4033\n",
      "Epoch [255/500], Batch [15/23], Loss: 3874.4790\n",
      "Epoch [255/500], Batch [20/23], Loss: 1477.2471\n",
      "Epoch [256/500], Batch [5/23], Loss: 2271.1353\n",
      "Epoch [256/500], Batch [10/23], Loss: 1197.8990\n",
      "Epoch [256/500], Batch [15/23], Loss: 2862.1270\n",
      "Epoch [256/500], Batch [20/23], Loss: 2342.4902\n",
      "Epoch [257/500], Batch [5/23], Loss: 2373.9746\n",
      "Epoch [257/500], Batch [10/23], Loss: 2482.4170\n",
      "Epoch [257/500], Batch [15/23], Loss: 9345.8672\n",
      "Epoch [257/500], Batch [20/23], Loss: 1493.2715\n",
      "Epoch [258/500], Batch [5/23], Loss: 1967.7271\n",
      "Epoch [258/500], Batch [10/23], Loss: 1559.3020\n",
      "Epoch [258/500], Batch [15/23], Loss: 7041.9097\n",
      "Epoch [258/500], Batch [20/23], Loss: 2617.3245\n",
      "Epoch [259/500], Batch [5/23], Loss: 1492.7595\n",
      "Epoch [259/500], Batch [10/23], Loss: 2405.9736\n",
      "Epoch [259/500], Batch [15/23], Loss: 3525.0237\n",
      "Epoch [259/500], Batch [20/23], Loss: 2648.2156\n",
      "Epoch [260/500], Batch [5/23], Loss: 3530.3076\n",
      "Epoch [260/500], Batch [10/23], Loss: 2385.2195\n",
      "Epoch [260/500], Batch [15/23], Loss: 3134.1692\n",
      "Epoch [260/500], Batch [20/23], Loss: 3910.9863\n",
      "Epoch [261/500], Batch [5/23], Loss: 3288.8105\n",
      "Epoch [261/500], Batch [10/23], Loss: 5375.9805\n",
      "Epoch [261/500], Batch [15/23], Loss: 8637.9023\n",
      "Epoch [261/500], Batch [20/23], Loss: 3482.0996\n",
      "Epoch [262/500], Batch [5/23], Loss: 2468.3137\n",
      "Epoch [262/500], Batch [10/23], Loss: 3168.0972\n",
      "Epoch [262/500], Batch [15/23], Loss: 2470.4858\n",
      "Epoch [262/500], Batch [20/23], Loss: 3475.7605\n",
      "Epoch [263/500], Batch [5/23], Loss: 1615.7783\n",
      "Epoch [263/500], Batch [10/23], Loss: 3132.0190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [263/500], Batch [15/23], Loss: 2041.8789\n",
      "Epoch [263/500], Batch [20/23], Loss: 4864.2852\n",
      "Epoch [264/500], Batch [5/23], Loss: 1381.5549\n",
      "Epoch [264/500], Batch [10/23], Loss: 7494.5859\n",
      "Epoch [264/500], Batch [15/23], Loss: 3072.7024\n",
      "Epoch [264/500], Batch [20/23], Loss: 3105.2759\n",
      "Epoch [265/500], Batch [5/23], Loss: 2874.4275\n",
      "Epoch [265/500], Batch [10/23], Loss: 4783.7944\n",
      "Epoch [265/500], Batch [15/23], Loss: 3810.0339\n",
      "Epoch [265/500], Batch [20/23], Loss: 3629.6812\n",
      "Epoch [266/500], Batch [5/23], Loss: 2883.6516\n",
      "Epoch [266/500], Batch [10/23], Loss: 4822.5527\n",
      "Epoch [266/500], Batch [15/23], Loss: 2902.5049\n",
      "Epoch [266/500], Batch [20/23], Loss: 3443.5781\n",
      "Epoch [267/500], Batch [5/23], Loss: 2816.7422\n",
      "Epoch [267/500], Batch [10/23], Loss: 3717.2783\n",
      "Epoch [267/500], Batch [15/23], Loss: 2365.8657\n",
      "Epoch [267/500], Batch [20/23], Loss: 3437.9141\n",
      "Epoch [268/500], Batch [5/23], Loss: 1989.2892\n",
      "Epoch [268/500], Batch [10/23], Loss: 1760.2925\n",
      "Epoch [268/500], Batch [15/23], Loss: 3862.3286\n",
      "Epoch [268/500], Batch [20/23], Loss: 6733.9658\n",
      "Epoch [269/500], Batch [5/23], Loss: 1875.2305\n",
      "Epoch [269/500], Batch [10/23], Loss: 3563.2415\n",
      "Epoch [269/500], Batch [15/23], Loss: 2787.7026\n",
      "Epoch [269/500], Batch [20/23], Loss: 2709.7622\n",
      "Epoch [270/500], Batch [5/23], Loss: 5058.4150\n",
      "Epoch [270/500], Batch [10/23], Loss: 1370.2706\n",
      "Epoch [270/500], Batch [15/23], Loss: 1795.3704\n",
      "Epoch [270/500], Batch [20/23], Loss: 1327.9919\n",
      "Epoch [271/500], Batch [5/23], Loss: 2002.1196\n",
      "Epoch [271/500], Batch [10/23], Loss: 2249.0271\n",
      "Epoch [271/500], Batch [15/23], Loss: 1604.5425\n",
      "Epoch [271/500], Batch [20/23], Loss: 6213.6470\n",
      "Epoch [272/500], Batch [5/23], Loss: 2609.1099\n",
      "Epoch [272/500], Batch [10/23], Loss: 2209.7510\n",
      "Epoch [272/500], Batch [15/23], Loss: 1338.5469\n",
      "Epoch [272/500], Batch [20/23], Loss: 2106.9072\n",
      "Epoch [273/500], Batch [5/23], Loss: 2368.0015\n",
      "Epoch [273/500], Batch [10/23], Loss: 5182.8203\n",
      "Epoch [273/500], Batch [15/23], Loss: 1900.9851\n",
      "Epoch [273/500], Batch [20/23], Loss: 2815.9512\n",
      "Epoch [274/500], Batch [5/23], Loss: 4621.9043\n",
      "Epoch [274/500], Batch [10/23], Loss: 5547.1787\n",
      "Epoch [274/500], Batch [15/23], Loss: 1714.7429\n",
      "Epoch [274/500], Batch [20/23], Loss: 1666.1082\n",
      "Epoch [275/500], Batch [5/23], Loss: 2226.4497\n",
      "Epoch [275/500], Batch [10/23], Loss: 1567.6482\n",
      "Epoch [275/500], Batch [15/23], Loss: 1345.4933\n",
      "Epoch [275/500], Batch [20/23], Loss: 2027.0793\n",
      "Epoch [276/500], Batch [5/23], Loss: 5595.3965\n",
      "Epoch [276/500], Batch [10/23], Loss: 2000.5812\n",
      "Epoch [276/500], Batch [15/23], Loss: 6180.3550\n",
      "Epoch [276/500], Batch [20/23], Loss: 2090.6270\n",
      "Epoch [277/500], Batch [5/23], Loss: 2360.4985\n",
      "Epoch [277/500], Batch [10/23], Loss: 3531.9907\n",
      "Epoch [277/500], Batch [15/23], Loss: 2768.1934\n",
      "Epoch [277/500], Batch [20/23], Loss: 3728.0381\n",
      "Epoch [278/500], Batch [5/23], Loss: 1493.7646\n",
      "Epoch [278/500], Batch [10/23], Loss: 3177.2231\n",
      "Epoch [278/500], Batch [15/23], Loss: 2991.9297\n",
      "Epoch [278/500], Batch [20/23], Loss: 2366.4749\n",
      "Epoch [279/500], Batch [5/23], Loss: 3326.1858\n",
      "Epoch [279/500], Batch [10/23], Loss: 1604.6010\n",
      "Epoch [279/500], Batch [15/23], Loss: 6824.2090\n",
      "Epoch [279/500], Batch [20/23], Loss: 2742.7388\n",
      "Epoch [280/500], Batch [5/23], Loss: 1403.3303\n",
      "Epoch [280/500], Batch [10/23], Loss: 6577.9668\n",
      "Epoch [280/500], Batch [15/23], Loss: 1595.8613\n",
      "Epoch [280/500], Batch [20/23], Loss: 1333.9099\n",
      "Epoch [281/500], Batch [5/23], Loss: 2442.9980\n",
      "Epoch [281/500], Batch [10/23], Loss: 1959.2220\n",
      "Epoch [281/500], Batch [15/23], Loss: 1537.1547\n",
      "Epoch [281/500], Batch [20/23], Loss: 1723.3036\n",
      "Epoch [282/500], Batch [5/23], Loss: 7527.5547\n",
      "Epoch [282/500], Batch [10/23], Loss: 2968.7849\n",
      "Epoch [282/500], Batch [15/23], Loss: 2399.7747\n",
      "Epoch [282/500], Batch [20/23], Loss: 2319.1470\n",
      "Epoch [283/500], Batch [5/23], Loss: 2704.5999\n",
      "Epoch [283/500], Batch [10/23], Loss: 2442.0974\n",
      "Epoch [283/500], Batch [15/23], Loss: 1679.3918\n",
      "Epoch [283/500], Batch [20/23], Loss: 1106.5000\n",
      "Epoch [284/500], Batch [5/23], Loss: 1140.8459\n",
      "Epoch [284/500], Batch [10/23], Loss: 4845.1030\n",
      "Epoch [284/500], Batch [15/23], Loss: 1764.5154\n",
      "Epoch [284/500], Batch [20/23], Loss: 6962.5278\n",
      "Epoch [285/500], Batch [5/23], Loss: 2958.4001\n",
      "Epoch [285/500], Batch [10/23], Loss: 7113.4663\n",
      "Epoch [285/500], Batch [15/23], Loss: 2581.5552\n",
      "Epoch [285/500], Batch [20/23], Loss: 3547.2949\n",
      "Epoch [286/500], Batch [5/23], Loss: 8196.2383\n",
      "Epoch [286/500], Batch [10/23], Loss: 6772.3486\n",
      "Epoch [286/500], Batch [15/23], Loss: 6519.5718\n",
      "Epoch [286/500], Batch [20/23], Loss: 6894.0645\n",
      "Epoch [287/500], Batch [5/23], Loss: 3726.6543\n",
      "Epoch [287/500], Batch [10/23], Loss: 3484.2317\n",
      "Epoch [287/500], Batch [15/23], Loss: 3763.6230\n",
      "Epoch [287/500], Batch [20/23], Loss: 2823.6746\n",
      "Epoch [288/500], Batch [5/23], Loss: 3125.5693\n",
      "Epoch [288/500], Batch [10/23], Loss: 2485.4236\n",
      "Epoch [288/500], Batch [15/23], Loss: 976.3392\n",
      "Epoch [288/500], Batch [20/23], Loss: 4205.6748\n",
      "Epoch [289/500], Batch [5/23], Loss: 1996.0759\n",
      "Epoch [289/500], Batch [10/23], Loss: 3924.8203\n",
      "Epoch [289/500], Batch [15/23], Loss: 2608.8296\n",
      "Epoch [289/500], Batch [20/23], Loss: 2040.3643\n",
      "Epoch [290/500], Batch [5/23], Loss: 1882.6445\n",
      "Epoch [290/500], Batch [10/23], Loss: 3912.0781\n",
      "Epoch [290/500], Batch [15/23], Loss: 2038.4445\n",
      "Epoch [290/500], Batch [20/23], Loss: 2530.4868\n",
      "Epoch [291/500], Batch [5/23], Loss: 2794.7554\n",
      "Epoch [291/500], Batch [10/23], Loss: 3325.4370\n",
      "Epoch [291/500], Batch [15/23], Loss: 3503.3286\n",
      "Epoch [291/500], Batch [20/23], Loss: 2421.9658\n",
      "Epoch [292/500], Batch [5/23], Loss: 6027.5986\n",
      "Epoch [292/500], Batch [10/23], Loss: 2109.6133\n",
      "Epoch [292/500], Batch [15/23], Loss: 3259.7920\n",
      "Epoch [292/500], Batch [20/23], Loss: 4327.3940\n",
      "Epoch [293/500], Batch [5/23], Loss: 2355.3420\n",
      "Epoch [293/500], Batch [10/23], Loss: 2913.0352\n",
      "Epoch [293/500], Batch [15/23], Loss: 4117.0840\n",
      "Epoch [293/500], Batch [20/23], Loss: 1407.1447\n",
      "Epoch [294/500], Batch [5/23], Loss: 1530.8240\n",
      "Epoch [294/500], Batch [10/23], Loss: 1295.6536\n",
      "Epoch [294/500], Batch [15/23], Loss: 2425.6987\n",
      "Epoch [294/500], Batch [20/23], Loss: 8135.7295\n",
      "Epoch [295/500], Batch [5/23], Loss: 817.2142\n",
      "Epoch [295/500], Batch [10/23], Loss: 1917.2452\n",
      "Epoch [295/500], Batch [15/23], Loss: 3540.8943\n",
      "Epoch [295/500], Batch [20/23], Loss: 1666.1196\n",
      "Epoch [296/500], Batch [5/23], Loss: 2949.2666\n",
      "Epoch [296/500], Batch [10/23], Loss: 1293.7356\n",
      "Epoch [296/500], Batch [15/23], Loss: 3804.4160\n",
      "Epoch [296/500], Batch [20/23], Loss: 1524.7405\n",
      "Epoch [297/500], Batch [5/23], Loss: 2138.0767\n",
      "Epoch [297/500], Batch [10/23], Loss: 2304.7109\n",
      "Epoch [297/500], Batch [15/23], Loss: 2616.1089\n",
      "Epoch [297/500], Batch [20/23], Loss: 3037.0347\n",
      "Epoch [298/500], Batch [5/23], Loss: 988.4291\n",
      "Epoch [298/500], Batch [10/23], Loss: 3842.2119\n",
      "Epoch [298/500], Batch [15/23], Loss: 11335.8320\n",
      "Epoch [298/500], Batch [20/23], Loss: 3219.7258\n",
      "Epoch [299/500], Batch [5/23], Loss: 1498.1588\n",
      "Epoch [299/500], Batch [10/23], Loss: 2354.8423\n",
      "Epoch [299/500], Batch [15/23], Loss: 3281.1526\n",
      "Epoch [299/500], Batch [20/23], Loss: 3125.4919\n",
      "Epoch [300/500], Batch [5/23], Loss: 2665.0552\n",
      "Epoch [300/500], Batch [10/23], Loss: 1260.8701\n",
      "Epoch [300/500], Batch [15/23], Loss: 2588.6851\n",
      "Epoch [300/500], Batch [20/23], Loss: 3039.5400\n",
      "Epoch [301/500], Batch [5/23], Loss: 1933.9280\n",
      "Epoch [301/500], Batch [10/23], Loss: 2999.9585\n",
      "Epoch [301/500], Batch [15/23], Loss: 1069.9352\n",
      "Epoch [301/500], Batch [20/23], Loss: 2233.7253\n",
      "Epoch [302/500], Batch [5/23], Loss: 3411.2378\n",
      "Epoch [302/500], Batch [10/23], Loss: 2931.8259\n",
      "Epoch [302/500], Batch [15/23], Loss: 2349.2627\n",
      "Epoch [302/500], Batch [20/23], Loss: 1615.0037\n",
      "Epoch [303/500], Batch [5/23], Loss: 3798.7803\n",
      "Epoch [303/500], Batch [10/23], Loss: 1684.3926\n",
      "Epoch [303/500], Batch [15/23], Loss: 2830.5415\n",
      "Epoch [303/500], Batch [20/23], Loss: 3604.4858\n",
      "Epoch [304/500], Batch [5/23], Loss: 2858.1848\n",
      "Epoch [304/500], Batch [10/23], Loss: 3927.2966\n",
      "Epoch [304/500], Batch [15/23], Loss: 2207.5713\n",
      "Epoch [304/500], Batch [20/23], Loss: 2019.3333\n",
      "Epoch [305/500], Batch [5/23], Loss: 2397.0640\n",
      "Epoch [305/500], Batch [10/23], Loss: 2769.8032\n",
      "Epoch [305/500], Batch [15/23], Loss: 1334.5247\n",
      "Epoch [305/500], Batch [20/23], Loss: 4002.0098\n",
      "Epoch [306/500], Batch [5/23], Loss: 1837.4504\n",
      "Epoch [306/500], Batch [10/23], Loss: 2235.0347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [306/500], Batch [15/23], Loss: 1795.7606\n",
      "Epoch [306/500], Batch [20/23], Loss: 5166.0991\n",
      "Epoch [307/500], Batch [5/23], Loss: 9193.7188\n",
      "Epoch [307/500], Batch [10/23], Loss: 7497.7134\n",
      "Epoch [307/500], Batch [15/23], Loss: 11189.7598\n",
      "Epoch [307/500], Batch [20/23], Loss: 4841.9966\n",
      "Epoch [308/500], Batch [5/23], Loss: 4668.4424\n",
      "Epoch [308/500], Batch [10/23], Loss: 5450.5859\n",
      "Epoch [308/500], Batch [15/23], Loss: 6169.8413\n",
      "Epoch [308/500], Batch [20/23], Loss: 3211.2461\n",
      "Epoch [309/500], Batch [5/23], Loss: 2783.0869\n",
      "Epoch [309/500], Batch [10/23], Loss: 1523.4912\n",
      "Epoch [309/500], Batch [15/23], Loss: 2427.3833\n",
      "Epoch [309/500], Batch [20/23], Loss: 3387.6182\n",
      "Epoch [310/500], Batch [5/23], Loss: 984.6142\n",
      "Epoch [310/500], Batch [10/23], Loss: 3772.5947\n",
      "Epoch [310/500], Batch [15/23], Loss: 3470.1394\n",
      "Epoch [310/500], Batch [20/23], Loss: 2161.1030\n",
      "Epoch [311/500], Batch [5/23], Loss: 1958.7573\n",
      "Epoch [311/500], Batch [10/23], Loss: 1848.1641\n",
      "Epoch [311/500], Batch [15/23], Loss: 2461.2749\n",
      "Epoch [311/500], Batch [20/23], Loss: 3394.4929\n",
      "Epoch [312/500], Batch [5/23], Loss: 1360.8857\n",
      "Epoch [312/500], Batch [10/23], Loss: 4719.3818\n",
      "Epoch [312/500], Batch [15/23], Loss: 1659.9299\n",
      "Epoch [312/500], Batch [20/23], Loss: 1860.5322\n",
      "Epoch [313/500], Batch [5/23], Loss: 4633.6338\n",
      "Epoch [313/500], Batch [10/23], Loss: 1594.5819\n",
      "Epoch [313/500], Batch [15/23], Loss: 1152.3774\n",
      "Epoch [313/500], Batch [20/23], Loss: 3102.3887\n",
      "Epoch [314/500], Batch [5/23], Loss: 1672.3938\n",
      "Epoch [314/500], Batch [10/23], Loss: 1088.9503\n",
      "Epoch [314/500], Batch [15/23], Loss: 3262.5303\n",
      "Epoch [314/500], Batch [20/23], Loss: 2727.7478\n",
      "Epoch [315/500], Batch [5/23], Loss: 3483.7756\n",
      "Epoch [315/500], Batch [10/23], Loss: 3124.5168\n",
      "Epoch [315/500], Batch [15/23], Loss: 1867.1831\n",
      "Epoch [315/500], Batch [20/23], Loss: 4080.4507\n",
      "Epoch [316/500], Batch [5/23], Loss: 1881.2662\n",
      "Epoch [316/500], Batch [10/23], Loss: 2084.6724\n",
      "Epoch [316/500], Batch [15/23], Loss: 1524.3997\n",
      "Epoch [316/500], Batch [20/23], Loss: 3817.1323\n",
      "Epoch [317/500], Batch [5/23], Loss: 3699.2178\n",
      "Epoch [317/500], Batch [10/23], Loss: 2548.0715\n",
      "Epoch [317/500], Batch [15/23], Loss: 6748.2002\n",
      "Epoch [317/500], Batch [20/23], Loss: 2527.8931\n",
      "Epoch [318/500], Batch [5/23], Loss: 3618.6909\n",
      "Epoch [318/500], Batch [10/23], Loss: 3396.7988\n",
      "Epoch [318/500], Batch [15/23], Loss: 5415.0991\n",
      "Epoch [318/500], Batch [20/23], Loss: 6823.4282\n",
      "Epoch [319/500], Batch [5/23], Loss: 5557.9160\n",
      "Epoch [319/500], Batch [10/23], Loss: 4246.4780\n",
      "Epoch [319/500], Batch [15/23], Loss: 2382.9214\n",
      "Epoch [319/500], Batch [20/23], Loss: 1300.7419\n",
      "Epoch [320/500], Batch [5/23], Loss: 3819.9854\n",
      "Epoch [320/500], Batch [10/23], Loss: 1824.7170\n",
      "Epoch [320/500], Batch [15/23], Loss: 754.1619\n",
      "Epoch [320/500], Batch [20/23], Loss: 1205.1958\n",
      "Epoch [321/500], Batch [5/23], Loss: 6357.7617\n",
      "Epoch [321/500], Batch [10/23], Loss: 798.8814\n",
      "Epoch [321/500], Batch [15/23], Loss: 4302.1543\n",
      "Epoch [321/500], Batch [20/23], Loss: 2745.2559\n",
      "Epoch [322/500], Batch [5/23], Loss: 3643.5430\n",
      "Epoch [322/500], Batch [10/23], Loss: 3178.3369\n",
      "Epoch [322/500], Batch [15/23], Loss: 2328.6821\n",
      "Epoch [322/500], Batch [20/23], Loss: 1996.7720\n",
      "Epoch [323/500], Batch [5/23], Loss: 2384.8792\n",
      "Epoch [323/500], Batch [10/23], Loss: 2654.6064\n",
      "Epoch [323/500], Batch [15/23], Loss: 1088.4968\n",
      "Epoch [323/500], Batch [20/23], Loss: 2958.1736\n",
      "Epoch [324/500], Batch [5/23], Loss: 6287.9912\n",
      "Epoch [324/500], Batch [10/23], Loss: 4525.7803\n",
      "Epoch [324/500], Batch [15/23], Loss: 4334.9062\n",
      "Epoch [324/500], Batch [20/23], Loss: 3607.5215\n",
      "Epoch [325/500], Batch [5/23], Loss: 4721.4380\n",
      "Epoch [325/500], Batch [10/23], Loss: 1977.0153\n",
      "Epoch [325/500], Batch [15/23], Loss: 2215.9438\n",
      "Epoch [325/500], Batch [20/23], Loss: 3784.3511\n",
      "Epoch [326/500], Batch [5/23], Loss: 3320.5356\n",
      "Epoch [326/500], Batch [10/23], Loss: 2571.6924\n",
      "Epoch [326/500], Batch [15/23], Loss: 3023.9402\n",
      "Epoch [326/500], Batch [20/23], Loss: 2529.0657\n",
      "Epoch [327/500], Batch [5/23], Loss: 5685.4731\n",
      "Epoch [327/500], Batch [10/23], Loss: 1797.0057\n",
      "Epoch [327/500], Batch [15/23], Loss: 2006.7360\n",
      "Epoch [327/500], Batch [20/23], Loss: 1305.8114\n",
      "Epoch [328/500], Batch [5/23], Loss: 4834.2891\n",
      "Epoch [328/500], Batch [10/23], Loss: 2317.6670\n",
      "Epoch [328/500], Batch [15/23], Loss: 4463.8760\n",
      "Epoch [328/500], Batch [20/23], Loss: 2904.1274\n",
      "Epoch [329/500], Batch [5/23], Loss: 4152.5635\n",
      "Epoch [329/500], Batch [10/23], Loss: 5854.8770\n",
      "Epoch [329/500], Batch [15/23], Loss: 9728.1953\n",
      "Epoch [329/500], Batch [20/23], Loss: 5031.2300\n",
      "Epoch [330/500], Batch [5/23], Loss: 3807.9858\n",
      "Epoch [330/500], Batch [10/23], Loss: 4251.9033\n",
      "Epoch [330/500], Batch [15/23], Loss: 3393.5640\n",
      "Epoch [330/500], Batch [20/23], Loss: 3049.9893\n",
      "Epoch [331/500], Batch [5/23], Loss: 7415.1006\n",
      "Epoch [331/500], Batch [10/23], Loss: 1809.8066\n",
      "Epoch [331/500], Batch [15/23], Loss: 3886.1470\n",
      "Epoch [331/500], Batch [20/23], Loss: 2587.0762\n",
      "Epoch [332/500], Batch [5/23], Loss: 6529.6099\n",
      "Epoch [332/500], Batch [10/23], Loss: 1279.0267\n",
      "Epoch [332/500], Batch [15/23], Loss: 1151.7119\n",
      "Epoch [332/500], Batch [20/23], Loss: 2668.0374\n",
      "Epoch [333/500], Batch [5/23], Loss: 1492.9111\n",
      "Epoch [333/500], Batch [10/23], Loss: 3347.7681\n",
      "Epoch [333/500], Batch [15/23], Loss: 1786.3020\n",
      "Epoch [333/500], Batch [20/23], Loss: 4519.2090\n",
      "Epoch [334/500], Batch [5/23], Loss: 1417.5157\n",
      "Epoch [334/500], Batch [10/23], Loss: 2014.1179\n",
      "Epoch [334/500], Batch [15/23], Loss: 3898.2048\n",
      "Epoch [334/500], Batch [20/23], Loss: 2602.3384\n",
      "Epoch [335/500], Batch [5/23], Loss: 2512.1001\n",
      "Epoch [335/500], Batch [10/23], Loss: 3090.9270\n",
      "Epoch [335/500], Batch [15/23], Loss: 2679.4744\n",
      "Epoch [335/500], Batch [20/23], Loss: 1243.0032\n",
      "Epoch [336/500], Batch [5/23], Loss: 4471.4287\n",
      "Epoch [336/500], Batch [10/23], Loss: 3250.1074\n",
      "Epoch [336/500], Batch [15/23], Loss: 1209.6339\n",
      "Epoch [336/500], Batch [20/23], Loss: 1361.6366\n",
      "Epoch [337/500], Batch [5/23], Loss: 1864.4895\n",
      "Epoch [337/500], Batch [10/23], Loss: 3464.2349\n",
      "Epoch [337/500], Batch [15/23], Loss: 3398.3655\n",
      "Epoch [337/500], Batch [20/23], Loss: 1816.4553\n",
      "Epoch [338/500], Batch [5/23], Loss: 1540.4779\n",
      "Epoch [338/500], Batch [10/23], Loss: 2662.1292\n",
      "Epoch [338/500], Batch [15/23], Loss: 4692.6313\n",
      "Epoch [338/500], Batch [20/23], Loss: 3149.9644\n",
      "Epoch [339/500], Batch [5/23], Loss: 2233.2039\n",
      "Epoch [339/500], Batch [10/23], Loss: 2608.6565\n",
      "Epoch [339/500], Batch [15/23], Loss: 1979.9945\n",
      "Epoch [339/500], Batch [20/23], Loss: 2070.3699\n",
      "Epoch [340/500], Batch [5/23], Loss: 1818.3645\n",
      "Epoch [340/500], Batch [10/23], Loss: 1452.1659\n",
      "Epoch [340/500], Batch [15/23], Loss: 1311.5693\n",
      "Epoch [340/500], Batch [20/23], Loss: 4019.9990\n",
      "Epoch [341/500], Batch [5/23], Loss: 2744.8508\n",
      "Epoch [341/500], Batch [10/23], Loss: 1507.8539\n",
      "Epoch [341/500], Batch [15/23], Loss: 1569.7634\n",
      "Epoch [341/500], Batch [20/23], Loss: 1822.1263\n",
      "Epoch [342/500], Batch [5/23], Loss: 1664.4065\n",
      "Epoch [342/500], Batch [10/23], Loss: 8068.7422\n",
      "Epoch [342/500], Batch [15/23], Loss: 3071.3438\n",
      "Epoch [342/500], Batch [20/23], Loss: 1254.8085\n",
      "Epoch [343/500], Batch [5/23], Loss: 4820.1143\n",
      "Epoch [343/500], Batch [10/23], Loss: 1818.5957\n",
      "Epoch [343/500], Batch [15/23], Loss: 2137.0007\n",
      "Epoch [343/500], Batch [20/23], Loss: 2382.2520\n",
      "Epoch [344/500], Batch [5/23], Loss: 2592.3364\n",
      "Epoch [344/500], Batch [10/23], Loss: 1887.5518\n",
      "Epoch [344/500], Batch [15/23], Loss: 7128.2012\n",
      "Epoch [344/500], Batch [20/23], Loss: 1999.1638\n",
      "Epoch [345/500], Batch [5/23], Loss: 3580.3984\n",
      "Epoch [345/500], Batch [10/23], Loss: 3377.7241\n",
      "Epoch [345/500], Batch [15/23], Loss: 3839.8286\n",
      "Epoch [345/500], Batch [20/23], Loss: 5643.4390\n",
      "Epoch [346/500], Batch [5/23], Loss: 2288.0098\n",
      "Epoch [346/500], Batch [10/23], Loss: 2276.5620\n",
      "Epoch [346/500], Batch [15/23], Loss: 1676.5132\n",
      "Epoch [346/500], Batch [20/23], Loss: 2213.3467\n",
      "Epoch [347/500], Batch [5/23], Loss: 3246.0342\n",
      "Epoch [347/500], Batch [10/23], Loss: 2060.2188\n",
      "Epoch [347/500], Batch [15/23], Loss: 1126.4788\n",
      "Epoch [347/500], Batch [20/23], Loss: 3789.0764\n",
      "Epoch [348/500], Batch [5/23], Loss: 2245.6157\n",
      "Epoch [348/500], Batch [10/23], Loss: 3108.8223\n",
      "Epoch [348/500], Batch [15/23], Loss: 2770.5991\n",
      "Epoch [348/500], Batch [20/23], Loss: 1012.4841\n",
      "Epoch [349/500], Batch [5/23], Loss: 4239.9238\n",
      "Epoch [349/500], Batch [10/23], Loss: 3051.5552\n",
      "Epoch [349/500], Batch [15/23], Loss: 2057.8843\n",
      "Epoch [349/500], Batch [20/23], Loss: 2641.8796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [350/500], Batch [5/23], Loss: 1618.5869\n",
      "Epoch [350/500], Batch [10/23], Loss: 3255.1875\n",
      "Epoch [350/500], Batch [15/23], Loss: 2956.9478\n",
      "Epoch [350/500], Batch [20/23], Loss: 3489.0696\n",
      "Epoch [351/500], Batch [5/23], Loss: 2535.0320\n",
      "Epoch [351/500], Batch [10/23], Loss: 3868.1589\n",
      "Epoch [351/500], Batch [15/23], Loss: 3805.8960\n",
      "Epoch [351/500], Batch [20/23], Loss: 2053.1428\n",
      "Epoch [352/500], Batch [5/23], Loss: 1467.7554\n",
      "Epoch [352/500], Batch [10/23], Loss: 2567.2310\n",
      "Epoch [352/500], Batch [15/23], Loss: 6889.7754\n",
      "Epoch [352/500], Batch [20/23], Loss: 1885.2804\n",
      "Epoch [353/500], Batch [5/23], Loss: 3245.6572\n",
      "Epoch [353/500], Batch [10/23], Loss: 2770.2593\n",
      "Epoch [353/500], Batch [15/23], Loss: 5118.0068\n",
      "Epoch [353/500], Batch [20/23], Loss: 5344.7427\n",
      "Epoch [354/500], Batch [5/23], Loss: 994.8878\n",
      "Epoch [354/500], Batch [10/23], Loss: 1681.4827\n",
      "Epoch [354/500], Batch [15/23], Loss: 3489.3276\n",
      "Epoch [354/500], Batch [20/23], Loss: 1785.1262\n",
      "Epoch [355/500], Batch [5/23], Loss: 2152.8396\n",
      "Epoch [355/500], Batch [10/23], Loss: 5367.2598\n",
      "Epoch [355/500], Batch [15/23], Loss: 1224.9303\n",
      "Epoch [355/500], Batch [20/23], Loss: 4833.0684\n",
      "Epoch [356/500], Batch [5/23], Loss: 2008.2405\n",
      "Epoch [356/500], Batch [10/23], Loss: 3133.6812\n",
      "Epoch [356/500], Batch [15/23], Loss: 4170.5664\n",
      "Epoch [356/500], Batch [20/23], Loss: 5382.7158\n",
      "Epoch [357/500], Batch [5/23], Loss: 3204.0654\n",
      "Epoch [357/500], Batch [10/23], Loss: 3343.4031\n",
      "Epoch [357/500], Batch [15/23], Loss: 2993.9141\n",
      "Epoch [357/500], Batch [20/23], Loss: 2616.3291\n",
      "Epoch [358/500], Batch [5/23], Loss: 2568.4966\n",
      "Epoch [358/500], Batch [10/23], Loss: 1077.7124\n",
      "Epoch [358/500], Batch [15/23], Loss: 1519.2024\n",
      "Epoch [358/500], Batch [20/23], Loss: 3860.3259\n",
      "Epoch [359/500], Batch [5/23], Loss: 2848.6609\n",
      "Epoch [359/500], Batch [10/23], Loss: 1414.0896\n",
      "Epoch [359/500], Batch [15/23], Loss: 2559.5464\n",
      "Epoch [359/500], Batch [20/23], Loss: 7157.0015\n",
      "Epoch [360/500], Batch [5/23], Loss: 3271.7485\n",
      "Epoch [360/500], Batch [10/23], Loss: 1553.1594\n",
      "Epoch [360/500], Batch [15/23], Loss: 2708.5337\n",
      "Epoch [360/500], Batch [20/23], Loss: 3159.6475\n",
      "Epoch [361/500], Batch [5/23], Loss: 2288.3264\n",
      "Epoch [361/500], Batch [10/23], Loss: 3299.8486\n",
      "Epoch [361/500], Batch [15/23], Loss: 2702.1821\n",
      "Epoch [361/500], Batch [20/23], Loss: 3889.7588\n",
      "Epoch [362/500], Batch [5/23], Loss: 5726.6538\n",
      "Epoch [362/500], Batch [10/23], Loss: 2639.6021\n",
      "Epoch [362/500], Batch [15/23], Loss: 1509.3726\n",
      "Epoch [362/500], Batch [20/23], Loss: 2964.9180\n",
      "Epoch [363/500], Batch [5/23], Loss: 1950.6624\n",
      "Epoch [363/500], Batch [10/23], Loss: 2886.5371\n",
      "Epoch [363/500], Batch [15/23], Loss: 2771.8457\n",
      "Epoch [363/500], Batch [20/23], Loss: 1122.4382\n",
      "Epoch [364/500], Batch [5/23], Loss: 1221.3677\n",
      "Epoch [364/500], Batch [10/23], Loss: 1972.0803\n",
      "Epoch [364/500], Batch [15/23], Loss: 4336.1816\n",
      "Epoch [364/500], Batch [20/23], Loss: 2569.3188\n",
      "Epoch [365/500], Batch [5/23], Loss: 1841.9852\n",
      "Epoch [365/500], Batch [10/23], Loss: 2135.1885\n",
      "Epoch [365/500], Batch [15/23], Loss: 3010.2324\n",
      "Epoch [365/500], Batch [20/23], Loss: 4079.5210\n",
      "Epoch [366/500], Batch [5/23], Loss: 1556.1998\n",
      "Epoch [366/500], Batch [10/23], Loss: 1402.1945\n",
      "Epoch [366/500], Batch [15/23], Loss: 2639.2249\n",
      "Epoch [366/500], Batch [20/23], Loss: 3768.3975\n",
      "Epoch [367/500], Batch [5/23], Loss: 3030.0415\n",
      "Epoch [367/500], Batch [10/23], Loss: 8384.4805\n",
      "Epoch [367/500], Batch [15/23], Loss: 3632.1843\n",
      "Epoch [367/500], Batch [20/23], Loss: 4392.5732\n",
      "Epoch [368/500], Batch [5/23], Loss: 2553.1716\n",
      "Epoch [368/500], Batch [10/23], Loss: 2640.3879\n",
      "Epoch [368/500], Batch [15/23], Loss: 1947.4974\n",
      "Epoch [368/500], Batch [20/23], Loss: 1357.9037\n",
      "Epoch [369/500], Batch [5/23], Loss: 1955.8845\n",
      "Epoch [369/500], Batch [10/23], Loss: 7757.5132\n",
      "Epoch [369/500], Batch [15/23], Loss: 1668.8840\n",
      "Epoch [369/500], Batch [20/23], Loss: 5352.8867\n",
      "Epoch [370/500], Batch [5/23], Loss: 1117.6780\n",
      "Epoch [370/500], Batch [10/23], Loss: 2228.2532\n",
      "Epoch [370/500], Batch [15/23], Loss: 2670.9421\n",
      "Epoch [370/500], Batch [20/23], Loss: 1348.0693\n",
      "Epoch [371/500], Batch [5/23], Loss: 1068.4291\n",
      "Epoch [371/500], Batch [10/23], Loss: 3728.9648\n",
      "Epoch [371/500], Batch [15/23], Loss: 2835.6458\n",
      "Epoch [371/500], Batch [20/23], Loss: 1983.6564\n",
      "Epoch [372/500], Batch [5/23], Loss: 1895.8240\n",
      "Epoch [372/500], Batch [10/23], Loss: 1737.2769\n",
      "Epoch [372/500], Batch [15/23], Loss: 2195.5691\n",
      "Epoch [372/500], Batch [20/23], Loss: 3757.3945\n",
      "Epoch [373/500], Batch [5/23], Loss: 11780.6924\n",
      "Epoch [373/500], Batch [10/23], Loss: 5952.0039\n",
      "Epoch [373/500], Batch [15/23], Loss: 8197.4121\n",
      "Epoch [373/500], Batch [20/23], Loss: 3671.6711\n",
      "Epoch [374/500], Batch [5/23], Loss: 4679.9590\n",
      "Epoch [374/500], Batch [10/23], Loss: 2369.6206\n",
      "Epoch [374/500], Batch [15/23], Loss: 1385.0879\n",
      "Epoch [374/500], Batch [20/23], Loss: 5865.1606\n",
      "Epoch [375/500], Batch [5/23], Loss: 1523.1885\n",
      "Epoch [375/500], Batch [10/23], Loss: 3159.3394\n",
      "Epoch [375/500], Batch [15/23], Loss: 9187.1621\n",
      "Epoch [375/500], Batch [20/23], Loss: 2425.6509\n",
      "Epoch [376/500], Batch [5/23], Loss: 2274.1445\n",
      "Epoch [376/500], Batch [10/23], Loss: 1918.5532\n",
      "Epoch [376/500], Batch [15/23], Loss: 2583.7812\n",
      "Epoch [376/500], Batch [20/23], Loss: 2766.0566\n",
      "Epoch [377/500], Batch [5/23], Loss: 6846.1440\n",
      "Epoch [377/500], Batch [10/23], Loss: 2481.0435\n",
      "Epoch [377/500], Batch [15/23], Loss: 3267.0833\n",
      "Epoch [377/500], Batch [20/23], Loss: 3307.5205\n",
      "Epoch [378/500], Batch [5/23], Loss: 2129.2539\n",
      "Epoch [378/500], Batch [10/23], Loss: 3432.2527\n",
      "Epoch [378/500], Batch [15/23], Loss: 1627.9371\n",
      "Epoch [378/500], Batch [20/23], Loss: 2297.2305\n",
      "Epoch [379/500], Batch [5/23], Loss: 8202.8750\n",
      "Epoch [379/500], Batch [10/23], Loss: 4634.3013\n",
      "Epoch [379/500], Batch [15/23], Loss: 2804.5005\n",
      "Epoch [379/500], Batch [20/23], Loss: 2391.2305\n",
      "Epoch [380/500], Batch [5/23], Loss: 2390.2900\n",
      "Epoch [380/500], Batch [10/23], Loss: 6320.8755\n",
      "Epoch [380/500], Batch [15/23], Loss: 2980.6001\n",
      "Epoch [380/500], Batch [20/23], Loss: 3700.6201\n",
      "Epoch [381/500], Batch [5/23], Loss: 2917.5662\n",
      "Epoch [381/500], Batch [10/23], Loss: 3314.2332\n",
      "Epoch [381/500], Batch [15/23], Loss: 1758.1704\n",
      "Epoch [381/500], Batch [20/23], Loss: 1518.6040\n",
      "Epoch [382/500], Batch [5/23], Loss: 2815.6226\n",
      "Epoch [382/500], Batch [10/23], Loss: 3784.2205\n",
      "Epoch [382/500], Batch [15/23], Loss: 2604.8901\n",
      "Epoch [382/500], Batch [20/23], Loss: 2503.7351\n",
      "Epoch [383/500], Batch [5/23], Loss: 1782.0193\n",
      "Epoch [383/500], Batch [10/23], Loss: 2251.7615\n",
      "Epoch [383/500], Batch [15/23], Loss: 2178.9001\n",
      "Epoch [383/500], Batch [20/23], Loss: 2878.7917\n",
      "Epoch [384/500], Batch [5/23], Loss: 2346.9810\n",
      "Epoch [384/500], Batch [10/23], Loss: 2223.1138\n",
      "Epoch [384/500], Batch [15/23], Loss: 4081.6577\n",
      "Epoch [384/500], Batch [20/23], Loss: 1532.2559\n",
      "Epoch [385/500], Batch [5/23], Loss: 2408.7271\n",
      "Epoch [385/500], Batch [10/23], Loss: 1681.9736\n",
      "Epoch [385/500], Batch [15/23], Loss: 1837.6992\n",
      "Epoch [385/500], Batch [20/23], Loss: 2120.6562\n",
      "Epoch [386/500], Batch [5/23], Loss: 1732.2997\n",
      "Epoch [386/500], Batch [10/23], Loss: 1961.7563\n",
      "Epoch [386/500], Batch [15/23], Loss: 2208.7078\n",
      "Epoch [386/500], Batch [20/23], Loss: 1878.3047\n",
      "Epoch [387/500], Batch [5/23], Loss: 2173.5337\n",
      "Epoch [387/500], Batch [10/23], Loss: 1750.8207\n",
      "Epoch [387/500], Batch [15/23], Loss: 1526.5212\n",
      "Epoch [387/500], Batch [20/23], Loss: 5768.2119\n",
      "Epoch [388/500], Batch [5/23], Loss: 2152.0410\n",
      "Epoch [388/500], Batch [10/23], Loss: 2774.3491\n",
      "Epoch [388/500], Batch [15/23], Loss: 1591.6719\n",
      "Epoch [388/500], Batch [20/23], Loss: 1779.0725\n",
      "Epoch [389/500], Batch [5/23], Loss: 1491.0049\n",
      "Epoch [389/500], Batch [10/23], Loss: 1542.4259\n",
      "Epoch [389/500], Batch [15/23], Loss: 2011.0052\n",
      "Epoch [389/500], Batch [20/23], Loss: 1348.4718\n",
      "Epoch [390/500], Batch [5/23], Loss: 4020.5388\n",
      "Epoch [390/500], Batch [10/23], Loss: 7043.1934\n",
      "Epoch [390/500], Batch [15/23], Loss: 7294.6548\n",
      "Epoch [390/500], Batch [20/23], Loss: 3464.9768\n",
      "Epoch [391/500], Batch [5/23], Loss: 2806.8882\n",
      "Epoch [391/500], Batch [10/23], Loss: 8757.1191\n",
      "Epoch [391/500], Batch [15/23], Loss: 919.2826\n",
      "Epoch [391/500], Batch [20/23], Loss: 6122.2744\n",
      "Epoch [392/500], Batch [5/23], Loss: 1485.5032\n",
      "Epoch [392/500], Batch [10/23], Loss: 2214.8057\n",
      "Epoch [392/500], Batch [15/23], Loss: 2948.1201\n",
      "Epoch [392/500], Batch [20/23], Loss: 3001.5452\n",
      "Epoch [393/500], Batch [5/23], Loss: 5234.0366\n",
      "Epoch [393/500], Batch [10/23], Loss: 2973.8259\n",
      "Epoch [393/500], Batch [15/23], Loss: 2457.3547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [393/500], Batch [20/23], Loss: 1126.9038\n",
      "Epoch [394/500], Batch [5/23], Loss: 1751.1653\n",
      "Epoch [394/500], Batch [10/23], Loss: 3247.1372\n",
      "Epoch [394/500], Batch [15/23], Loss: 1958.2056\n",
      "Epoch [394/500], Batch [20/23], Loss: 2509.3960\n",
      "Epoch [395/500], Batch [5/23], Loss: 3636.7349\n",
      "Epoch [395/500], Batch [10/23], Loss: 2908.4800\n",
      "Epoch [395/500], Batch [15/23], Loss: 3281.1445\n",
      "Epoch [395/500], Batch [20/23], Loss: 1574.4088\n",
      "Epoch [396/500], Batch [5/23], Loss: 3720.0547\n",
      "Epoch [396/500], Batch [10/23], Loss: 1065.7046\n",
      "Epoch [396/500], Batch [15/23], Loss: 3391.1589\n",
      "Epoch [396/500], Batch [20/23], Loss: 929.8355\n",
      "Epoch [397/500], Batch [5/23], Loss: 7276.8140\n",
      "Epoch [397/500], Batch [10/23], Loss: 1005.9125\n",
      "Epoch [397/500], Batch [15/23], Loss: 2828.3096\n",
      "Epoch [397/500], Batch [20/23], Loss: 3758.0815\n",
      "Epoch [398/500], Batch [5/23], Loss: 3409.8093\n",
      "Epoch [398/500], Batch [10/23], Loss: 1164.1807\n",
      "Epoch [398/500], Batch [15/23], Loss: 1767.0109\n",
      "Epoch [398/500], Batch [20/23], Loss: 1682.1229\n",
      "Epoch [399/500], Batch [5/23], Loss: 1978.4791\n",
      "Epoch [399/500], Batch [10/23], Loss: 1118.8300\n",
      "Epoch [399/500], Batch [15/23], Loss: 4273.7871\n",
      "Epoch [399/500], Batch [20/23], Loss: 2381.7144\n",
      "Epoch [400/500], Batch [5/23], Loss: 1773.9541\n",
      "Epoch [400/500], Batch [10/23], Loss: 1639.4052\n",
      "Epoch [400/500], Batch [15/23], Loss: 2396.4116\n",
      "Epoch [400/500], Batch [20/23], Loss: 3042.9829\n",
      "Epoch [401/500], Batch [5/23], Loss: 1748.8757\n",
      "Epoch [401/500], Batch [10/23], Loss: 1797.1067\n",
      "Epoch [401/500], Batch [15/23], Loss: 2841.0605\n",
      "Epoch [401/500], Batch [20/23], Loss: 1578.6182\n",
      "Epoch [402/500], Batch [5/23], Loss: 2463.1238\n",
      "Epoch [402/500], Batch [10/23], Loss: 3574.4998\n",
      "Epoch [402/500], Batch [15/23], Loss: 5350.8955\n",
      "Epoch [402/500], Batch [20/23], Loss: 3367.1328\n",
      "Epoch [403/500], Batch [5/23], Loss: 1547.3772\n",
      "Epoch [403/500], Batch [10/23], Loss: 3428.9683\n",
      "Epoch [403/500], Batch [15/23], Loss: 1374.6652\n",
      "Epoch [403/500], Batch [20/23], Loss: 1938.6881\n",
      "Epoch [404/500], Batch [5/23], Loss: 2179.0010\n",
      "Epoch [404/500], Batch [10/23], Loss: 4402.2388\n",
      "Epoch [404/500], Batch [15/23], Loss: 1629.7913\n",
      "Epoch [404/500], Batch [20/23], Loss: 5492.0459\n",
      "Epoch [405/500], Batch [5/23], Loss: 1780.9489\n",
      "Epoch [405/500], Batch [10/23], Loss: 2696.4429\n",
      "Epoch [405/500], Batch [15/23], Loss: 2247.3538\n",
      "Epoch [405/500], Batch [20/23], Loss: 3465.3203\n",
      "Epoch [406/500], Batch [5/23], Loss: 1279.0903\n",
      "Epoch [406/500], Batch [10/23], Loss: 2950.7759\n",
      "Epoch [406/500], Batch [15/23], Loss: 1524.9620\n",
      "Epoch [406/500], Batch [20/23], Loss: 5566.3994\n",
      "Epoch [407/500], Batch [5/23], Loss: 1345.6893\n",
      "Epoch [407/500], Batch [10/23], Loss: 3129.6436\n",
      "Epoch [407/500], Batch [15/23], Loss: 1738.8970\n",
      "Epoch [407/500], Batch [20/23], Loss: 1665.3191\n",
      "Epoch [408/500], Batch [5/23], Loss: 1446.1025\n",
      "Epoch [408/500], Batch [10/23], Loss: 4043.4641\n",
      "Epoch [408/500], Batch [15/23], Loss: 7283.5981\n",
      "Epoch [408/500], Batch [20/23], Loss: 3133.2686\n",
      "Epoch [409/500], Batch [5/23], Loss: 1406.9678\n",
      "Epoch [409/500], Batch [10/23], Loss: 1594.4369\n",
      "Epoch [409/500], Batch [15/23], Loss: 1660.6892\n",
      "Epoch [409/500], Batch [20/23], Loss: 4879.7314\n",
      "Epoch [410/500], Batch [5/23], Loss: 1545.0500\n",
      "Epoch [410/500], Batch [10/23], Loss: 3878.8643\n",
      "Epoch [410/500], Batch [15/23], Loss: 5619.7769\n",
      "Epoch [410/500], Batch [20/23], Loss: 1648.8732\n",
      "Epoch [411/500], Batch [5/23], Loss: 1589.9690\n",
      "Epoch [411/500], Batch [10/23], Loss: 2496.7310\n",
      "Epoch [411/500], Batch [15/23], Loss: 3068.1196\n",
      "Epoch [411/500], Batch [20/23], Loss: 918.4004\n",
      "Epoch [412/500], Batch [5/23], Loss: 1619.1912\n",
      "Epoch [412/500], Batch [10/23], Loss: 2600.7886\n",
      "Epoch [412/500], Batch [15/23], Loss: 1947.2311\n",
      "Epoch [412/500], Batch [20/23], Loss: 5867.5635\n",
      "Epoch [413/500], Batch [5/23], Loss: 2881.4541\n",
      "Epoch [413/500], Batch [10/23], Loss: 8145.2432\n",
      "Epoch [413/500], Batch [15/23], Loss: 1307.7224\n",
      "Epoch [413/500], Batch [20/23], Loss: 4359.5557\n",
      "Epoch [414/500], Batch [5/23], Loss: 1705.9941\n",
      "Epoch [414/500], Batch [10/23], Loss: 3440.7793\n",
      "Epoch [414/500], Batch [15/23], Loss: 2605.8796\n",
      "Epoch [414/500], Batch [20/23], Loss: 3258.7224\n",
      "Epoch [415/500], Batch [5/23], Loss: 1891.0942\n",
      "Epoch [415/500], Batch [10/23], Loss: 2248.7788\n",
      "Epoch [415/500], Batch [15/23], Loss: 3928.7378\n",
      "Epoch [415/500], Batch [20/23], Loss: 2860.0627\n",
      "Epoch [416/500], Batch [5/23], Loss: 3950.3896\n",
      "Epoch [416/500], Batch [10/23], Loss: 3230.6372\n",
      "Epoch [416/500], Batch [15/23], Loss: 5444.3394\n",
      "Epoch [416/500], Batch [20/23], Loss: 2692.2544\n",
      "Epoch [417/500], Batch [5/23], Loss: 2539.9641\n",
      "Epoch [417/500], Batch [10/23], Loss: 1534.7156\n",
      "Epoch [417/500], Batch [15/23], Loss: 2589.4663\n",
      "Epoch [417/500], Batch [20/23], Loss: 2732.1143\n",
      "Epoch [418/500], Batch [5/23], Loss: 6041.1538\n",
      "Epoch [418/500], Batch [10/23], Loss: 2366.9961\n",
      "Epoch [418/500], Batch [15/23], Loss: 6305.3799\n",
      "Epoch [418/500], Batch [20/23], Loss: 3011.4685\n",
      "Epoch [419/500], Batch [5/23], Loss: 4233.6455\n",
      "Epoch [419/500], Batch [10/23], Loss: 2650.5337\n",
      "Epoch [419/500], Batch [15/23], Loss: 1798.6172\n",
      "Epoch [419/500], Batch [20/23], Loss: 1644.4991\n",
      "Epoch [420/500], Batch [5/23], Loss: 4649.6777\n",
      "Epoch [420/500], Batch [10/23], Loss: 2346.4626\n",
      "Epoch [420/500], Batch [15/23], Loss: 4161.1387\n",
      "Epoch [420/500], Batch [20/23], Loss: 2521.3997\n",
      "Epoch [421/500], Batch [5/23], Loss: 5080.8848\n",
      "Epoch [421/500], Batch [10/23], Loss: 2754.8687\n",
      "Epoch [421/500], Batch [15/23], Loss: 3859.6577\n",
      "Epoch [421/500], Batch [20/23], Loss: 4066.2273\n",
      "Epoch [422/500], Batch [5/23], Loss: 1854.8240\n",
      "Epoch [422/500], Batch [10/23], Loss: 4028.9783\n",
      "Epoch [422/500], Batch [15/23], Loss: 1755.5132\n",
      "Epoch [422/500], Batch [20/23], Loss: 2330.8044\n",
      "Epoch [423/500], Batch [5/23], Loss: 7154.0483\n",
      "Epoch [423/500], Batch [10/23], Loss: 3060.2776\n",
      "Epoch [423/500], Batch [15/23], Loss: 1797.7610\n",
      "Epoch [423/500], Batch [20/23], Loss: 2659.2295\n",
      "Epoch [424/500], Batch [5/23], Loss: 3590.0242\n",
      "Epoch [424/500], Batch [10/23], Loss: 2435.9385\n",
      "Epoch [424/500], Batch [15/23], Loss: 2704.6489\n",
      "Epoch [424/500], Batch [20/23], Loss: 1947.5573\n",
      "Epoch [425/500], Batch [5/23], Loss: 2959.2378\n",
      "Epoch [425/500], Batch [10/23], Loss: 1877.6884\n",
      "Epoch [425/500], Batch [15/23], Loss: 2874.6655\n",
      "Epoch [425/500], Batch [20/23], Loss: 2138.5254\n",
      "Epoch [426/500], Batch [5/23], Loss: 2654.6887\n",
      "Epoch [426/500], Batch [10/23], Loss: 2535.4355\n",
      "Epoch [426/500], Batch [15/23], Loss: 2213.8438\n",
      "Epoch [426/500], Batch [20/23], Loss: 2348.9705\n",
      "Epoch [427/500], Batch [5/23], Loss: 1807.2271\n",
      "Epoch [427/500], Batch [10/23], Loss: 1417.5873\n",
      "Epoch [427/500], Batch [15/23], Loss: 1450.7517\n",
      "Epoch [427/500], Batch [20/23], Loss: 3539.5029\n",
      "Epoch [428/500], Batch [5/23], Loss: 3227.4519\n",
      "Epoch [428/500], Batch [10/23], Loss: 1171.3066\n",
      "Epoch [428/500], Batch [15/23], Loss: 2244.2935\n",
      "Epoch [428/500], Batch [20/23], Loss: 3732.9211\n",
      "Epoch [429/500], Batch [5/23], Loss: 1487.2565\n",
      "Epoch [429/500], Batch [10/23], Loss: 4778.5039\n",
      "Epoch [429/500], Batch [15/23], Loss: 3792.3550\n",
      "Epoch [429/500], Batch [20/23], Loss: 1362.0203\n",
      "Epoch [430/500], Batch [5/23], Loss: 3325.0095\n",
      "Epoch [430/500], Batch [10/23], Loss: 2322.7434\n",
      "Epoch [430/500], Batch [15/23], Loss: 3464.4302\n",
      "Epoch [430/500], Batch [20/23], Loss: 1430.3950\n",
      "Epoch [431/500], Batch [5/23], Loss: 1198.2355\n",
      "Epoch [431/500], Batch [10/23], Loss: 3338.1921\n",
      "Epoch [431/500], Batch [15/23], Loss: 2053.4531\n",
      "Epoch [431/500], Batch [20/23], Loss: 3206.5315\n",
      "Epoch [432/500], Batch [5/23], Loss: 2393.6309\n",
      "Epoch [432/500], Batch [10/23], Loss: 1388.2102\n",
      "Epoch [432/500], Batch [15/23], Loss: 2612.8179\n",
      "Epoch [432/500], Batch [20/23], Loss: 3615.2356\n",
      "Epoch [433/500], Batch [5/23], Loss: 1477.9812\n",
      "Epoch [433/500], Batch [10/23], Loss: 3205.3816\n",
      "Epoch [433/500], Batch [15/23], Loss: 1626.2915\n",
      "Epoch [433/500], Batch [20/23], Loss: 6146.2476\n",
      "Epoch [434/500], Batch [5/23], Loss: 1550.5177\n",
      "Epoch [434/500], Batch [10/23], Loss: 3300.1191\n",
      "Epoch [434/500], Batch [15/23], Loss: 2066.0413\n",
      "Epoch [434/500], Batch [20/23], Loss: 2637.2371\n",
      "Epoch [435/500], Batch [5/23], Loss: 2995.6895\n",
      "Epoch [435/500], Batch [10/23], Loss: 2333.1545\n",
      "Epoch [435/500], Batch [15/23], Loss: 1440.5024\n",
      "Epoch [435/500], Batch [20/23], Loss: 2034.3074\n",
      "Epoch [436/500], Batch [5/23], Loss: 2934.1543\n",
      "Epoch [436/500], Batch [10/23], Loss: 2528.2319\n",
      "Epoch [436/500], Batch [15/23], Loss: 3491.6306\n",
      "Epoch [436/500], Batch [20/23], Loss: 2694.2168\n",
      "Epoch [437/500], Batch [5/23], Loss: 2991.7017\n",
      "Epoch [437/500], Batch [10/23], Loss: 3118.4785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [437/500], Batch [15/23], Loss: 4444.3716\n",
      "Epoch [437/500], Batch [20/23], Loss: 2427.2632\n",
      "Epoch [438/500], Batch [5/23], Loss: 5339.3828\n",
      "Epoch [438/500], Batch [10/23], Loss: 2676.1245\n",
      "Epoch [438/500], Batch [15/23], Loss: 6955.1045\n",
      "Epoch [438/500], Batch [20/23], Loss: 2614.6875\n",
      "Epoch [439/500], Batch [5/23], Loss: 6698.7837\n",
      "Epoch [439/500], Batch [10/23], Loss: 1737.5444\n",
      "Epoch [439/500], Batch [15/23], Loss: 3477.0522\n",
      "Epoch [439/500], Batch [20/23], Loss: 2666.3369\n",
      "Epoch [440/500], Batch [5/23], Loss: 4140.0542\n",
      "Epoch [440/500], Batch [10/23], Loss: 2536.8467\n",
      "Epoch [440/500], Batch [15/23], Loss: 1548.5020\n",
      "Epoch [440/500], Batch [20/23], Loss: 4513.1748\n",
      "Epoch [441/500], Batch [5/23], Loss: 2319.7012\n",
      "Epoch [441/500], Batch [10/23], Loss: 1777.7269\n",
      "Epoch [441/500], Batch [15/23], Loss: 3438.6829\n",
      "Epoch [441/500], Batch [20/23], Loss: 2204.3091\n",
      "Epoch [442/500], Batch [5/23], Loss: 4044.9490\n",
      "Epoch [442/500], Batch [10/23], Loss: 3398.1772\n",
      "Epoch [442/500], Batch [15/23], Loss: 2792.6812\n",
      "Epoch [442/500], Batch [20/23], Loss: 1931.6086\n",
      "Epoch [443/500], Batch [5/23], Loss: 3511.4170\n",
      "Epoch [443/500], Batch [10/23], Loss: 2323.1941\n",
      "Epoch [443/500], Batch [15/23], Loss: 1991.2440\n",
      "Epoch [443/500], Batch [20/23], Loss: 2783.5713\n",
      "Epoch [444/500], Batch [5/23], Loss: 4774.2559\n",
      "Epoch [444/500], Batch [10/23], Loss: 926.1119\n",
      "Epoch [444/500], Batch [15/23], Loss: 2039.8601\n",
      "Epoch [444/500], Batch [20/23], Loss: 4950.7192\n",
      "Epoch [445/500], Batch [5/23], Loss: 2165.2529\n",
      "Epoch [445/500], Batch [10/23], Loss: 3104.3140\n",
      "Epoch [445/500], Batch [15/23], Loss: 5965.7832\n",
      "Epoch [445/500], Batch [20/23], Loss: 1217.9996\n",
      "Epoch [446/500], Batch [5/23], Loss: 3843.2537\n",
      "Epoch [446/500], Batch [10/23], Loss: 1204.3334\n",
      "Epoch [446/500], Batch [15/23], Loss: 3193.0962\n",
      "Epoch [446/500], Batch [20/23], Loss: 9763.6172\n",
      "Epoch [447/500], Batch [5/23], Loss: 3044.0134\n",
      "Epoch [447/500], Batch [10/23], Loss: 2841.7207\n",
      "Epoch [447/500], Batch [15/23], Loss: 2667.0708\n",
      "Epoch [447/500], Batch [20/23], Loss: 3288.4375\n",
      "Epoch [448/500], Batch [5/23], Loss: 5507.3354\n",
      "Epoch [448/500], Batch [10/23], Loss: 3157.9111\n",
      "Epoch [448/500], Batch [15/23], Loss: 2752.8423\n",
      "Epoch [448/500], Batch [20/23], Loss: 1530.1897\n",
      "Epoch [449/500], Batch [5/23], Loss: 3940.1143\n",
      "Epoch [449/500], Batch [10/23], Loss: 2512.8350\n",
      "Epoch [449/500], Batch [15/23], Loss: 2481.4282\n",
      "Epoch [449/500], Batch [20/23], Loss: 1704.3245\n",
      "Epoch [450/500], Batch [5/23], Loss: 2084.3684\n",
      "Epoch [450/500], Batch [10/23], Loss: 2330.0366\n",
      "Epoch [450/500], Batch [15/23], Loss: 1149.8579\n",
      "Epoch [450/500], Batch [20/23], Loss: 1203.4132\n",
      "Epoch [451/500], Batch [5/23], Loss: 4777.4365\n",
      "Epoch [451/500], Batch [10/23], Loss: 3876.4934\n",
      "Epoch [451/500], Batch [15/23], Loss: 3106.7612\n",
      "Epoch [451/500], Batch [20/23], Loss: 1874.7246\n",
      "Epoch [452/500], Batch [5/23], Loss: 1601.1050\n",
      "Epoch [452/500], Batch [10/23], Loss: 2817.0396\n",
      "Epoch [452/500], Batch [15/23], Loss: 3641.9822\n",
      "Epoch [452/500], Batch [20/23], Loss: 1641.8889\n",
      "Epoch [453/500], Batch [5/23], Loss: 1130.1478\n",
      "Epoch [453/500], Batch [10/23], Loss: 6455.4248\n",
      "Epoch [453/500], Batch [15/23], Loss: 2620.0691\n",
      "Epoch [453/500], Batch [20/23], Loss: 4154.2437\n",
      "Epoch [454/500], Batch [5/23], Loss: 2226.5264\n",
      "Epoch [454/500], Batch [10/23], Loss: 1935.1483\n",
      "Epoch [454/500], Batch [15/23], Loss: 2634.7024\n",
      "Epoch [454/500], Batch [20/23], Loss: 2882.6050\n",
      "Epoch [455/500], Batch [5/23], Loss: 1084.4282\n",
      "Epoch [455/500], Batch [10/23], Loss: 6578.4121\n",
      "Epoch [455/500], Batch [15/23], Loss: 2776.8359\n",
      "Epoch [455/500], Batch [20/23], Loss: 5464.6328\n",
      "Epoch [456/500], Batch [5/23], Loss: 4966.4971\n",
      "Epoch [456/500], Batch [10/23], Loss: 3178.5012\n",
      "Epoch [456/500], Batch [15/23], Loss: 5156.7983\n",
      "Epoch [456/500], Batch [20/23], Loss: 2664.5320\n",
      "Epoch [457/500], Batch [5/23], Loss: 2519.6548\n",
      "Epoch [457/500], Batch [10/23], Loss: 2624.9709\n",
      "Epoch [457/500], Batch [15/23], Loss: 3306.5283\n",
      "Epoch [457/500], Batch [20/23], Loss: 4388.2402\n",
      "Epoch [458/500], Batch [5/23], Loss: 1522.9658\n",
      "Epoch [458/500], Batch [10/23], Loss: 3848.7900\n",
      "Epoch [458/500], Batch [15/23], Loss: 1434.5476\n",
      "Epoch [458/500], Batch [20/23], Loss: 5767.4893\n",
      "Epoch [459/500], Batch [5/23], Loss: 3326.2539\n",
      "Epoch [459/500], Batch [10/23], Loss: 6125.3047\n",
      "Epoch [459/500], Batch [15/23], Loss: 2652.4246\n",
      "Epoch [459/500], Batch [20/23], Loss: 2542.6475\n",
      "Epoch [460/500], Batch [5/23], Loss: 3416.1050\n",
      "Epoch [460/500], Batch [10/23], Loss: 3787.8618\n",
      "Epoch [460/500], Batch [15/23], Loss: 2649.8359\n",
      "Epoch [460/500], Batch [20/23], Loss: 872.3891\n",
      "Epoch [461/500], Batch [5/23], Loss: 2299.8159\n",
      "Epoch [461/500], Batch [10/23], Loss: 2723.2112\n",
      "Epoch [461/500], Batch [15/23], Loss: 1258.1865\n",
      "Epoch [461/500], Batch [20/23], Loss: 899.4574\n",
      "Epoch [462/500], Batch [5/23], Loss: 2117.0486\n",
      "Epoch [462/500], Batch [10/23], Loss: 2935.7656\n",
      "Epoch [462/500], Batch [15/23], Loss: 9448.9785\n",
      "Epoch [462/500], Batch [20/23], Loss: 1815.7517\n",
      "Epoch [463/500], Batch [5/23], Loss: 2928.8171\n",
      "Epoch [463/500], Batch [10/23], Loss: 3505.5552\n",
      "Epoch [463/500], Batch [15/23], Loss: 3349.4663\n",
      "Epoch [463/500], Batch [20/23], Loss: 1762.5999\n",
      "Epoch [464/500], Batch [5/23], Loss: 1723.6083\n",
      "Epoch [464/500], Batch [10/23], Loss: 2890.2776\n",
      "Epoch [464/500], Batch [15/23], Loss: 2573.6252\n",
      "Epoch [464/500], Batch [20/23], Loss: 2974.5383\n",
      "Epoch [465/500], Batch [5/23], Loss: 3112.5813\n",
      "Epoch [465/500], Batch [10/23], Loss: 2394.7832\n",
      "Epoch [465/500], Batch [15/23], Loss: 3756.7751\n",
      "Epoch [465/500], Batch [20/23], Loss: 5391.0166\n",
      "Epoch [466/500], Batch [5/23], Loss: 1662.4697\n",
      "Epoch [466/500], Batch [10/23], Loss: 1804.3010\n",
      "Epoch [466/500], Batch [15/23], Loss: 2178.8845\n",
      "Epoch [466/500], Batch [20/23], Loss: 1521.6836\n",
      "Epoch [467/500], Batch [5/23], Loss: 1637.9125\n",
      "Epoch [467/500], Batch [10/23], Loss: 3273.0850\n",
      "Epoch [467/500], Batch [15/23], Loss: 1284.2737\n",
      "Epoch [467/500], Batch [20/23], Loss: 2622.2563\n",
      "Epoch [468/500], Batch [5/23], Loss: 2670.5540\n",
      "Epoch [468/500], Batch [10/23], Loss: 1697.7438\n",
      "Epoch [468/500], Batch [15/23], Loss: 4001.8074\n",
      "Epoch [468/500], Batch [20/23], Loss: 3335.2651\n",
      "Epoch [469/500], Batch [5/23], Loss: 3870.1541\n",
      "Epoch [469/500], Batch [10/23], Loss: 3514.9297\n",
      "Epoch [469/500], Batch [15/23], Loss: 2600.8721\n",
      "Epoch [469/500], Batch [20/23], Loss: 1648.4301\n",
      "Epoch [470/500], Batch [5/23], Loss: 2322.9226\n",
      "Epoch [470/500], Batch [10/23], Loss: 3790.5356\n",
      "Epoch [470/500], Batch [15/23], Loss: 4171.6870\n",
      "Epoch [470/500], Batch [20/23], Loss: 4387.0776\n",
      "Epoch [471/500], Batch [5/23], Loss: 2699.2192\n",
      "Epoch [471/500], Batch [10/23], Loss: 1865.0955\n",
      "Epoch [471/500], Batch [15/23], Loss: 1537.7117\n",
      "Epoch [471/500], Batch [20/23], Loss: 5797.2559\n",
      "Epoch [472/500], Batch [5/23], Loss: 2049.9575\n",
      "Epoch [472/500], Batch [10/23], Loss: 1613.2035\n",
      "Epoch [472/500], Batch [15/23], Loss: 1225.7219\n",
      "Epoch [472/500], Batch [20/23], Loss: 980.1339\n",
      "Epoch [473/500], Batch [5/23], Loss: 4442.2344\n",
      "Epoch [473/500], Batch [10/23], Loss: 3342.5427\n",
      "Epoch [473/500], Batch [15/23], Loss: 2367.5620\n",
      "Epoch [473/500], Batch [20/23], Loss: 6242.9727\n",
      "Epoch [474/500], Batch [5/23], Loss: 5207.1270\n",
      "Epoch [474/500], Batch [10/23], Loss: 3878.8638\n",
      "Epoch [474/500], Batch [15/23], Loss: 1242.4119\n",
      "Epoch [474/500], Batch [20/23], Loss: 2183.9028\n",
      "Epoch [475/500], Batch [5/23], Loss: 2552.2759\n",
      "Epoch [475/500], Batch [10/23], Loss: 6440.2681\n",
      "Epoch [475/500], Batch [15/23], Loss: 2774.9004\n",
      "Epoch [475/500], Batch [20/23], Loss: 2179.7705\n",
      "Epoch [476/500], Batch [5/23], Loss: 3486.7456\n",
      "Epoch [476/500], Batch [10/23], Loss: 2822.5742\n",
      "Epoch [476/500], Batch [15/23], Loss: 2240.3662\n",
      "Epoch [476/500], Batch [20/23], Loss: 3034.5566\n",
      "Epoch [477/500], Batch [5/23], Loss: 1912.6171\n",
      "Epoch [477/500], Batch [10/23], Loss: 2722.9333\n",
      "Epoch [477/500], Batch [15/23], Loss: 5855.3501\n",
      "Epoch [477/500], Batch [20/23], Loss: 2636.2617\n",
      "Epoch [478/500], Batch [5/23], Loss: 1391.1501\n",
      "Epoch [478/500], Batch [10/23], Loss: 1390.6282\n",
      "Epoch [478/500], Batch [15/23], Loss: 2030.7467\n",
      "Epoch [478/500], Batch [20/23], Loss: 5439.4580\n",
      "Epoch [479/500], Batch [5/23], Loss: 2125.1152\n",
      "Epoch [479/500], Batch [10/23], Loss: 7905.5166\n",
      "Epoch [479/500], Batch [15/23], Loss: 3311.7549\n",
      "Epoch [479/500], Batch [20/23], Loss: 2743.2935\n",
      "Epoch [480/500], Batch [5/23], Loss: 6298.4512\n",
      "Epoch [480/500], Batch [10/23], Loss: 1813.1267\n",
      "Epoch [480/500], Batch [15/23], Loss: 3377.0732\n",
      "Epoch [480/500], Batch [20/23], Loss: 1567.5271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [481/500], Batch [5/23], Loss: 2409.5063\n",
      "Epoch [481/500], Batch [10/23], Loss: 3514.9783\n",
      "Epoch [481/500], Batch [15/23], Loss: 2578.9302\n",
      "Epoch [481/500], Batch [20/23], Loss: 2044.7725\n",
      "Epoch [482/500], Batch [5/23], Loss: 4086.8262\n",
      "Epoch [482/500], Batch [10/23], Loss: 2238.1445\n",
      "Epoch [482/500], Batch [15/23], Loss: 2847.3320\n",
      "Epoch [482/500], Batch [20/23], Loss: 2858.7561\n",
      "Epoch [483/500], Batch [5/23], Loss: 1442.5381\n",
      "Epoch [483/500], Batch [10/23], Loss: 4166.3931\n",
      "Epoch [483/500], Batch [15/23], Loss: 2489.1211\n",
      "Epoch [483/500], Batch [20/23], Loss: 1910.7345\n",
      "Epoch [484/500], Batch [5/23], Loss: 1215.8702\n",
      "Epoch [484/500], Batch [10/23], Loss: 3751.3975\n",
      "Epoch [484/500], Batch [15/23], Loss: 3206.8997\n",
      "Epoch [484/500], Batch [20/23], Loss: 1209.3870\n",
      "Epoch [485/500], Batch [5/23], Loss: 1383.6061\n",
      "Epoch [485/500], Batch [10/23], Loss: 3595.7844\n",
      "Epoch [485/500], Batch [15/23], Loss: 1087.9666\n",
      "Epoch [485/500], Batch [20/23], Loss: 5088.2612\n",
      "Epoch [486/500], Batch [5/23], Loss: 1329.5989\n",
      "Epoch [486/500], Batch [10/23], Loss: 4961.1001\n",
      "Epoch [486/500], Batch [15/23], Loss: 5970.7534\n",
      "Epoch [486/500], Batch [20/23], Loss: 1063.7358\n",
      "Epoch [487/500], Batch [5/23], Loss: 3986.8872\n",
      "Epoch [487/500], Batch [10/23], Loss: 1570.7726\n",
      "Epoch [487/500], Batch [15/23], Loss: 1705.0806\n",
      "Epoch [487/500], Batch [20/23], Loss: 4442.9028\n",
      "Epoch [488/500], Batch [5/23], Loss: 1407.6785\n",
      "Epoch [488/500], Batch [10/23], Loss: 2097.7136\n",
      "Epoch [488/500], Batch [15/23], Loss: 783.6914\n",
      "Epoch [488/500], Batch [20/23], Loss: 2654.0195\n",
      "Epoch [489/500], Batch [5/23], Loss: 2460.9358\n",
      "Epoch [489/500], Batch [10/23], Loss: 3054.8408\n",
      "Epoch [489/500], Batch [15/23], Loss: 2448.7163\n",
      "Epoch [489/500], Batch [20/23], Loss: 2329.4663\n",
      "Epoch [490/500], Batch [5/23], Loss: 1978.2009\n",
      "Epoch [490/500], Batch [10/23], Loss: 6120.9204\n",
      "Epoch [490/500], Batch [15/23], Loss: 2558.3013\n",
      "Epoch [490/500], Batch [20/23], Loss: 3802.6455\n",
      "Epoch [491/500], Batch [5/23], Loss: 4645.1655\n",
      "Epoch [491/500], Batch [10/23], Loss: 3741.3833\n",
      "Epoch [491/500], Batch [15/23], Loss: 1516.9679\n",
      "Epoch [491/500], Batch [20/23], Loss: 2765.7910\n",
      "Epoch [492/500], Batch [5/23], Loss: 2381.5774\n",
      "Epoch [492/500], Batch [10/23], Loss: 1836.7302\n",
      "Epoch [492/500], Batch [15/23], Loss: 3891.4004\n",
      "Epoch [492/500], Batch [20/23], Loss: 1856.8640\n",
      "Epoch [493/500], Batch [5/23], Loss: 1198.6901\n",
      "Epoch [493/500], Batch [10/23], Loss: 2921.8306\n",
      "Epoch [493/500], Batch [15/23], Loss: 1211.8975\n",
      "Epoch [493/500], Batch [20/23], Loss: 1129.0968\n",
      "Epoch [494/500], Batch [5/23], Loss: 2871.0256\n",
      "Epoch [494/500], Batch [10/23], Loss: 3138.0542\n",
      "Epoch [494/500], Batch [15/23], Loss: 1737.2810\n",
      "Epoch [494/500], Batch [20/23], Loss: 4331.5947\n",
      "Epoch [495/500], Batch [5/23], Loss: 4284.4150\n",
      "Epoch [495/500], Batch [10/23], Loss: 2976.5745\n",
      "Epoch [495/500], Batch [15/23], Loss: 1430.1899\n",
      "Epoch [495/500], Batch [20/23], Loss: 847.7214\n",
      "Epoch [496/500], Batch [5/23], Loss: 5539.0742\n",
      "Epoch [496/500], Batch [10/23], Loss: 2081.6206\n",
      "Epoch [496/500], Batch [15/23], Loss: 3364.4773\n",
      "Epoch [496/500], Batch [20/23], Loss: 3967.5273\n",
      "Epoch [497/500], Batch [5/23], Loss: 1454.1422\n",
      "Epoch [497/500], Batch [10/23], Loss: 2037.8639\n",
      "Epoch [497/500], Batch [15/23], Loss: 1355.4609\n",
      "Epoch [497/500], Batch [20/23], Loss: 3201.8450\n",
      "Epoch [498/500], Batch [5/23], Loss: 4847.6582\n",
      "Epoch [498/500], Batch [10/23], Loss: 2446.6306\n",
      "Epoch [498/500], Batch [15/23], Loss: 3301.9080\n",
      "Epoch [498/500], Batch [20/23], Loss: 2068.9199\n",
      "Epoch [499/500], Batch [5/23], Loss: 3695.1616\n",
      "Epoch [499/500], Batch [10/23], Loss: 1754.7166\n",
      "Epoch [499/500], Batch [15/23], Loss: 7701.7583\n",
      "Epoch [499/500], Batch [20/23], Loss: 1186.8378\n",
      "Epoch [500/500], Batch [5/23], Loss: 4466.7314\n",
      "Epoch [500/500], Batch [10/23], Loss: 5082.6973\n",
      "Epoch [500/500], Batch [15/23], Loss: 2275.1770\n",
      "Epoch [500/500], Batch [20/23], Loss: 1876.0857\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    xlstmtime.train()\n",
    "    for batch_idx, (X_b, y_b) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        y_b_pred = xlstmtime(X_b)\n",
    "        y_b_pred = y_b_pred[:,-1,:] # select state for last element in sequence\n",
    "        loss = criterion(y_b_pred, y_b)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c11bd8",
   "metadata": {},
   "source": [
    "## Examine outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "748626cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_after_train = torch.from_numpy(X_examples[228:229,:].astype('float32')).to(device=device).reshape(-1, 50 , 1)\n",
    "y_after_train = y_examples[228:229]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff5c65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_after_train = xlstmtime(X_after_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c62430d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2171.7285]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_after_train[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d26570a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2151.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_after_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "576df898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2384.5, 2350. , 2240.5, 2257. , 2266. , 2260.5, 2260. , 2241. ,\n",
       "        2255. , 2142. , 2134. , 2140. , 2165. , 2142. , 2105.5, 2123.5,\n",
       "        2108. , 2125. , 2163.5, 2150. , 2118. , 2142. , 2158. , 2159. ,\n",
       "        2170. , 2132. , 2159. , 2183.5, 2170. , 2122.5, 2080. , 2100.5,\n",
       "        2123.5, 2097. , 2070.5, 2145. , 2192. , 2117. , 2140. , 2152.5,\n",
       "        2178. , 2202.5, 2196.5, 2213. , 2194. , 2195. , 2168. , 2136. ,\n",
       "        2143.5, 2185.5]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_examples[228:229,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35cc3848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2350. , 2240.5, 2257. , 2266. , 2260.5, 2260. , 2241. , 2255. ,\n",
       "        2142. , 2134. , 2140. , 2165. , 2142. , 2105.5, 2123.5, 2108. ,\n",
       "        2125. , 2163.5, 2150. , 2118. , 2142. , 2158. , 2159. , 2170. ,\n",
       "        2132. , 2159. , 2183.5, 2170. , 2122.5, 2080. , 2100.5, 2123.5,\n",
       "        2097. , 2070.5, 2145. , 2192. , 2117. , 2140. , 2152.5, 2178. ,\n",
       "        2202.5, 2196.5, 2213. , 2194. , 2195. , 2168. , 2136. , 2143.5,\n",
       "        2185.5, 2151. ],\n",
       "       [2240.5, 2257. , 2266. , 2260.5, 2260. , 2241. , 2255. , 2142. ,\n",
       "        2134. , 2140. , 2165. , 2142. , 2105.5, 2123.5, 2108. , 2125. ,\n",
       "        2163.5, 2150. , 2118. , 2142. , 2158. , 2159. , 2170. , 2132. ,\n",
       "        2159. , 2183.5, 2170. , 2122.5, 2080. , 2100.5, 2123.5, 2097. ,\n",
       "        2070.5, 2145. , 2192. , 2117. , 2140. , 2152.5, 2178. , 2202.5,\n",
       "        2196.5, 2213. , 2194. , 2195. , 2168. , 2136. , 2143.5, 2185.5,\n",
       "        2151. , 2143. ]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_examples[229:231,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3344286a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.10 (nxai_xlstm)",
   "language": "python",
   "name": "nxai_xlstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
